{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17ca9a8f",
   "metadata": {
    "id": "17ca9a8f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a2f9eb3",
   "metadata": {
    "id": "6a2f9eb3"
   },
   "outputs": [],
   "source": [
    "actions_map = {\n",
    "    0: 'take 1 coin',\n",
    "    1: 'coup',\n",
    "    2: 'take 2 coins',\n",
    "    3: 'take 3 coins',\n",
    "    4: 'steal 2 coins',\n",
    "    5: 'assassinate',\n",
    "    6: 'exchange',\n",
    "    7: 'challenge',\n",
    "    8: 'block foreign aid',\n",
    "    9: 'block stealing',\n",
    "    10: 'block assassination'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d15621e",
   "metadata": {
    "id": "4d15621e"
   },
   "outputs": [],
   "source": [
    "class Action:\n",
    "    def __init__(self, name, challengeable, response_card, response_action,\n",
    "                 p1_net_coins, p2_net_coins, p1_net_cards, p2_net_cards, vector, utility):\n",
    "        self.name = name\n",
    "        self.challengeable = challengeable\n",
    "        self.response_card = response_card\n",
    "        self.response_action = response_action\n",
    "        self.p1_net_coins = p1_net_coins\n",
    "        self.p2_net_coins = p2_net_coins\n",
    "        self.p1_net_cards = p1_net_cards\n",
    "        self.p2_net_cards = p2_net_cards\n",
    "#         self.base_utility = base_utility\n",
    "#         self.p_bluff = p_bluff\n",
    "        self.vector = vector\n",
    "        self.utility = utility\n",
    "\n",
    "    def update_responses(self, response_card, response_action):\n",
    "        self.response_card = response_card\n",
    "        self.response_action = response_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baef8cbb",
   "metadata": {
    "id": "baef8cbb"
   },
   "outputs": [],
   "source": [
    "take_1 = Action(actions_map[0], False, None, None, 1, 0, 0, 0, [0], 1.0)\n",
    "\n",
    "coup = Action(actions_map[1], False, None, None, -7, 0, 0, -1, [1], 8.0)\n",
    "\n",
    "take_2 = Action(actions_map[2], True, 'Duke', actions_map[8], 2, 0, 0, 0, [2], 2.0)\n",
    "\n",
    "take_3 = Action(actions_map[3], True, None, actions_map[7], 3, 0, 0, 0, [3], 5.0)\n",
    "\n",
    "steal_2 = Action(actions_map[4], True, ['Captain', 'Ambassador'], actions_map[9], 2, -2, 0, 0,[4], 6.0)\n",
    "\n",
    "assassinate = Action(actions_map[5], True, 'Contessa', actions_map[10], -3, 0, 0, -1, [5], 10.0)\n",
    "\n",
    "exchange = Action(actions_map[6], True, None, actions_map[7], 0, 0, 0, 0,[6], 3.0)\n",
    "\n",
    "# challenge = Action(actions_map[7], False, None, None, 0, 0, -1, -1, 1, 0)\n",
    "\n",
    "block_take_2 = Action(actions_map[8], True, None, actions_map[7], 0, -2, 0, 0, [7], None)\n",
    "\n",
    "block_steal = Action(actions_map[9], True, None, actions_map[7], 2, -2, 0, 0, [8], None)\n",
    "\n",
    "block_assassination = Action(actions_map[10], True, None, actions_map[7], 0, 0, 1, 0, [9], None)\n",
    "\n",
    "# utilities = [a.utility for a in [take_1, coup, take_2, take_3, steal_2, assassinate, exchange]]\n",
    "\n",
    "# arr = np.exp(utilities - np.max(utilities))\n",
    "\n",
    "# so\n",
    "\n",
    "# challenge =\n",
    "\n",
    "actions = {\n",
    "    0: take_1,\n",
    "    1: coup,\n",
    "    2: take_2,\n",
    "    3: take_3,\n",
    "    4: steal_2,\n",
    "    5: assassinate,\n",
    "    6: exchange,\n",
    "    7: block_take_2,\n",
    "    8: block_steal,\n",
    "    9: block_assassination\n",
    "}\n",
    "\n",
    "take_2.response_action = actions[7]\n",
    "steal_2.response_action = actions[8]\n",
    "assassinate.response_action = actions[9]\n",
    "\n",
    "influences = {\n",
    "    'Duke': [take_3, block_take_2, take_1, coup],\n",
    "    'Captain': [steal_2, block_steal, take_2, take_1, coup],\n",
    "    'Assassin': [assassinate, take_2, take_1, coup],\n",
    "    'Contessa': [take_2, block_assassination, take_1, coup],\n",
    "    'Ambassador': [exchange, block_steal, take_2, take_1, coup]\n",
    "    }\n",
    "\n",
    "inf_map = {\n",
    "    'Dead': 0,\n",
    "    'Duke': 1,\n",
    "    'Captain': 2,\n",
    "    'Assassin': 3,\n",
    "    'Contessa': 4,\n",
    "    'Ambassador': 5,\n",
    "    'Hidden': 6\n",
    "}\n",
    "\n",
    "\n",
    "influences_reverse = {\n",
    "    take_1: ['Duke', 'Captain', 'Assassin', 'Contessa', 'Ambassador'],\n",
    "    coup: ['Duke', 'Captain', 'Assassin', 'Contessa', 'Ambassador'],\n",
    "    take_2: ['Captain', 'Assassin', 'Contessa', 'Ambassador'],\n",
    "    take_3: ['Duke'],\n",
    "    steal_2: ['Captain'],\n",
    "    assassinate: ['Assassin'],\n",
    "    exchange: ['Ambassador'],\n",
    "    block_take_2: ['Duke'],\n",
    "    block_steal: ['Captain','Ambassador'],\n",
    "    block_assassination: ['Contessa']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d617da4f",
   "metadata": {
    "id": "d617da4f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=128):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        # self.dropout = nn.Dropout(0.2)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc3 = nn.Linear(hidden_size, 64)\n",
    "        self.fc4 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: shape (batch_size, seq_len, state_size)\n",
    "        returns: shape (batch_size, action_size)\n",
    "        \"\"\"\n",
    "        # 1) Pass each frame in the sequence through a linear + ReLU\n",
    "        #    => shape (batch_size, seq_len, hidden_size)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        # x = self.dropout(x)\n",
    "\n",
    "        # 2) Pass entire sequence through RNN\n",
    "        #    => out: shape (batch_size, seq_len, hidden_size)\n",
    "        out, _ = self.rnn(x)\n",
    "\n",
    "        # 3) We want the LAST hidden vector from the sequence (i.e. out[:, -1, :])\n",
    "        last_out = out[:, -1, :]  # shape (batch_size, hidden_size)\n",
    "\n",
    "        # 4) Pass to fully connected heads for Q-values\n",
    "        x = self.fc3(last_out)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x)  # shape (batch_size, action_size)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "embedding_cards = nn.Embedding(7, 1)\n",
    "cards_tens = torch.tensor([0,1,2,3,4,5,6])\n",
    "cards_emb = embedding_cards(cards_tens)\n",
    "\n",
    "# actions = {\n",
    "#     0: take_1,\n",
    "#     1: coup,\n",
    "#     2: take_2,\n",
    "#     3: take_3,\n",
    "#     4: steal_2,\n",
    "#     5: assassinate,\n",
    "#     6: exchange,\n",
    "#     7: block_take_2,\n",
    "#     8: block_steal,\n",
    "#     9: block_assassination\n",
    "# }\n",
    "\n",
    "embedding_actions = nn.Embedding(8, 3)\n",
    "actions_tens = torch.tensor([0,1,2,3,4,5,6,7])\n",
    "actions_emb = embedding_actions(actions_tens)\n",
    "\n",
    "embedding_coins = nn.Embedding(13, 2)\n",
    "coins_tens = torch.tensor([0,1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "coins_emb = embedding_coins(coins_tens)\n",
    "\n",
    "embedding_players = nn.Embedding(5, 3)\n",
    "players_tens = torch.tensor([0,1,2,3,4])\n",
    "players_emb = embedding_players(players_tens)\n",
    "\n",
    "state_size_a = 12\n",
    "state_size_b = 13\n",
    "action_size = 16\n",
    "block_size = 2\n",
    "challenge_size = 2\n",
    "card_size = 2\n",
    "\n",
    "criterion = nn.HuberLoss(delta=1.0)\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.005\n",
    "min_epsilon = 0.01\n",
    "# batch_size = 64\n",
    "# replay_buffer_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "EvwXxvtVf4_3",
   "metadata": {
    "id": "EvwXxvtVf4_3"
   },
   "outputs": [],
   "source": [
    "class StateSummarizer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, embedding_size):\n",
    "        super(StateSummarizer, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, embedding_size)\n",
    "\n",
    "    def forward(self, next_states):\n",
    "        # Initialize hidden state and cell state\n",
    "        h0 = torch.zeros(1, next_states.shape[0], self.hidden_size).to(next_states.device)\n",
    "        c0 = torch.zeros(1, next_states.shape[0], self.hidden_size).to(next_states.device)\n",
    "\n",
    "        # Pass the sequence of next states through the LSTM\n",
    "        out, _ = self.lstm(next_states, (h0, c0))\n",
    "\n",
    "        # print(f'out.shape: {out.shape}')\n",
    "\n",
    "        # Take the last hidden state as the summary\n",
    "        summary = torch.mean(out, dim=1)  # Average across the sequence dimension (dim=1)\n",
    "\n",
    "        # Project the summary to the desired embedding size\n",
    "        embedding = self.fc(summary)\n",
    "        return embedding\n",
    "\n",
    "# summarizer = StateSummarizer(12, 64, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "952658b8",
   "metadata": {
    "id": "952658b8"
   },
   "outputs": [],
   "source": [
    "class Bot:\n",
    "    def __init__(self, cards, num_coins, hostility, name, action_q, block_q, challenge_q, card_q,\n",
    "                 optimizer_action, optimizer_block, optimizer_challenge, optimizer_card,\n",
    "                 summarizer):\n",
    "        self.cards = cards\n",
    "        self.num_coins = num_coins\n",
    "        self.hostility = hostility\n",
    "        self.name = name\n",
    "        self.action_q = action_q\n",
    "        self.block_q = block_q\n",
    "        self.challenge_q = challenge_q\n",
    "        self.card_q = card_q\n",
    "        self.optimizer_action = optimizer_action\n",
    "        self.optimizer_block = optimizer_block\n",
    "        self.optimizer_challenge = optimizer_challenge\n",
    "        self.optimizer_card = optimizer_card\n",
    "        self.summarizer = summarizer\n",
    "\n",
    "    def num_coins_adj(self, n):\n",
    "        self.num_coins += n\n",
    "\n",
    "    def cards_adj(self, card):\n",
    "        self.cards.remove(card)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7826ddbf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7826ddbf",
    "outputId": "c842b236-49ba-4702-94b9-21937734e97e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Captain', 'Assassin']\n",
      "['Contessa', 'Contessa']\n",
      "['Assassin', 'Captain']\n",
      "['Assassin', 'Duke']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "bag = ['Duke', 'Captain', 'Assassin', 'Contessa', 'Ambassador'] * 3\n",
    "random.shuffle(bag)\n",
    "\n",
    "bots = []\n",
    "bluff_degree = 0\n",
    "\n",
    "for i in range(4):\n",
    "    cards = random.sample(bag, 2)\n",
    "    for card in cards:\n",
    "        bag.remove(card)\n",
    "#     kb = []\n",
    "\n",
    "    action_q = QNetwork(state_size_a, action_size)\n",
    "    optimizer_action = optim.Adam(action_q.parameters(), lr=learning_rate)\n",
    "\n",
    "    block_q = QNetwork(state_size_b, block_size)\n",
    "    optimizer_block = optim.Adam(block_q.parameters(), lr=learning_rate)\n",
    "\n",
    "    challenge_q = QNetwork(state_size_b, challenge_size)\n",
    "    optimizer_challenge = optim.Adam(challenge_q.parameters(), lr=learning_rate)\n",
    "\n",
    "    card_q = QNetwork(state_size_b, card_size)\n",
    "    optimizer_card = optim.Adam(card_q.parameters(), lr=learning_rate)\n",
    "\n",
    "    summarizer = StateSummarizer(12, 64, 12)\n",
    "\n",
    "    bots.append(Bot(cards, 2, None, i, action_q, block_q, challenge_q, card_q,\n",
    "                    optimizer_action, optimizer_block, optimizer_challenge, optimizer_card, summarizer))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for bot in bots:\n",
    "    print(bot.cards)\n",
    "print(bots[0].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "av5X5iZs2UJQ",
   "metadata": {
    "id": "av5X5iZs2UJQ"
   },
   "outputs": [],
   "source": [
    "replay_buffer = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "TadCAcsPTrIz",
   "metadata": {
    "id": "TadCAcsPTrIz"
   },
   "outputs": [],
   "source": [
    "# actions = {\n",
    "#     0: take_1,\n",
    "#     1: coup,\n",
    "#     2: take_2,\n",
    "#     3: take_3,\n",
    "#     4: steal_2,\n",
    "#     5: assassinate,\n",
    "#     6: exchange,\n",
    "#     7: block_take_2,\n",
    "#     8: block_steal,\n",
    "#     9: block_assassination\n",
    "# }\n",
    "\n",
    "def get_legal_actions(bot, bots):\n",
    "  \"\"\"Returns a list of legal action indices for the given bot.\"\"\"\n",
    "  legal_actions = []\n",
    "\n",
    "  if bot.num_coins >= 10:\n",
    "    return [1]\n",
    "\n",
    "  # Always legal actions\n",
    "  legal_actions.extend([0, 2, 3, 6])  # Income, Foreign Aid, Tax, Exchange are always legal\n",
    "\n",
    "  # Conditional actions\n",
    "  if bot.num_coins >= 7:\n",
    "    legal_actions.append(1)  # Coup\n",
    "  if bot.num_coins >= 3:\n",
    "    legal_actions.append(5)  # Assassinate (if enough coins)\n",
    "\n",
    "  # Actions that target other players\n",
    "  for other_bot in bots:\n",
    "    if other_bot != bot and other_bot.num_coins > 1 : #Can't steal from players with no coins\n",
    "      legal_actions.append(4) #Steal\n",
    "      break  # Only need to add steal once if there's a valid target\n",
    "\n",
    "  return legal_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cc07371",
   "metadata": {
    "id": "0cc07371"
   },
   "outputs": [],
   "source": [
    "def action_selection(i, bots, actions_vector, actions, epsilon, state):\n",
    "\n",
    "    legal_actions = get_legal_actions(bots[i], bots)  # Get list of legal actions\n",
    "\n",
    "    if int(bots[i].name) == 0:\n",
    "      if random.random() >= epsilon:\n",
    "          q_values = bots[i].action_q(state)\n",
    "          max_q_value = float('-inf')\n",
    "          best_action = None\n",
    "\n",
    "          # Iterate through Q-values and find the maximum for legal actions\n",
    "          for action_idx, q_value in enumerate(q_values[0]):  # q_values[0] is assumed to be a 1D tensor\n",
    "              if action_idx in legal_actions and q_value.item() > max_q_value:\n",
    "                  max_q_value = q_value.item()\n",
    "                  best_action = action_idx\n",
    "\n",
    "          # If no best action was selected from Q-values, choose randomly from legal actions\n",
    "          if best_action is None:\n",
    "              best_action = random.choice(legal_actions)\n",
    "\n",
    "          # Now determine the target bot: choose the bot (other than self) with the highest number of cards,\n",
    "          # and if there's a tie, choose the one with the most coins.\n",
    "          target = None\n",
    "          max_cards = -1\n",
    "          max_coins = -1\n",
    "          for bot_idx, other_bot in enumerate(bots):\n",
    "              if int(other_bot.name) != 0:  # Skip self (bot 0)\n",
    "                  num_coins = other_bot.num_coins  # Assuming 'cards' is a list of the bot's cards\n",
    "                  if num_coins > max_coins:\n",
    "                      max_coins = num_coins  # Assuming num_coins is a numeric attribute\n",
    "                      target = bots[bot_idx]\n",
    "\n",
    "          # Return the chosen action and the target bot index\n",
    "          return [best_action, target]\n",
    "      else:\n",
    "        arr = [actions[a].utility for a in legal_actions]\n",
    "        e_x = np.exp(arr - np.max(arr))\n",
    "        softmax = e_x / e_x.sum(axis=0)\n",
    "        max_idx = np.argmax(softmax)\n",
    "        action = legal_actions[max_idx]\n",
    "        # Random action selection:\n",
    "        if bots[i].num_coins >= 10:\n",
    "            action = 1  # Coup\n",
    "        \n",
    "        target = None\n",
    "        if (actions[action].p2_net_coins != 0 or actions[action].p2_net_cards != 0) and actions[action].response_action != 'challenge':\n",
    "            targets = bots[:i] + bots[i+1:]\n",
    "            valid_targets = [bot for bot in targets if bot.num_coins >= -actions[action].p2_net_coins]\n",
    "            if valid_targets:\n",
    "                target = random.choice(valid_targets)\n",
    "\n",
    "        return [action, target]\n",
    "\n",
    "# actions = {\n",
    "#     0: take_1,\n",
    "#     1: coup,\n",
    "#     2: take_2,\n",
    "#     3: take_3,\n",
    "#     4: steal_2,\n",
    "#     5: assassinate,\n",
    "#     6: exchange,\n",
    "#     7: block_take_2,\n",
    "#     8: block_steal,\n",
    "#     9: block_assassination\n",
    "# }\n",
    "\n",
    "    else:\n",
    "\n",
    "      target = None\n",
    "      action = None\n",
    "      targets = bots[:i] + bots[i+1:]\n",
    "\n",
    "      # Play truthfully:\n",
    "      # bot = bots[i]\n",
    "      if bots[i].num_coins >= 10:  # Coup if possible\n",
    "          action = 1  # Coup action index\n",
    "          target = random.choice(targets)  # Choose a random target\n",
    "      else:\n",
    "          # Prioritize actions based on cards and coins:\n",
    "          if 'Duke' in bots[i].cards:  # Take 3 coins if Duke\n",
    "              action = 3  # Take 3 coins action index\n",
    "          elif 'Captain' in bots[i].cards and 4 in legal_actions:\n",
    "              action = 4  # Steal action index\n",
    "              valid_targets = [bot for bot in targets if bot.num_coins >= 2]\n",
    "              if valid_targets:\n",
    "                  target = random.choice(valid_targets)\n",
    "              # target = bots.index(random.choice([other_bot for other_bot in bots if other_bot != bots[i] and other_bot.num_coins > 0]))\n",
    "          elif 'Assassin' in bots[i].cards and 5 in legal_actions:\n",
    "              action = 5  # Assassinate action index\n",
    "              target = random.choice(targets)  # Choose a random target\n",
    "          elif 'Ambassador' in bots[i].cards:  # Exchange if Ambassador\n",
    "              action = 6  # Exchange action index\n",
    "          else:  # Otherwise, take income\n",
    "              action = 0  # Take 1 coin action index\n",
    "          if bots[i].num_coins >= 7:\n",
    "            action = 1 # Coup if previous action was not legal and can coup\n",
    "            target = random.choice(targets)\n",
    "          elif 'Duke' not in bots[i].cards:\n",
    "            action = 2 # foreign aid if previous action was not legal and can foreign aid\n",
    "          # else:\n",
    "          #   action = 0 # income if previous action was not legal and can only income\n",
    "\n",
    "      # If action requires a target and none is selected yet:\n",
    "      # if (actions[action].p2_net_coins != 0 or actions[action].p2_net_cards != 0) and actions[action].response_action != 'challenge' and target is None:\n",
    "      #     target = bots.index(random.choice(bots[:i] + bots[i+1:]))\n",
    "\n",
    "      return [action, target]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def reaction_selection(i, bots, target, response_action, epsilon, state):\n",
    "\n",
    "  if target is None:\n",
    "\n",
    "    target = random.choice(bots[:i] + bots[i+1:])\n",
    "\n",
    "  if random.random() >= epsilon and int(target.name) == 0:\n",
    "\n",
    "    # state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "    q_values = target.block_q(state)\n",
    "    return torch.argmax(q_values).item(), target\n",
    "\n",
    "  elif int(target.name) != 0:\n",
    "\n",
    "    # Play truthfully - block only if has the card\n",
    "    if response_action.name == actions_map[8]:  # Block foreign aid\n",
    "        if 'Duke' in target.cards:\n",
    "            return 1, target  # Block\n",
    "        else:\n",
    "            return 0, target  # Pass\n",
    "    elif response_action.name == actions_map[9]:  # Block stealing\n",
    "        if 'Captain' in target.cards or 'Ambassador' in target.cards:\n",
    "            return 1, target  # Block\n",
    "        else:\n",
    "            return 0, target  # Pass\n",
    "    elif response_action.name == actions_map[10]:  # Block assassination\n",
    "        if 'Contessa' in target.cards:\n",
    "            return 1, target  # Block\n",
    "        else:\n",
    "            return 0, target  # Pass\n",
    "    else:  # Other actions (cannot be blocked truthfully)\n",
    "        return 0, target  # Pass\n",
    "\n",
    "  else:\n",
    "\n",
    "    return random.choice([0, 1]), target\n",
    "\n",
    "\n",
    "def challenge_selection(epsilon, state, bot):\n",
    "\n",
    "  if random.random() >= epsilon and int(bot.name) == 0:\n",
    "\n",
    "    return 0\n",
    "\n",
    "    # state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "    # q_values = bot.challenge_q(state_tensor)\n",
    "    # return torch.argmax(q_values).item()\n",
    "\n",
    "  else:\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def card_selection(bot, cards, epsilon, state, action):\n",
    "\n",
    "  if random.random() >= epsilon and len(cards) > 1 and int(bot.name) == 0:\n",
    "\n",
    "    # state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "    q_values = bot.card_q(state)\n",
    "    card_index = torch.argmax(q_values).item()  # Get index (0 or 1)\n",
    "    return card_index  # Return the index directly\n",
    "\n",
    "  else:\n",
    "\n",
    "    c = random.choice(cards)\n",
    "\n",
    "    c = cards.index(c)\n",
    "\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af20170f",
   "metadata": {
    "id": "af20170f"
   },
   "outputs": [],
   "source": [
    "def perform_action(bot, target, action, discard_pile, state, card_chosen, epsilon, bag):\n",
    "\n",
    "    if target is not None:\n",
    "\n",
    "        target.num_coins += action.p2_net_coins\n",
    "\n",
    "        if action.p2_net_cards < 0 and len(target.cards) > 0:\n",
    "\n",
    "            card = card_selection(target, target.cards, epsilon, state, action)\n",
    "            # print(card)\n",
    "\n",
    "            x = target.cards[card]\n",
    "\n",
    "            discard_pile.append(inf_map[x])\n",
    "\n",
    "            card_chosen = card\n",
    "\n",
    "            target.cards.remove(x)\n",
    "\n",
    "    bot.num_coins += action.p1_net_coins\n",
    "\n",
    "    if action == exchange:\n",
    "\n",
    "        card = card_selection(bot, bot.cards, epsilon, state, action)\n",
    "\n",
    "        x = bot.cards[card]\n",
    "\n",
    "        c = random.sample(bag, 2)\n",
    "\n",
    "        # arr = [x] + c\n",
    "\n",
    "        next_choice = card_selection(bot, c, epsilon, state, action)\n",
    "\n",
    "        next_choice = c[next_choice]\n",
    "\n",
    "        arr = [x] + [next_choice]\n",
    "\n",
    "        final_choice = card_selection(bot, arr, epsilon, state, action)\n",
    "\n",
    "        card_chosen = final_choice\n",
    "\n",
    "        final_choice = arr[final_choice]\n",
    "\n",
    "        arr.remove(final_choice)\n",
    "\n",
    "        for i in arr:\n",
    "            bag.insert(-1, i)\n",
    "\n",
    "        random.shuffle(bag)\n",
    "        bot.cards.insert(-1, final_choice)\n",
    "        bot.cards.remove(x)\n",
    "\n",
    "    return bot, target, discard_pile, card_chosen, bag\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "LCgzznC-DY3Y",
   "metadata": {
    "id": "LCgzznC-DY3Y"
   },
   "outputs": [],
   "source": [
    "def reset_game(bots_copy):\n",
    "  bag = ['Duke', 'Captain', 'Assassin', 'Contessa', 'Ambassador'] * 3\n",
    "  random.shuffle(bag)\n",
    "  new_bots = []  # Create a new list\n",
    "  for i, bot in enumerate(bots_copy):\n",
    "      cards = random.sample(bag, 2)\n",
    "      for card in cards:\n",
    "          bag.remove(card)\n",
    "      new_bots.append(Bot(cards, 2, None, f'{i}', bot.action_q, bot.block_q, bot.challenge_q, bot.card_q,\n",
    "                          bot.optimizer_action, bot.optimizer_block, bot.optimizer_challenge, bot.optimizer_card))\n",
    "  return new_bots  # Return the new list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c6e59e2",
   "metadata": {
    "id": "3c6e59e2"
   },
   "outputs": [],
   "source": [
    "# Base Game Loop\n",
    "\n",
    "def game_loop_random(bots, actions, influences_reverse, epsilon):\n",
    "    bag = ['Duke', 'Captain', 'Assassin', 'Contessa', 'Ambassador'] * 3\n",
    "    random.shuffle(bag)\n",
    "\n",
    "    bots_copy = copy.deepcopy(bots)\n",
    "\n",
    "    states = torch.empty((0, 12), dtype=torch.float32)\n",
    "    states_block = torch.empty((0, 13), dtype=torch.float32)\n",
    "    states_challenge = torch.empty((0, 13), dtype=torch.float32)\n",
    "    states_card = torch.empty((0, 13), dtype=torch.float32)\n",
    "\n",
    "    discard_piles = []\n",
    "    discard_piles.append([])\n",
    "    acting_players = []\n",
    "    reacting_players = []\n",
    "    current_players = [[1,1,1,1]]\n",
    "    actions_game = [7]\n",
    "    reactions_game = [0]\n",
    "    challenges_game = [0]\n",
    "    challenges_direction = []\n",
    "    cards_game = []\n",
    "    coins_game = []\n",
    "\n",
    "    # state_tuples = []\n",
    "\n",
    "    rewards = [0]\n",
    "\n",
    "    action_history = [7]\n",
    "    reaction_history = [0]\n",
    "    challenge_history = [0]\n",
    "    card_history = [0]\n",
    "\n",
    "    cards_turn = [[inf_map[c] for c in bots[0].cards],\n",
    "                  [6,6],\n",
    "                  [6,6],\n",
    "                  [6,6]]\n",
    "    # for i in range(3):\n",
    "\n",
    "    #     cards_ind = [inf_map[c] for c in bot.cards]\n",
    "    #     cards_turn.append(cards_ind)\n",
    "\n",
    "    cards_game.append(cards_turn)\n",
    "\n",
    "    coins_turn = [2,2,2,2]\n",
    "    coins_game.append(coins_turn)\n",
    "    done = []\n",
    "    cards_chosen = [0]\n",
    "\n",
    "    t = 0\n",
    "\n",
    "\n",
    "#     print(cards_game[-1])\n",
    "#     print(coins_game[-1])\n",
    "\n",
    "    while len(bots) > 1:\n",
    "\n",
    "        if t > 100 and bots[0].name == 0:\n",
    "          done.append(1)\n",
    "          rewards.append(1.0)\n",
    "          break\n",
    "\n",
    "\n",
    "#         for bot in bots:\n",
    "#             print(f'{bot.name}')\n",
    "        i = 0\n",
    "        while i < len(bots):\n",
    "\n",
    "            # print(i)\n",
    "\n",
    "            # for bot in bots:\n",
    "            #   print(bot.cards)\n",
    "\n",
    "            card_chosen = 0\n",
    "#             print(i)\n",
    "\n",
    "            if len(bots) == 1:\n",
    "                # done.append(1)\n",
    "                break\n",
    "\n",
    "\n",
    "            done.append(0)\n",
    "            rewards.append(0)\n",
    "\n",
    "            challenge_dir = 2\n",
    "\n",
    "            discard_pile = copy.deepcopy(discard_piles[-1])\n",
    "\n",
    "            curr = None\n",
    "            try:\n",
    "                curr = bots[i]\n",
    "            except:\n",
    "                i = 0\n",
    "                curr = bots[i]\n",
    "\n",
    "            acting_players.append(int(curr.name))\n",
    "\n",
    "            # cards_state = cards_game[-1]\n",
    "            # coins_state = coins_game[-1]\n",
    "            # current_players_state = current_players[-1]\n",
    "\n",
    "            # cards_game: N x 4 x 2 -> 8 tensors of size N\n",
    "            cards_game_tensors = [\n",
    "                torch.tensor([cards_game[i][j][k] for i in range(len(cards_game))] )\n",
    "                for j in range(4) for k in range(2)\n",
    "            ]\n",
    "\n",
    "            # current_players: N x 4 -> 1 tensor of size N\n",
    "            current_players_tensors = torch.tensor([sum(row) for row in current_players])\n",
    "\n",
    "            coins_game_tensors = [\n",
    "                      torch.tensor([coins_game[i][j] for i in range(len(coins_game))] )\n",
    "                      for j in range(4)\n",
    "            ]\n",
    "\n",
    "            # discard_piles: N x y (max y = 7) -> 7 tensors of size N\n",
    "            max_discard_len = 7  # Maximum possible length of discard_piles\n",
    "            discard_piles_tensors = [\n",
    "                torch.tensor([discard_piles[i][j] if j < len(discard_piles[i]) else 0\n",
    "                              for i in range(len(discard_piles))] )\n",
    "                for j in range(max_discard_len)\n",
    "            ]\n",
    "\n",
    "            # Concatenate tensors for states_action\n",
    "            # states_action = torch.cat(([\n",
    "            #     torch.tensor(acting_players[1:]).unsqueeze(1), # unsqueeze to add a dimension\n",
    "            #     torch.tensor(reacting_players).unsqueeze(1),\n",
    "            #     torch.tensor(reactions_game).unsqueeze(1),\n",
    "            #     torch.tensor(challenges_game).unsqueeze(1),\n",
    "            #     torch.tensor(current_players_tensors).unsqueeze(1),\n",
    "            #     *[t.unsqueeze(1) for t in cards_game_tensors],\n",
    "            #     *[t.unsqueeze(1) for t in discard_piles_tensors],\n",
    "            #     *[t.unsqueeze(1) for t in coins_game_tensors],\n",
    "            #     torch.tensor(done).unsqueeze(1)\n",
    "            # ]), 1)  # changed dim to 1\n",
    "\n",
    "            state = None\n",
    "\n",
    "            if int(bots[0].name) == 0:\n",
    "\n",
    "              # 1. Cards in play (embedded):\n",
    "              cards_in_play_embedded = []\n",
    "              for card_name in influences.keys():\n",
    "                  num_in_discard = discard_piles[-1].count(inf_map[card_name])\n",
    "                  num_in_play = 3 - num_in_discard  # Assuming 3 of each card initially\n",
    "                  cards_in_play_embedded.append(torch.tensor(num_in_play))  # Keep as tensor\n",
    "\n",
    "              # Stack the embeddings to create a 2D tensor\n",
    "              cards_in_play_embedded = torch.stack(cards_in_play_embedded).squeeze()\n",
    "\n",
    "              # 4. Bot 0's normalized coins:\n",
    "              bot0_coins_normalized = bots[0].num_coins / 12 # Normalize to 0-1 range (assuming max coins is 12)\n",
    "\n",
    "              # 5. Average cards of other players (normalized and embedded):\n",
    "              other_bots_cards = [len(bot.cards) for bot in bots if bot != bots[0]]\n",
    "              avg_other_cards = sum(other_bots_cards) / len(other_bots_cards) if other_bots_cards else 0  # Avoid division by zero\n",
    "              avg_other_cards_normalized = avg_other_cards / 2  # Normalize to 0-1 range (assuming max cards per bot is 2)\n",
    "              # avg_other_cards_embedded = embedding_cards(torch.tensor(int(avg_other_cards_normalized))).tolist()  # Assuming embedding_cards is your embedding layer\n",
    "\n",
    "              # 6. Bot 0's current cards (embedded):\n",
    "              bot0_cards_embedded = []\n",
    "              for card in bots[0].cards:\n",
    "                  bot0_cards_embedded.append(embedding_cards(torch.tensor(inf_map[card])))  # Keep as tensor\n",
    "\n",
    "              # If the bot has cards, concatenate the embeddings. Otherwise, create a zero tensor\n",
    "              if bot0_cards_embedded:\n",
    "                  bot0_cards_embedded = torch.cat(bot0_cards_embedded)\n",
    "              else:\n",
    "                  # Create a zero tensor with the expected shape if bot has no cards\n",
    "                  bot0_cards_embedded = torch.cat((embedding_cards(torch.tensor(0)), embedding_cards(torch.tensor(0))))\n",
    "\n",
    "              if len(bot0_cards_embedded) == 1:\n",
    "                  bot0_cards_embedded = torch.cat((bot0_cards_embedded, embedding_cards(torch.tensor(0))))\n",
    "\n",
    "              # 7. The last action taken (embedded):\n",
    "              last_action = actions_game[-1]\n",
    "              last_action_embedded = embedding_actions(torch.tensor(last_action))  # Keep as a tensor\n",
    "\n",
    "              state = torch.cat(([cards_in_play_embedded,\n",
    "                                  torch.tensor([bot0_coins_normalized]),\n",
    "                                  torch.tensor([avg_other_cards_normalized]),\n",
    "                                  bot0_cards_embedded,\n",
    "                                  last_action_embedded])\n",
    "                                ).type(torch.float32)\n",
    "\n",
    "              state_block = torch.cat([state, torch.tensor([reactions_game[-1]])], 0)\n",
    "              state_challenge = torch.cat([state, torch.tensor([challenges_game[-1]])], 0)\n",
    "              state_card = torch.cat([state, torch.tensor([cards_chosen[-1]])], 0)\n",
    "\n",
    "              # states_temp = torch.cat((states_temp, state.unsqueeze(0)), 0)\n",
    "              # states_summarized = bot.summarizer(states_temp)\n",
    "              states = torch.cat([states, state.unsqueeze(0)], 0)\n",
    "              states_block = torch.cat([states_block, state_block.unsqueeze(0)], 0)\n",
    "              states_challenge = torch.cat([states_challenge, state_challenge.unsqueeze(0)], 0)\n",
    "              states_card = torch.cat([states_card, state_card.unsqueeze(0)], 0)\n",
    "\n",
    "\n",
    "            action_stack = []\n",
    "\n",
    "            action_vector = [0,1,2,3,4,5,6]\n",
    "            for j in action_vector:\n",
    "                if actions[j].p1_net_coins * (-1) > bots[i].num_coins:\n",
    "                    action_vector.remove(j)\n",
    "\n",
    "            # state = None\n",
    "            # state_tensor = None\n",
    "            # if bots[0].name == 0:\n",
    "            #   state = get_state(bots, discard_pile, action_history, reaction_history, challenge_history, card_history, bots[i], network_type=\"action\")\n",
    "            #   state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            #   print(len(state_tensor[0]))\n",
    "\n",
    "            action_selection_output = action_selection(i, bots, action_vector, actions, epsilon, states.unsqueeze(0))\n",
    "\n",
    "            action = action_selection_output[0]\n",
    "\n",
    "            for x in [states, states_block, states_challenge, states_card]:\n",
    "              x[-1][9:12] = torch.tensor(embedding_actions(torch.tensor(action)))\n",
    "\n",
    "            try:\n",
    "              last_state = states[-2]\n",
    "            except:\n",
    "              last_state = None\n",
    "\n",
    "            # state_tuples.append((states[-1], actions[action], rewards[-1], last_state, done[-1]))\n",
    "\n",
    "            if actions_game[-1] == 7:\n",
    "              actions_game[-1] = action\n",
    "\n",
    "            else:\n",
    "              actions_game.append(action)\n",
    "\n",
    "            action_e = actions_emb[action]\n",
    "\n",
    "            action = actions[action]\n",
    "    #         print(action_selection_output[1])\n",
    "\n",
    "            # print(f'bot {bots[i].name} is performing action {action.name}')\n",
    "            # print(f'target is {action_selection_output[1]}')\n",
    "\n",
    "            target = None\n",
    "            reacting_player = 4\n",
    "            challenge = 0\n",
    "            reaction = 0\n",
    "            try:\n",
    "              target = action_selection_output[1]\n",
    "              reacting_player = int(target.name)\n",
    "            except:\n",
    "              target = None\n",
    "              # reacting_player = 4\n",
    "                # print(\"no target\")\n",
    "            # if target is not None:\n",
    "            #     print(f'target is {target.name}')\n",
    "\n",
    "            action_stack.append(action)\n",
    "\n",
    "            if action.response_action is not None and target is None:\n",
    "              target = random.choice(bots)\n",
    "              reacting_player = int(target.name)\n",
    "            reacting_players.append(reacting_player)\n",
    "\n",
    "            # state_reaction = copy.deepcopy(state_action)\n",
    "            # state_reaction.append(action)\n",
    "\n",
    "            # state_challenge = copy.deepcopy(state_action)\n",
    "            # state_challenge.append(action)\n",
    "\n",
    "            # state_card = copy.deepcopy(state_action)\n",
    "            # state_card.append(action)\n",
    "\n",
    "            if action.response_action is not None and action.response_action != 'challenge':  # is blockable?\n",
    "\n",
    "                response, target = reaction_selection(i, bots, target, action.response_action, epsilon, states_block.unsqueeze(0))\n",
    "\n",
    "                # reacting_player = int(target.name)\n",
    "\n",
    "#                 try:\n",
    "#                     print(f'bot {target.name} is considering blocking')\n",
    "#                 except:\n",
    "#                     print(\"no target, check reaction selection\")\n",
    "\n",
    "                if response == 1:\n",
    "\n",
    "                    reaction = 1\n",
    "\n",
    "                    reactions_game.append(1)\n",
    "\n",
    "#                     reacting_players.append(int(target.name))\n",
    "\n",
    "                    action_stack.append(action.response_action)\n",
    "                    # state_challenge.append(reaction)\n",
    "                    # state_card.append(reaction)\n",
    "\n",
    "                    # print(f'bot {target.name} is performing action {action.response_action.name} against bot {bots[i].name}')\n",
    "\n",
    "                else:\n",
    "\n",
    "                    reactions_game.append(0)\n",
    "\n",
    "            else:\n",
    "\n",
    "                reactions_game.append(0)\n",
    "\n",
    "#                     print(f'target will not block')\n",
    "\n",
    "            if action_stack[-1].response_action == 'challenge':  # is challengeable?\n",
    "\n",
    "                response = challenge_selection(epsilon, states_challenge.unsqueeze(0), target if len(action_stack) == 3 else bots[i])\n",
    "\n",
    "                if response == 1:\n",
    "\n",
    "                    challenge = 1\n",
    "\n",
    "                    challenges_game.append(1)\n",
    "\n",
    "                    action_stack.append('challenge')\n",
    "\n",
    "                    if len(action_stack) == 3:\n",
    "                        challenge_dir = 1\n",
    "\n",
    "                    else:\n",
    "                        challenge_dir = 0\n",
    "                    challenges_direction.append(challenge_dir)\n",
    "\n",
    "                else:\n",
    "\n",
    "                    challenges_game.append(0)\n",
    "                    challenges_direction.append(challenge_dir)\n",
    "\n",
    "#                     print('no challenge')\n",
    "\n",
    "            else:\n",
    "\n",
    "                challenges_game.append(0)\n",
    "                challenges_direction.append(2)\n",
    "\n",
    "            # challenges_game.append(0)\n",
    "            # challenges_direction.append(2)\n",
    "\n",
    "            while len(action_stack) != 0:\n",
    "\n",
    "                # state_card.append(challenge)\n",
    "                # state_card.append(challenge_dir)\n",
    "\n",
    "                a = action_stack.pop()\n",
    "                # if a != 'challenge':\n",
    "                #   print(a.name)\n",
    "\n",
    "                if int(bots[0].name) != 0:\n",
    "                  rewards[-1] = -1.0\n",
    "                  done[-1] = 1\n",
    "\n",
    "                if a == 'challenge':\n",
    "\n",
    "                    print('error!')\n",
    "\n",
    "                    if len(action_stack) > 1:\n",
    "\n",
    "                        if influences_reverse[action_stack[-1]] in target.cards:\n",
    "\n",
    "                            # print(f'bot {bots[i].name} has lost the challenge')\n",
    "\n",
    "                            card = 0\n",
    "                            if len(bots[i].cards) > 1:\n",
    "\n",
    "                              card = card_selection(bots[i], bots[i].cards, epsilon, states_card.unsqueeze(0), a)\n",
    "\n",
    "                            # print(card)\n",
    "                            x = bots[i].cards[card]\n",
    "\n",
    "                            discard_pile.append(inf_map[x])\n",
    "\n",
    "                            card_chosen = card\n",
    "                            # card = inf_map.get(card)\n",
    "\n",
    "                            bots[i].cards.remove(x)\n",
    "\n",
    "                            if len(bots[i].cards) == 0:\n",
    "\n",
    "                                print (f'bot {bots[i].name} is out!')\n",
    "\n",
    "                                bots.remove(bots[i])\n",
    "\n",
    "                                i -= 1\n",
    "\n",
    "                            target.cards.remove(influences_reverse[action_stack[-1]])\n",
    "                            bag.insert(influences_reverse[action_stack[-1]])\n",
    "                            random.shuffle(bag)\n",
    "                            c = random.sample(bag, 1)\n",
    "                            bag.remove(c)\n",
    "                            target.cards.insert(c)\n",
    "\n",
    "                            action_stack.clear()\n",
    "\n",
    "                        else:\n",
    "\n",
    "                            # print(f'bot {target.name} has lost the challenge')\n",
    "\n",
    "                            card = 0\n",
    "                            if len(target.cards) > 1:\n",
    "                              card = card_selection(target, target.cards, epsilon, states_card.unsqueeze(0), a)\n",
    "\n",
    "                            # print(card)\n",
    "                            x = target.cards[card]\n",
    "\n",
    "                            discard_pile.append(inf_map[x])\n",
    "\n",
    "                            card_chosen = card\n",
    "\n",
    "                            # card = inf_map.get(card)\n",
    "\n",
    "                            target.cards.remove(x)\n",
    "\n",
    "                            if len(target.cards) == 0:\n",
    "\n",
    "                                bots.remove(target)\n",
    "\n",
    "                                # print (f'bot {target.name} is out!')\n",
    "\n",
    "                            action_stack.pop()\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        if influences_reverse[action_stack[-1]] in bots[i].cards:\n",
    "\n",
    "                            # print(f'bot {target.name} has lost the challenge')\n",
    "\n",
    "                            card = 0\n",
    "                            if len(target.cards) > 1:\n",
    "                              card = card_selection(target, target.cards, epsilon, states_card.unsqueeze(0), a)\n",
    "\n",
    "                            # print(card)\n",
    "                            x = target.cards[card]\n",
    "\n",
    "                            discard_pile.append(inf_map[x])\n",
    "\n",
    "                            card_chosen = card\n",
    "                            # card = inf_map.get(card)\n",
    "\n",
    "                            target.cards.remove(x)\n",
    "\n",
    "                            if len(target.cards) == 0:\n",
    "\n",
    "                                # print (f'bot {target.name} is out!')\n",
    "\n",
    "                                bots.remove(target)\n",
    "\n",
    "                            bots[i].cards.remove(influences_reverse[action_stack[-1]])\n",
    "                            bag.insert(influences_reverse[action_stack[-1]])\n",
    "                            random.shuffle(bag)\n",
    "                            c = random.sample(bag, 1)\n",
    "                            bag.remove(c)\n",
    "                            bots[i].cards.insert(c)\n",
    "\n",
    "                        else:\n",
    "\n",
    "                            # print(f'bot {bots[i].name} has lost the challenge')\n",
    "\n",
    "                            card = 0\n",
    "                            if len(bots[i].cards) > 1:\n",
    "                              card = card_selection(bots[i], bots[i].cards, epsilon, states_card.unsqueeze(0), a)\n",
    "                            # print(card)\n",
    "                            x = bots[i].cards[card]\n",
    "\n",
    "                            discard_pile.append(inf_map[x])\n",
    "\n",
    "                            card_chosen = card\n",
    "                            # card = inf_map.get(card)\n",
    "\n",
    "                            bots[i].cards.remove(x)\n",
    "\n",
    "                            if len(bots[i].cards) == 0:\n",
    "\n",
    "                                # print (f'bot {bots[i].name} is out!')\n",
    "\n",
    "                                bots.remove(bots[i])\n",
    "\n",
    "                                i -= 1\n",
    "\n",
    "                            action_stack.pop()\n",
    "\n",
    "                else:\n",
    "\n",
    "                    # print(f'current action: {a.name}')\n",
    "\n",
    "                    if len(action_stack) == 1:\n",
    "\n",
    "                        target, curr, discard_pile, card_chosen, bag = perform_action(target, curr, a, discard_pile, states_card.unsqueeze(0), card_chosen, epsilon, bag)\n",
    "                        if curr in bots:\n",
    "                            if len(curr.cards) == 0:\n",
    "                                if curr in bots:\n",
    "                                    bots.remove(curr)\n",
    "                                    # print(f'{curr.name} is out!')\n",
    "                                    i -= 1\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        curr, target, discard_pile, card_chosen, bag = perform_action(curr, target, a, discard_pile, states_card.unsqueeze(0), card_chosen, epsilon, bag)\n",
    "                        if target in bots:\n",
    "                            if len(target.cards) == 0:\n",
    "                                if target in bots:\n",
    "                                    bots.remove(target)\n",
    "                                    # print(f'{target.name} is out!')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # print(f'bot {curr.name} has {curr.num_coins} coins.')\n",
    "            # if target is not None:\n",
    "            #     print(f'bot {target.name} has {target.num_coins} coins.')\n",
    "\n",
    "            # print(f'bot {curr.name} has {len(curr.cards)} cards.')\n",
    "            # if target is not None:\n",
    "            #     print(f'bot {target.name} has {len(target.cards)} cards.')\n",
    "\n",
    "\n",
    "            curr_players = [0,0,0,0]\n",
    "            for bot in bots:\n",
    "#                 print(int(bot.name))\n",
    "                curr_players[int(bot.name)] = 1\n",
    "            # print(curr_players)\n",
    "            current_players.append(curr_players)\n",
    "\n",
    "            cards_turn = [[0,0],\n",
    "                          [0,0],\n",
    "                          [0,0],\n",
    "                          [0,0]]\n",
    "            coins_turn = [0,0,0,0]\n",
    "\n",
    "            # if len(bots[0].cards) == 0:\n",
    "            #     cards_turn[0] = [0,0]\n",
    "            # if len(bots[0].cards) == 1:\n",
    "            #     cards_turn[0].append(0)\n",
    "\n",
    "            for bot in bots:\n",
    "\n",
    "                bot_index = int(bot.name)\n",
    "\n",
    "                cards_ind = []\n",
    "\n",
    "                cards_turn[bot_index] = [inf_map[c] for c in bot.cards]\n",
    "\n",
    "                if len(bot.cards) == 0:\n",
    "                    cards_turn[bot_index] = [0, 0]\n",
    "                if len(bot.cards) == 1:\n",
    "                    cards_turn[bot_index].append(0)\n",
    "\n",
    "                coins_turn[bot_index] = bot.num_coins\n",
    "                \n",
    "            for m in range(4):\n",
    "                if curr_players[m] == 0:\n",
    "                    coins_turn[m] == coins_game[-1][m]\n",
    "\n",
    "            cards_game.append(cards_turn)\n",
    "            coins_game.append(coins_turn)\n",
    "\n",
    "            discard_piles.append(discard_pile)\n",
    "\n",
    "            # reacting_players.append(reacting_player)\n",
    "            # reactions_game.append(reaction)\n",
    "            # challenges_game.append(challenge)\n",
    "            # challenges_direction.append(challenge_dir)\n",
    "            cards_chosen.append(card_chosen)\n",
    "\n",
    "            action_history.append(action)\n",
    "            reaction_history.append(reaction)\n",
    "            challenge_history.append(challenge)\n",
    "            card_history.append(card_chosen)\n",
    "            \n",
    "            if int(bots[0].name) == 0:\n",
    "                if coins_game[-1][0] > coins_game[-2][0]:\n",
    "                    rewards[-1] += 0.25\n",
    "                elif coins_game[-1][0] < coins_game[-2][0]:\n",
    "                    if acting_players[-1] != 0:\n",
    "                        rewards[-1] -= 0.25\n",
    "                if cards_game[-1][0].count(0) > cards_game[-2][0].count(0):\n",
    "                    rewards[-1] -= 0.5\n",
    "                for k in range(1, len(cards_game[-1])):\n",
    "                    if cards_game[-1][k].count(0) > cards_game[-2][k].count(0):\n",
    "                        rewards[-1] += 0.5\n",
    "\n",
    "            # print(actions_game[-1])\n",
    "\n",
    "            i += 1\n",
    "            t += 1\n",
    "#             print(cards_turn)\n",
    "#             print(coins_turn)\n",
    "# #             print(curr_players)\n",
    "#             print(discard_pile)\n",
    "\n",
    "\n",
    "    # print(f'bot {bots[0].name} wins!')\n",
    "    acting_players.append(4)\n",
    "    reacting_players.append(4)\n",
    "    actions_game.append(7)\n",
    "    # reactions_game.append(0)\n",
    "    # challenges_game.append(0)\n",
    "    challenges_direction.append(2)\n",
    "    done.append(1)\n",
    "    # rewards = copy.deepcopy(done)\n",
    "    if int(bots[0].name) != 0:\n",
    "      rewards[-1] = -1.0\n",
    "    else:\n",
    "      rewards[-1] = 1.0\n",
    "\n",
    "\n",
    "    # Reset Game\n",
    "\n",
    "    bag = ['Duke', 'Captain', 'Assassin', 'Contessa', 'Ambassador'] * 3\n",
    "    random.shuffle(bag)\n",
    "    # new_bots = []  # Create a new list\n",
    "\n",
    "    for bot in bots_copy:\n",
    "        bot.cards = random.sample(bag, 2)\n",
    "        for card in bot.cards:\n",
    "            bag.remove(card)\n",
    "        bot.num_coins = 2\n",
    "\n",
    "    bots = bots_copy\n",
    "\n",
    "\n",
    "    return discard_piles, acting_players, reacting_players, current_players, actions_game, reactions_game, challenges_game, cards_game, coins_game, challenges_direction, done, rewards, cards_chosen, bots_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "pV7TxFDjnEC9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pV7TxFDjnEC9",
    "outputId": "21ca060d-bbbc-4461-8f51-a1d0b751c684"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "38\n",
      "38\n",
      "38\n",
      "38\n",
      "38\n",
      "38\n",
      "38\n",
      "   acting_players  reacting_players  actions_game  reactions_game  \\\n",
      "0               0                 1             3               0   \n",
      "1               1                 1             2               0   \n",
      "2               2                 3             2               0   \n",
      "\n",
      "   challenges_game  challenges_direction                        cards_game  \\\n",
      "0                0                     2  [[2, 3], [6, 6], [6, 6], [6, 6]]   \n",
      "1                0                     2  [[2, 3], [4, 4], [3, 2], [3, 1]]   \n",
      "2                0                     2  [[2, 3], [4, 4], [3, 2], [3, 1]]   \n",
      "\n",
      "   card_chosen    coins_game  done  rewards  \n",
      "0            0  [2, 2, 2, 2]     0     0.00  \n",
      "1            0  [5, 2, 2, 2]     0     0.25  \n",
      "2            0  [5, 4, 2, 2]     0     0.00  \n",
      "\n",
      "    acting_players  reacting_players  actions_game  reactions_game  \\\n",
      "35               3                 1             1               0   \n",
      "36               0                 3             1               0   \n",
      "37               4                 4             7               0   \n",
      "\n",
      "    challenges_game  challenges_direction                        cards_game  \\\n",
      "35                0                     2  [[3, 0], [4, 0], [0, 0], [1, 0]]   \n",
      "36                0                     2  [[3, 0], [0, 0], [0, 0], [1, 0]]   \n",
      "37                0                     2  [[3, 0], [0, 0], [0, 0], [0, 0]]   \n",
      "\n",
      "    card_chosen     coins_game  done  rewards  \n",
      "35            0  [12, 5, 0, 9]     0      0.0  \n",
      "36            0  [12, 0, 0, 2]     0      0.5  \n",
      "37            0   [5, 0, 0, 0]     1      1.0  \n",
      "\n",
      "    acting_players  reacting_players  actions_game  reactions_game  \\\n",
      "10               2                 3             2               1   \n",
      "11               3                 0             1               1   \n",
      "12               0                 1             1               0   \n",
      "13               1                 2             2               0   \n",
      "14               2                 1             2               0   \n",
      "\n",
      "    challenges_game  challenges_direction                        cards_game  \\\n",
      "10                0                     2  [[2, 3], [4, 4], [3, 2], [3, 1]]   \n",
      "11                0                     2  [[2, 3], [4, 4], [3, 2], [3, 1]]   \n",
      "12                0                     2  [[3, 0], [4, 4], [3, 2], [3, 1]]   \n",
      "13                0                     2  [[3, 0], [4, 0], [3, 2], [3, 1]]   \n",
      "14                0                     2  [[3, 0], [4, 0], [3, 2], [3, 1]]   \n",
      "\n",
      "    card_chosen     coins_game  done  rewards  \n",
      "10            0  [11, 6, 2, 8]     0      0.0  \n",
      "11            0  [11, 6, 2, 8]     0      0.0  \n",
      "12            0  [11, 6, 2, 1]     0     -0.5  \n",
      "13            0   [4, 6, 2, 1]     0      0.5  \n",
      "14            0   [4, 8, 2, 1]     0      0.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dx/zdhgw_j12hv1vs2r2s94qphr0000gn/T/ipykernel_19201/1706364523.py:218: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x[-1][9:12] = torch.tensor(embedding_actions(torch.tensor(action)))\n"
     ]
    }
   ],
   "source": [
    "### Test ###\n",
    "\n",
    "win_rate = 0.0\n",
    "\n",
    "# for i in range(50):\n",
    "\n",
    "discard_piles, acting_players, reacting_players, current_players, actions_game, reactions_game, challenges_game, cards_game, coins_game, challenges_direction, done, rewards, cards_chosen, bots_copy = game_loop_random(bots, actions, influences_reverse, 0.0)\n",
    "\n",
    "bots = bots_copy\n",
    "\n",
    "  # if rewards[-1] == 1.0:\n",
    "  #   win_rate += 1.0\n",
    "\n",
    "# print(win_rate/50)\n",
    "print(len(acting_players))\n",
    "print(len(reacting_players))\n",
    "print(len(actions_game))\n",
    "print(len(reactions_game))\n",
    "print(len(challenges_game))\n",
    "print(len(cards_game))\n",
    "print(len(coins_game))\n",
    "print(len(done))\n",
    "\n",
    "\n",
    "data = {\n",
    "    'acting_players': acting_players,\n",
    "    'reacting_players': reacting_players,\n",
    "    'actions_game': actions_game,\n",
    "    'reactions_game': reactions_game,\n",
    "    'challenges_game': challenges_game,\n",
    "    'challenges_direction': challenges_direction,\n",
    "    'cards_game': cards_game,\n",
    "    'card_chosen': cards_chosen,\n",
    "    'coins_game': coins_game,\n",
    "    'done': done,\n",
    "    'rewards': rewards\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data = data)\n",
    "\n",
    "print(df.head(3))\n",
    "print()\n",
    "print(df.tail(3))\n",
    "print()\n",
    "\n",
    "print(df[10:15])\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be4160d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "id": "be4160d0",
    "outputId": "db4659c9-e96d-492f-f3cf-4c955fcf61b9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os import stat\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "# num_episodes = 10000\n",
    "# max_steps_per_episode = 200\n",
    "# epsilon = 1.0\n",
    "# list_division = 4\n",
    "# gamma = 0.99\n",
    "\n",
    "bot = copy.deepcopy(bots[0])\n",
    "\n",
    "# avg_losses_action = []\n",
    "# avg_losses_block = []\n",
    "# avg_losses_challenge = []\n",
    "# avg_losses_card = []\n",
    "\n",
    "# # bots.remove(bots[-1])\n",
    "# # bots.remove(bots[-1])\n",
    "\n",
    "# win_rates = []\n",
    "# avg_game_lengths = []\n",
    "# bot0_avgs = []\n",
    "# opp_avgs = []\n",
    "\n",
    "# data_fraction = 1/5\n",
    "\n",
    "# batch_size = 64\n",
    "\n",
    "# for episode in range(num_episodes):\n",
    "\n",
    "#   replay_buffer_actions = []\n",
    "#   replay_buffer_blocks = []\n",
    "#   replay_buffer_challenges = []\n",
    "#   replay_buffer_cards = []\n",
    "\n",
    "#   print(f'episode {episode} of {num_episodes}')\n",
    "#   print(f'epsilon: {epsilon}')\n",
    "#   print(f'gamma: {gamma}')\n",
    "\n",
    "#   # discard_piles, acting_players, reacting_players, current_players, actions_game, reactions_game, challenges_game, cards_game, coins_game, challenges_direction, done, rewards, cards_chosen, bots_copy = game_loop_random(bots, actions, influences_reverse, epsilon)\n",
    "#   # bots = bots_copy\n",
    "#   state = torch.empty((0, 12), dtype=torch.float32)  # Assume state_size = 25 for action network\n",
    "#   state_block = torch.empty((0, 13), dtype=torch.float32)\n",
    "#   state_challenge = torch.empty((0, 13), dtype=torch.float32)\n",
    "#   state_card = torch.empty((0, 13), dtype=torch.float32)\n",
    "#   # states_action = torch.empty((0, 24), dtype=torch.float32)  # Assume state_size = 25 for action network\n",
    "#   # next_states_action = torch.empty((0, 24), dtype=torch.float32)\n",
    "#   actions_main = torch.empty((0,), dtype=torch.int64)\n",
    "#   # states_block = torch.empty((0, 23), dtype=torch.float32)  # Assume state_size = 24 for block network\n",
    "#   # next_states_block = torch.empty((0, 23), dtype=torch.float32)\n",
    "#   actions_block = torch.empty((0,), dtype=torch.int64)\n",
    "#   # states_challenge = torch.empty((0, 24), dtype=torch.float32)  # Assume state_size = 25 for challenge network\n",
    "#   # next_states_challenge = torch.empty((0, 24), dtype=torch.float32)\n",
    "#   actions_challenge = torch.empty((0,), dtype=torch.int64)\n",
    "#   # states_card = torch.empty((0, 19), dtype=torch.float32)  # Assume state_size = 20 for card network\n",
    "#   # next_states_card = torch.empty((0, 19), dtype=torch.float32)\n",
    "#   actions_card = torch.empty((0,), dtype=torch.int64)\n",
    "#   rewards = torch.empty((0,), dtype=torch.float32)\n",
    "#   done = torch.empty((0,), dtype=torch.float32)\n",
    "#   game_length_sum = 0\n",
    "#   all_discard_piles = []\n",
    "#   acting_players = []\n",
    "#   reacting_players = []\n",
    "#   reactions_game = []\n",
    "\n",
    "#   num_games = 0\n",
    "\n",
    "\n",
    "#   while len(state) <= 75 * batch_size:\n",
    "#     # print(f'game {num_games}')\n",
    "\n",
    "#     discard_pile, acting_player, reacting_player, current_player, action_game, reaction_game, challenge_game, card_game, coin_game, challenge_direction, done_0, reward, card_chosen, bots_copy = game_loop_random(bots, actions, influences_reverse, epsilon)\n",
    "\n",
    "#     num_games += 1\n",
    "#     # print(f'Game Number {num_games}')\n",
    "\n",
    "#     # if random.random():\n",
    "#     #   print(reacting_player)\n",
    "\n",
    "#     # start_index = int(3 * len(acting_player) / list_division)\n",
    "\n",
    "#     bots = bots_copy\n",
    "\n",
    "#     game_length_sum += len(acting_player)\n",
    "\n",
    "#     split_point = int((1 - data_fraction) * len(acting_player))\n",
    "#     acting_players += acting_player\n",
    "#     reacting_players += reacting_player\n",
    "#     current_players = current_player\n",
    "#     actions_game = action_game\n",
    "#     reactions_game += reaction_game\n",
    "#     challenges_game = challenge_game\n",
    "#     cards_game = card_game\n",
    "#     coins_game = coin_game\n",
    "#     challenges_direction = challenge_direction\n",
    "#     cards_chosen = card_chosen\n",
    "#     discard_piles = discard_pile\n",
    "#     all_discard_piles += discard_pile\n",
    "\n",
    "#     avg_game_lengths.append(game_length_sum / 100)\n",
    "\n",
    "#     # 1. Cards in play (embedded):\n",
    "#     all_cards_in_play_embedded = []\n",
    "\n",
    "#     for current_discard_pile in discard_pile:\n",
    "#         cards_in_play_embedded = []\n",
    "#         for card_name in influences.keys():\n",
    "#             num_in_discard = current_discard_pile.count(inf_map[card_name])\n",
    "#             num_in_play = 3 - num_in_discard\n",
    "#             cards_in_play_embedded.append(torch.tensor(num_in_play)) # Remove .tolist() here\n",
    "\n",
    "#         all_cards_in_play_embedded.append(torch.stack(cards_in_play_embedded)) # Stack the embedded tensors here\n",
    "\n",
    "#     # Convert to a single tensor outside the loop\n",
    "#     all_cards_in_play_embedded = torch.stack(all_cards_in_play_embedded)\n",
    "\n",
    "#     # 4. Bot 0's normalized coins:\n",
    "#     bot0_coins_normalized = torch.tensor(coin_game)[:, 0] / 12  # Get Bot 0's coins and normalize\n",
    "\n",
    "#     # 5. Average cards of other players (normalized and embedded):\n",
    "#     avg_other_cards_normalized = []\n",
    "#     for step_cards in cards_game:\n",
    "#         other_bots_cards = [len([card for card in bot_cards if card != 0])\n",
    "#                           for bot_cards in step_cards[1:]]  # Exclude Bot 0\n",
    "#         avg_other_cards = sum(other_bots_cards) / len(other_bots_cards) if other_bots_cards else 0\n",
    "#         avg_other_cards_normalized.append(avg_other_cards / 2)\n",
    "\n",
    "#     avg_other_cards_normalized = torch.tensor(avg_other_cards_normalized)\n",
    "#     # avg_other_cards_embedded = embedding_cards(torch.tensor(int(avg_other_cards_normalized))).tolist()  # Assuming embedding_cards is your embedding layer\n",
    "\n",
    "#     # 6. Bot 0's current cards (embedded):\n",
    "#     all_bot0_cards_embedded = []  # Store embedded cards for all steps\n",
    "\n",
    "#     for step_cards in cards_game:\n",
    "#         bot0_cards_embedded = []\n",
    "#         for card in step_cards[0]:  # Get Bot 0's cards for this step\n",
    "#             if card != 0:  # Assuming 0 represents the absence of a card\n",
    "#                 bot0_cards_embedded.extend(embedding_cards(torch.tensor(card)).tolist())\n",
    "\n",
    "#         # If Bot 0 has no cards, add zero embeddings for consistency\n",
    "#         while len(bot0_cards_embedded) < embedding_cards.embedding_dim * 2:  # Assuming 2 cards max\n",
    "#             bot0_cards_embedded.extend([0] * embedding_cards.embedding_dim)\n",
    "\n",
    "#         all_bot0_cards_embedded.append(torch.tensor(bot0_cards_embedded))  # Convert to tensor and store\n",
    "\n",
    "#     all_bot0_cards_embedded = torch.stack(all_bot0_cards_embedded) # Stack to create a 2D tensor\n",
    "\n",
    "#     # 7. The last action taken (embedded):\n",
    "#     all_last_action_embedded = []  # Store embedded last actions for all steps\n",
    "\n",
    "#     actions_game.insert(0, 7)  # Add a dummy action at the beginning\n",
    "#     reaction_game.insert(0, 0)  # Add a dummy reaction at the beginning\n",
    "#     challenges_game.insert(0, 0)  # Add a dummy challenge at the beginning\n",
    "\n",
    "#     for i in range(len(actions_game[:-1])):\n",
    "#         last_action = actions_game[i]  # Get the action for the current step\n",
    "#         last_action_embedded = embedding_actions(torch.tensor(last_action)).tolist()\n",
    "#         all_last_action_embedded.append(last_action_embedded)\n",
    "\n",
    "#     all_last_action_embedded = torch.tensor(all_last_action_embedded)  # Convert to a tensor\n",
    "\n",
    "#     new_state = torch.cat(([all_cards_in_play_embedded.unsqueeze(-1),\n",
    "#                         bot0_coins_normalized.unsqueeze(-1).unsqueeze(-1),\n",
    "#                         avg_other_cards_normalized.unsqueeze(-1).unsqueeze(-1),\n",
    "#                         all_bot0_cards_embedded.unsqueeze(-1),\n",
    "#                         all_last_action_embedded.unsqueeze(-1)]),\n",
    "#                       1).squeeze(2)\n",
    "#     state = torch.cat([state, new_state], 0)\n",
    "\n",
    "#     # print(new_state.shape)\n",
    "\n",
    "#     # print(torch.tensor(reactions_game[:-1]).shape)\n",
    "\n",
    "#     new_state_block = torch.cat([new_state, torch.tensor(reaction_game[:-1]).unsqueeze(1)], 1)\n",
    "#     state_block = torch.cat([state_block, new_state_block], 0)\n",
    "\n",
    "#     new_state_challenge = torch.cat([new_state, torch.tensor(challenges_game[1:]).unsqueeze(1)], 1)\n",
    "#     state_challenge = torch.cat([state_challenge, new_state_challenge], 0)\n",
    "\n",
    "#     new_state_card = torch.cat([new_state, torch.tensor(cards_chosen).unsqueeze(1)], 1)\n",
    "#     state_card = torch.cat([state_card, new_state_card], 0)\n",
    "\n",
    "#     new_actions_main = torch.tensor(actions_game[1:]).type(torch.int64)\n",
    "#     actions_main = torch.cat([actions_main, new_actions_main], 0)\n",
    "\n",
    "\n",
    "#     new_actions_block = torch.tensor(reaction_game).type(torch.int64)\n",
    "#     actions_block = torch.cat([actions_block, new_actions_block], 0)\n",
    "\n",
    "\n",
    "#     new_actions_challenge = torch.tensor(challenges_game).type(torch.int64)\n",
    "#     actions_challenge = torch.cat([actions_challenge, new_actions_challenge], 0)\n",
    "\n",
    "\n",
    "#     new_actions_card = torch.tensor(cards_chosen).type(torch.int64)\n",
    "#     actions_card = torch.cat([actions_card, new_actions_card], 0)\n",
    "\n",
    "#     new_rewards = torch.tensor(reward).type(torch.float32)\n",
    "#     rewards = torch.cat([rewards, new_rewards], 0)\n",
    "\n",
    "#     new_done = torch.tensor(done_0).type(torch.float32)\n",
    "#     done = torch.cat([done, new_done], 0)\n",
    "\n",
    "\n",
    "\n",
    "#   print(f'Number of games in episode {episode}: {num_games}')\n",
    "\n",
    "#   print(state.shape)\n",
    "#   print(state_block.shape)\n",
    "#   print(state_challenge.shape)\n",
    "#   print(state_card.shape)\n",
    "\n",
    "#   states_action = torch.empty((0, 12), dtype=torch.float32)\n",
    "#   states_block = torch.empty((0, 13), dtype=torch.float32)\n",
    "#   states_challenge = torch.empty((0, 13), dtype=torch.float32)\n",
    "#   states_card = torch.empty((0, 13), dtype=torch.float32)\n",
    "\n",
    "#   # Assuming you have a list called 'all_states' that contains all the states\n",
    "#   # generated using the 'new_state' calculation you provided\n",
    "#   # and 'acting_players' list that has acting players per state,\n",
    "#   # and 'reacting_players' list for reacting players\n",
    "\n",
    "#   # all_states = []  # Initialize with your existing state generation logic\n",
    "\n",
    "#   # print(state.shape)\n",
    "#   # print(all_bot0_cards_embedded.shape)\n",
    "\n",
    "#   # Create a dictionary to store current states and their corresponding next states\n",
    "#   state_transitions = defaultdict(list)\n",
    "\n",
    "#   for i in range(len(state) - 1):  # Iterate through all states (except the last one)\n",
    "#       # print(next_state[7:9])\n",
    "#       current_state = state[i]\n",
    "#       next_state = state[i + 1]\n",
    "\n",
    "#       # Add the next state to the list of next states for the current state\n",
    "#       state_transitions[tuple(current_state.tolist())].append(next_state)  # Convert to tuple for dictionary key\n",
    "\n",
    "#   state_indices = {}\n",
    "\n",
    "#   for i, state_tensor in enumerate(state):\n",
    "#       state_indices[tuple(state_tensor.tolist())] = i\n",
    "\n",
    "#   for i in range(len(state) - 1):  # Iterate through all states (except the last one)\n",
    "\n",
    "#       current_state_action = state[i]\n",
    "#       current_state_block = state_block[i]\n",
    "#       current_state_challenge = state_challenge[i]\n",
    "#       current_state_card = state_card[i]\n",
    "#       # next_state = state[i + 1]\n",
    "\n",
    "#       # --- Action Network ---\n",
    "#       if acting_players[i] == 0:  # Check acting player for current state\n",
    "#           states_action = torch.cat([states_action, current_state_action.unsqueeze(0)], 0)\n",
    "#       # else:\n",
    "#       #     next_states_action = torch.cat([next_states_action, next_state.unsqueeze(0)], 0)\n",
    "\n",
    "#       # --- Reaction Network & Challenge Network ---\n",
    "#       if reacting_players[i] == 0:  # Bot 0 is the reacting player\n",
    "#           states_block = torch.cat([states_block, current_state_block.unsqueeze(0)], 0)\n",
    "#       # else:\n",
    "#       #     next_states_block = torch.cat([next_states_block, next_state.unsqueeze(0)], 0)\n",
    "\n",
    "#       # --- Challenge Network ---\n",
    "#       if reacting_players[i] == 0:\n",
    "#           states_challenge = torch.cat([states_challenge, current_state_challenge.unsqueeze(0)], 0)\n",
    "#       elif acting_players[i] == 0 and reactions_game[i+1] == 1: # Check acting player for current state\n",
    "#           states_challenge = torch.cat([states_challenge, current_state_challenge.unsqueeze(0)], 0)\n",
    "#       # else:\n",
    "#       #     next_states_challenge = torch.cat([next_states_challenge, next_state.unsqueeze(0)], 0)\n",
    "\n",
    "#       # --- Card Network ---\n",
    "#       # Assuming 'all_discard_piles' contains discard piles for each state\n",
    "#       # and 'all_bot0_cards_embedded' contains Bot 0's cards for each state\n",
    "\n",
    "#       # current_discard_pile_size = len(all_discard_piles[i])\n",
    "#       # next_discard_pile_size = len(all_discard_piles[i + 1])\n",
    "\n",
    "#       # Check if Bot 0 lost a card in the transition\n",
    "#       bot0_current_cards = state_card[i][7:9]\n",
    "#       bot0_next_cards    = state_card[i+1][7:9]\n",
    "#       if not torch.equal(bot0_current_cards, bot0_next_cards):\n",
    "\n",
    "#           states_card = torch.cat([states_card, current_state_card.unsqueeze(0)], 0)\n",
    "#       # else:\n",
    "#       #     next_states_card = torch.cat([next_states_card, next_state.unsqueeze(0)], 0)\n",
    "\n",
    "\n",
    "#   # print(state.shape)\n",
    "\n",
    "#   # print(states_action.shape)\n",
    "#   # print(next_states_action.shape)\n",
    "#   # print(len(states_action) + len(next_states_action))\n",
    "\n",
    "#   # print(states_block.shape)\n",
    "#   # print(next_states_block.shape)\n",
    "#   # print(len(states_block) + len(next_states_block))\n",
    "\n",
    "#   # print(states_challenge.shape)\n",
    "#   # print(next_states_challenge.shape)\n",
    "#   # print(len(states_challenge) + len(next_states_challenge))\n",
    "\n",
    "#   # print(states_card.shape)\n",
    "#   # print(next_states_card.shape)\n",
    "#   # print(len(states_card) + len(next_states_card))\n",
    "\n",
    "\n",
    "#   losses_action = []\n",
    "#   losses_block = []\n",
    "#   losses_challenge = []\n",
    "#   losses_card = []\n",
    "\n",
    "#   ############################################\n",
    "# # Example: On-the-fly building raw sequences\n",
    "# #          and passing them into the RNN Q\n",
    "# ############################################\n",
    "#   num_batches_action = len(states_action) // batch_size\n",
    "\n",
    "#   for i in range(num_batches_action):\n",
    "#     # -------------------------------------------------\n",
    "#     # 1) Sample a batch of indices\n",
    "#     # -------------------------------------------------\n",
    "#     batch_indices_action = random.sample(\n",
    "#         range(len(states_action)),\n",
    "#         min(batch_size, len(states_action))\n",
    "#     )\n",
    "\n",
    "#     batch_states_action = torch.stack([states_action[j] for j in batch_indices_action])\n",
    "#     batch_actions_main  = actions_main[batch_indices_action]  # shape (batch_size,)\n",
    "#     # (Optional) Make sure batch_actions_main is torch.LongTensor\n",
    "#     batch_actions_main  = torch.tensor(batch_actions_main, dtype=torch.long)\n",
    "\n",
    "#     # Lists to store Q-values we compute for each item in this mini-batch\n",
    "#     q_values_current_list = []\n",
    "#     q_values_nextmax_list = []\n",
    "#     rewards_list          = []\n",
    "#     done_list             = []\n",
    "\n",
    "#     # -------------------------------------------------\n",
    "#     # 2) Process each single-frame \"state\" in the batch\n",
    "#     #    to build the raw sequences for current & next\n",
    "#     # -------------------------------------------------\n",
    "#     for idx, single_frame_state in enumerate(batch_states_action):\n",
    "#         # ------------------\n",
    "#         # Find global index\n",
    "#         # ------------------\n",
    "#         indices = torch.where((state == single_frame_state).all(dim=1))[0]\n",
    "#         if len(indices) == 0:\n",
    "#             # We did not find it => treat as terminal\n",
    "#             # => Q(s) is whatever, we can do 0\n",
    "#             # => Or we skip, but let's just do zero and done=1\n",
    "#             q_values_current_list.append(torch.tensor(0.0))  # single float\n",
    "#             q_values_nextmax_list.append(torch.tensor(0.0))\n",
    "#             rewards_list.append(torch.tensor(0.0))\n",
    "#             done_list.append(torch.tensor(1.0))\n",
    "#             continue\n",
    "\n",
    "#         state_index = indices[0].item()\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # (a) Build the \"current\" sequence: from last done to now\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         cur_seq_frames = []\n",
    "#         back_index = state_index\n",
    "#         while back_index >= 0 and done[back_index] != 1:\n",
    "#             cur_seq_frames.insert(0, state[back_index])  # front => chronological\n",
    "#             back_index -= 1\n",
    "#         # shape => (seq_len, 12)\n",
    "#         # try:\n",
    "#         if len(cur_seq_frames) == 0:\n",
    "#             cur_seq_frames.append(state[state_index])\n",
    "#         cur_seq_tensor = torch.stack(cur_seq_frames, dim=0)\n",
    "#         # except:\n",
    "#         #   cur_seq_tensor = torch.stack(single_frame_state, dim=0)\n",
    "#         # shape => (1, seq_len, 12)\n",
    "#         cur_seq_tensor = cur_seq_tensor.unsqueeze(0)\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # (b) Build the \"next\" sequence\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         next_seq_frames = []\n",
    "#         rewards_for_seq = []\n",
    "#         current_index = state_index + 1\n",
    "#         while (current_index < len(state) and acting_players[current_index] != 0):\n",
    "#             next_seq_frames.append(state[current_index])\n",
    "#             rewards_for_seq.append(rewards[current_index])\n",
    "#             if done[current_index] == 1:\n",
    "#                 break\n",
    "#             current_index += 1\n",
    "\n",
    "#         # We will decide how to handle \"no next frames\"\n",
    "#         if len(next_seq_frames) == 0:\n",
    "#             # Means terminal or no next chunk\n",
    "#             next_seq_tensor = None  # we'll handle it below\n",
    "#         else:\n",
    "#             next_seq_tensor = torch.stack(next_seq_frames, dim=0)\n",
    "#             next_seq_tensor = next_seq_tensor.unsqueeze(0)  # shape (1, seq_len2, 12)\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # (c) Forward pass for the current sequence\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # shape => (1, action_dim) if batch_size=1\n",
    "#         out_current = bot.action_q(cur_seq_tensor)\n",
    "#         # We gather the chosen actions Q\n",
    "#         chosen_action = batch_actions_main[idx]  # a single int\n",
    "#         q_val_current = out_current[0, chosen_action]  # shape => scalar\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # (d) Forward pass for the next sequence (if it exists)\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         if next_seq_tensor is None:\n",
    "#             # Terminal. Let's define next_max_Q = 0\n",
    "#             q_val_nextmax = torch.tensor(0.0)\n",
    "#             # Reward from the first step after current_index if in bounds\n",
    "#             if current_index < len(rewards):\n",
    "#                 r = rewards[current_index]\n",
    "#             else:\n",
    "#                 r = 0.0\n",
    "#             done_flag = 1.0\n",
    "#         else:\n",
    "#             out_next = bot.action_q(next_seq_tensor)        # shape => (1, action_dim)\n",
    "#             q_val_nextmax = out_next.max(dim=1)[0].squeeze() # scalar\n",
    "#             # Reward is average of rewards_for_seq\n",
    "#             if len(rewards_for_seq) > 0:\n",
    "#                 r = sum(rewards_for_seq) / len(rewards_for_seq)\n",
    "#             else:\n",
    "#                 r = 0.0\n",
    "#             # Are we done? If in range, check done[current_index]\n",
    "#             if current_index < len(done):\n",
    "#                 done_flag = float(done[current_index])\n",
    "#             else:\n",
    "#                 done_flag = 1.0\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # (e) Collect everything in lists\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         q_values_current_list.append(q_val_current)\n",
    "#         q_values_nextmax_list.append(q_val_nextmax)\n",
    "#         rewards_list.append(torch.tensor(r, dtype=torch.float32))\n",
    "#         done_list.append(torch.tensor(done_flag, dtype=torch.float32))\n",
    "\n",
    "#     # -------------------------------------------------\n",
    "#     # 3) Convert results to Tensors\n",
    "#     #    so we can do a standard DQN loss\n",
    "#     # -------------------------------------------------\n",
    "#     # shape => (batch_size,)\n",
    "#     q_current_t     = torch.stack(q_values_current_list)  # current Q chosen action\n",
    "#     q_nextmax_t     = torch.stack(q_values_nextmax_list)\n",
    "#     rewards_t       = torch.stack(rewards_list)\n",
    "#     done_t          = torch.stack(done_list)\n",
    "\n",
    "#     # DQN target\n",
    "#     target_q = rewards_t + gamma * q_nextmax_t * (1.0 - done_t)\n",
    "\n",
    "#     # -------------------------------------------------\n",
    "#     # 4) Compute loss & backprop\n",
    "#     # -------------------------------------------------\n",
    "#     loss_action = criterion(q_current_t, target_q)\n",
    "#     bot.optimizer_action.zero_grad()\n",
    "#     loss_action.backward()\n",
    "#     bot.optimizer_action.step()\n",
    "\n",
    "#     # -------------------------------------------------\n",
    "#     # 5) Remove used transitions from replay\n",
    "#     #    (Optional, as in your original code)\n",
    "#     # -------------------------------------------------\n",
    "#     states_action_np = states_action.cpu().numpy()\n",
    "#     actions_main_np  = actions_main.cpu().numpy()\n",
    "\n",
    "#     states_action_np = np.delete(states_action_np, batch_indices_action, axis=0)\n",
    "#     actions_main_np  = np.delete(actions_main_np,  batch_indices_action, axis=0)\n",
    "\n",
    "#     states_action = torch.tensor(states_action_np, dtype=torch.float32)\n",
    "#     actions_main  = torch.tensor(actions_main_np,  dtype=torch.int64)\n",
    "\n",
    "#     losses_action.append(loss_action.item())\n",
    "\n",
    "\n",
    "#     # print('action success')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#   ###############################################################################\n",
    "# # RNN-based DQN training loop for your \"block\" decision, without a summarizer\n",
    "# ###############################################################################\n",
    "#   num_batches_block = len(states_block) // batch_size\n",
    "\n",
    "#   for i in range(num_batches_block):\n",
    "#     # 1) Sample a random batch of indices\n",
    "#     batch_indices_block = random.sample(\n",
    "#         range(len(states_block)),\n",
    "#         min(batch_size, len(states_block))\n",
    "#     )\n",
    "\n",
    "#     # 2) Gather Tensors for this mini-batch\n",
    "#     batch_states_block  = torch.stack([states_block[j] for j in batch_indices_block])\n",
    "#     batch_actions_block = actions_block[batch_indices_block]  # shape (batch_size,)\n",
    "#     # Make sure actions are long-int for gather\n",
    "#     batch_actions_block = torch.tensor(batch_actions_block, dtype=torch.long)\n",
    "\n",
    "#     # Lists to store results for each item in the batch\n",
    "#     q_values_current_list = []\n",
    "#     q_values_nextmax_list = []\n",
    "#     rewards_list          = []\n",
    "#     done_list             = []\n",
    "\n",
    "#     # ------------------------------------------------------------------\n",
    "#     # 3) For each single-frame in batch_states_block:\n",
    "#     #    (a) Build \"current\" sequence (backwards)\n",
    "#     #    (b) Build \"next\" sequence (forwards)\n",
    "#     #    (c) Forward pass each through RNN\n",
    "#     #    (d) Collect chosen-action Q, next-max Q, reward, and done\n",
    "#     # ------------------------------------------------------------------\n",
    "#     for idx, single_frame_state in enumerate(batch_states_block):\n",
    "#         # 3.1) Find the global index in your big `state` array\n",
    "#         indices = torch.where((state_block == single_frame_state).all(dim=1))[0]\n",
    "\n",
    "#         if len(indices) == 0:\n",
    "#             # If we didn't find it, treat as terminal\n",
    "#             q_values_current_list.append(torch.tensor(0.0))\n",
    "#             q_values_nextmax_list.append(torch.tensor(0.0))\n",
    "#             rewards_list.append(torch.tensor(0.0))\n",
    "#             done_list.append(torch.tensor(1.0))\n",
    "#             continue\n",
    "\n",
    "#         state_index = indices[0].item()\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # (a) Build \"current\" sequence by going backward until done\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         cur_seq_frames = []\n",
    "#         back_index = state_index\n",
    "#         while back_index >= 0 and done[back_index] != 1:\n",
    "#             cur_seq_frames.insert(0, state_block[back_index])  # front => chronological\n",
    "#             back_index -= 1\n",
    "#         # If we ended up with an empty chunk, fallback to single frame\n",
    "#         if len(cur_seq_frames) == 0:\n",
    "#             cur_seq_frames.append(state_block[state_index])\n",
    "\n",
    "#         # shape: (seq_len, 12)\n",
    "#         cur_seq_tensor = torch.stack(cur_seq_frames, dim=0)\n",
    "#         # shape: (1, seq_len, 12) for the RNN\n",
    "#         cur_seq_tensor = cur_seq_tensor.unsqueeze(0)\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # (b) Build \"next\" sequence by going forward until done or not reacting\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         next_seq_frames = []\n",
    "#         rewards_for_seq = []\n",
    "#         current_idx = state_index + 1\n",
    "\n",
    "#         while (current_idx < len(state) and reacting_players[current_idx] != 0):\n",
    "#             next_seq_frames.append(state_block[current_idx])\n",
    "#             rewards_for_seq.append(rewards[current_idx])\n",
    "#             if done[current_idx] == 1:\n",
    "#                 break\n",
    "#             current_idx += 1\n",
    "\n",
    "#         # We will define \"no next frames\" => terminal\n",
    "#         if len(next_seq_frames) == 0:\n",
    "#             next_seq_tensor = None\n",
    "#         else:\n",
    "#             next_seq_tensor = torch.stack(next_seq_frames, dim=0)\n",
    "#             next_seq_tensor = next_seq_tensor.unsqueeze(0)  # (1, seq_len2, 12)\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # (c) Forward pass: current sequence\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         out_current = bot.block_q(cur_seq_tensor)  # shape => (1, num_actions)\n",
    "#         chosen_action = batch_actions_block[idx]\n",
    "#         q_val_current = out_current[0, chosen_action]  # => scalar\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # (d) Forward pass: next sequence (if exists)\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         if next_seq_tensor is None:\n",
    "#             # Terminal\n",
    "#             q_val_nextmax = torch.tensor(0.0)\n",
    "#             # Reward\n",
    "#             if current_idx < len(rewards):\n",
    "#                 r = rewards[current_idx]\n",
    "#             else:\n",
    "#                 r = 0.0\n",
    "#             done_flag = 1.0\n",
    "#         else:\n",
    "#             out_next = bot.block_q(next_seq_tensor)         # => (1, num_actions)\n",
    "#             q_val_nextmax = out_next.max(dim=1)[0].squeeze()  # => scalar\n",
    "\n",
    "#             # Reward = average (or sum) across the forward chunk\n",
    "#             if len(rewards_for_seq) > 0:\n",
    "#                 r = sum(rewards_for_seq) / len(rewards_for_seq)\n",
    "#             else:\n",
    "#                 r = 0.0\n",
    "\n",
    "#             # Done or not\n",
    "#             if current_idx < len(done):\n",
    "#                 done_flag = float(done[current_idx])\n",
    "#             else:\n",
    "#                 done_flag = 1.0\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # (e) Store in lists\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         q_values_current_list.append(q_val_current)\n",
    "#         q_values_nextmax_list.append(q_val_nextmax)\n",
    "#         rewards_list.append(torch.tensor(r, dtype=torch.float32))\n",
    "#         done_list.append(torch.tensor(done_flag, dtype=torch.float32))\n",
    "\n",
    "#     # ------------------------------------------------------------------\n",
    "#     # 4) Convert the results to Tensors for a standard DQN update\n",
    "#     # ------------------------------------------------------------------\n",
    "#     q_current_t = torch.stack(q_values_current_list)   # shape (batch_size,)\n",
    "#     q_nextmax_t = torch.stack(q_values_nextmax_list)   # shape (batch_size,)\n",
    "#     rewards_t   = torch.stack(rewards_list)            # shape (batch_size,)\n",
    "#     done_t      = torch.stack(done_list)               # shape (batch_size,)\n",
    "\n",
    "#     target_q = rewards_t + gamma * q_nextmax_t * (1.0 - done_t)\n",
    "\n",
    "#     # ------------------------------------------------------------------\n",
    "#     # 5) Compute the loss & backprop\n",
    "#     # ------------------------------------------------------------------\n",
    "#     loss_block = criterion(q_current_t, target_q)\n",
    "#     bot.optimizer_block.zero_grad()\n",
    "#     loss_block.backward()\n",
    "#     bot.optimizer_block.step()\n",
    "\n",
    "#     # ------------------------------------------------------------------\n",
    "#     # 6) Remove these samples from your replay buffer (optional)\n",
    "#     # ------------------------------------------------------------------\n",
    "#     # Convert Tensors to NumPy for deletion\n",
    "#     states_block_np  = states_block.cpu().numpy()\n",
    "#     actions_block_np = actions_block.cpu().numpy()\n",
    "\n",
    "#     states_block_np  = np.delete(states_block_np,  batch_indices_block, axis=0)\n",
    "#     actions_block_np = np.delete(actions_block_np, batch_indices_block, axis=0)\n",
    "\n",
    "#     states_block  = torch.tensor(states_block_np,  dtype=torch.float32)\n",
    "#     actions_block = torch.tensor(actions_block_np, dtype=torch.int64)\n",
    "\n",
    "#     # If you have next_states_block or others, remove them similarly...\n",
    "#     # next_states_block = np.delete(...)\n",
    "\n",
    "#     losses_block.append(loss_block.item())\n",
    "\n",
    "\n",
    "#     # print('block success')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#   ##############################################################################\n",
    "# # RNN-based DQN loop for your \"challenge\" decision, without a separate summarizer\n",
    "# ##############################################################################\n",
    "\n",
    "# #   num_batches_challenge = len(states_challenge) // batch_size\n",
    "\n",
    "# #   for i in range(num_batches_challenge):\n",
    "# #     # 1) Sample a batch of indices\n",
    "# #     batch_indices_challenge = random.sample(\n",
    "# #         range(len(states_challenge)),\n",
    "# #         min(batch_size, len(states_challenge))\n",
    "# #     )\n",
    "\n",
    "# #     # 2) Gather the states & actions for this mini-batch\n",
    "# #     batch_states_challenge  = torch.stack([states_challenge[j] for j in batch_indices_challenge])\n",
    "# #     batch_actions_challenge = actions_challenge[batch_indices_challenge]\n",
    "# #     batch_actions_challenge = torch.tensor(batch_actions_challenge, dtype=torch.long)\n",
    "\n",
    "# #     # Lists to store results for the DQN update\n",
    "# #     q_values_current_list = []\n",
    "# #     q_values_nextmax_list = []\n",
    "# #     rewards_list          = []\n",
    "# #     done_list             = []\n",
    "\n",
    "# #     # ----------------------------------------------------------------\n",
    "# #     # 3) For each single-frame in batch_states_challenge:\n",
    "# #     #    - Build backward-chunk (current)\n",
    "# #     #    - Build forward-chunk (next) per your challenge logic\n",
    "# #     #    - RNN forward pass -> Q-values\n",
    "# #     # ----------------------------------------------------------------\n",
    "# #     for idx, single_frame_state in enumerate(batch_states_challenge):\n",
    "# #         # Try to locate this state in the global `state` array\n",
    "# #         indices = torch.where((state_challenge == single_frame_state).all(dim=1))[0]\n",
    "\n",
    "# #         if len(indices) == 0:\n",
    "# #             # If not found, treat as terminal\n",
    "# #             # Q(current) = 0, Q(next_max) = 0, reward=0, done=1\n",
    "# #             q_values_current_list.append(torch.tensor(0.0))\n",
    "# #             q_values_nextmax_list.append(torch.tensor(0.0))\n",
    "# #             rewards_list.append(torch.tensor(0.0))\n",
    "# #             done_list.append(torch.tensor(1.0))\n",
    "# #             continue\n",
    "\n",
    "# #         state_index = indices[0].item()\n",
    "\n",
    "# #         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# #         # 3A) Build the backward-chunk for the current state\n",
    "# #         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# #         cur_seq_frames = []\n",
    "# #         back_index = state_index\n",
    "# #         # Move backward until we hit done=1 or index < 0\n",
    "# #         # (You could also incorporate more challenge-specific conditions\n",
    "# #         # if you want symmetrical logic with forward-chunk.)\n",
    "# #         while back_index >= 0 and done[back_index] != 1:\n",
    "# #             cur_seq_frames.insert(0, state_challenge[back_index])  # front => chronological order\n",
    "# #             back_index -= 1\n",
    "\n",
    "# #         if len(cur_seq_frames) == 0:\n",
    "# #             # fallback to single frame if we got nothing\n",
    "# #             cur_seq_frames.append(state_challenge[state_index])\n",
    "\n",
    "# #         # Shape: (seq_len, 12)\n",
    "# #         cur_seq_tensor = torch.stack(cur_seq_frames, dim=0)\n",
    "# #         # Shape: (1, seq_len, 12) for the RNN\n",
    "# #         cur_seq_tensor = cur_seq_tensor.unsqueeze(0)\n",
    "\n",
    "# #         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# #         # 3B) Build the forward-chunk for the next state\n",
    "# #         #     Use your \"challenge\" condition:\n",
    "# #         #         reacting_players[idx] != 0\n",
    "# #         #         AND (acting_players[idx] != 0 OR reactions_game[idx] != 1)\n",
    "# #         #         AND done[idx] != 1\n",
    "# #         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# #         next_seq_frames = []\n",
    "# #         rewards_for_seq = []\n",
    "# #         current_idx     = state_index + 1\n",
    "\n",
    "# #         while (\n",
    "# #             current_idx < len(state) and reacting_players[idx] != 0\n",
    "# #                 and (acting_players[idx] != 0 or reactions_game[idx] != 1)):\n",
    "# #             next_seq_frames.append(state_challenge[current_idx])\n",
    "# #             rewards_for_seq.append(rewards[current_idx])\n",
    "# #             if done[current_idx] == 1:\n",
    "# #                 break\n",
    "# #             current_idx += 1\n",
    "\n",
    "# #         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# #         # RNN Forward pass for the \"current\" sequence\n",
    "# #         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# #         out_current = bot.challenge_q(cur_seq_tensor)  # shape (1, num_actions)\n",
    "# #         chosen_action = batch_actions_challenge[idx]\n",
    "# #         q_val_current = out_current[0, chosen_action]   # => scalar\n",
    "\n",
    "# #         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# #         # RNN Forward pass for the \"next\" sequence\n",
    "# #         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# #         q_val_nextmax = torch.tensor(0.0)\n",
    "# #         r = 0.0\n",
    "# #         done_flag = 1.0\n",
    "# #         if len(next_seq_frames) == 0:\n",
    "# #             # Terminal\n",
    "# #             q_val_nextmax = torch.tensor(0.0)\n",
    "# #             # Reward:\n",
    "# #             if current_idx < len(rewards):\n",
    "# #                 r = rewards[current_idx]\n",
    "# #             else:\n",
    "# #                 r = 0.0\n",
    "# #             done_flag = 1.0\n",
    "# #         else:\n",
    "# #             # Non-terminal\n",
    "# #             next_seq_tensor = torch.stack(next_seq_frames, dim=0).unsqueeze(0)\n",
    "# #             out_next        = bot.challenge_q(next_seq_tensor)   # shape (1, num_actions)\n",
    "# #             q_val_nextmax   = out_next.max(dim=1)[0].squeeze()   # => scalar\n",
    "\n",
    "# #             # Reward as average (or adjust logic as you see fit)\n",
    "# #             if len(rewards_for_seq) > 0:\n",
    "# #                 r = sum(rewards_for_seq) / len(rewards_for_seq)\n",
    "# #             else:\n",
    "# #                 r = 0.0\n",
    "\n",
    "# #             if current_idx < len(done):\n",
    "# #                 done_flag = float(done[current_idx])\n",
    "# #             else:\n",
    "# #                 done_flag = 1.0  # out of bounds => terminal\n",
    "\n",
    "# #         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# #         # Collect everything for the DQN update\n",
    "# #         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# #         q_values_current_list.append(q_val_current)\n",
    "# #         q_values_nextmax_list.append(q_val_nextmax)\n",
    "# #         rewards_list.append(torch.tensor(r, dtype=torch.float32))\n",
    "# #         done_list.append(torch.tensor(done_flag, dtype=torch.float32))\n",
    "\n",
    "# #     # ----------------------------------------------------------------\n",
    "# #     # 4) Convert to Tensors & compute DQN loss\n",
    "# #     # ----------------------------------------------------------------\n",
    "# #     q_current_t = torch.stack(q_values_current_list)  # shape (batch_size,)\n",
    "# #     q_nextmax_t = torch.stack(q_values_nextmax_list)  # shape (batch_size,)\n",
    "# #     rewards_t   = torch.stack(rewards_list)           # shape (batch_size,)\n",
    "# #     done_t      = torch.stack(done_list)              # shape (batch_size,)\n",
    "\n",
    "# #     target_q = rewards_t + gamma * q_nextmax_t * (1.0 - done_t)\n",
    "\n",
    "# #     loss_challenge = criterion(q_current_t, target_q)\n",
    "\n",
    "# #     # ----------------------------------------------------------------\n",
    "# #     # 5) Backprop & optimize\n",
    "# #     # ----------------------------------------------------------------\n",
    "# #     bot.optimizer_challenge.zero_grad()\n",
    "# #     loss_challenge.backward()\n",
    "# #     bot.optimizer_challenge.step()\n",
    "\n",
    "# #     # ----------------------------------------------------------------\n",
    "# #     # 6) Remove these samples from your replay buffer\n",
    "# #     # ----------------------------------------------------------------\n",
    "# #     # Convert Tensors -> NumPy for np.delete\n",
    "# #     states_challenge_np  = states_challenge.cpu().numpy()\n",
    "# #     actions_challenge_np = actions_challenge.cpu().numpy()\n",
    "# #     # If you have \"next_states_challenge\" or others, similarly convert them\n",
    "\n",
    "# #     states_challenge_np  = np.delete(states_challenge_np,  batch_indices_challenge, axis=0)\n",
    "# #     actions_challenge_np = np.delete(actions_challenge_np, batch_indices_challenge, axis=0)\n",
    "\n",
    "# #     # Rebuild your PyTorch Tensors\n",
    "# #     states_challenge  = torch.tensor(states_challenge_np,  dtype=torch.float32)\n",
    "# #     actions_challenge = torch.tensor(actions_challenge_np, dtype=torch.int64)\n",
    "\n",
    "# #     # If you store next_states_challenge, remove them as well:\n",
    "# #     # next_states_challenge_np = next_states_challenge.cpu().numpy()\n",
    "# #     # next_states_challenge_np = np.delete(next_states_challenge_np, batch_indices_challenge, axis=0)\n",
    "# #     # next_states_challenge = torch.tensor(next_states_challenge_np, dtype=torch.float32)\n",
    "\n",
    "# #     losses_challenge.append(loss_challenge.item())\n",
    "\n",
    "\n",
    "#     # print('challenge success')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#   ##############################################################################\n",
    "# # RNN-based training loop for your \"card\" decision,\n",
    "# # building raw backward and forward sequences on-the-fly\n",
    "# ##############################################################################\n",
    "#   num_batches_card = len(states_card) // batch_size\n",
    "\n",
    "#   for i in range(num_batches_card):\n",
    "#     if num_batches_card == 0:\n",
    "#       raise Exception(\"Something went wrong\")\n",
    "\n",
    "#     # for i in range(num_batches_card):\n",
    "\n",
    "#     # 1) Sample batch indices\n",
    "#     batch_indices_card = random.sample(\n",
    "#         range(len(states_card)),\n",
    "#         min(batch_size, len(states_card))\n",
    "#     )\n",
    "\n",
    "#     # 2) Gather states & actions for this batch\n",
    "#     batch_states_card  = torch.stack([states_card[j] for j in batch_indices_card])  # (batch_size, 12)\n",
    "#     batch_actions_card = actions_card[batch_indices_card]                           # (batch_size,)\n",
    "#     batch_actions_card = torch.tensor(batch_actions_card, dtype=torch.long)\n",
    "\n",
    "#     # We'll collect Q-values and targets for the entire batch\n",
    "#     q_current_list = []\n",
    "#     q_nextmax_list = []\n",
    "#     reward_list    = []\n",
    "#     done_list      = []\n",
    "\n",
    "#     # ------------------------------------------------------------------------\n",
    "#     # 3) For each single-frame in batch_states_card, build backward & forward\n",
    "#     # ------------------------------------------------------------------------\n",
    "#     for idx, single_frame_state in enumerate(batch_states_card):\n",
    "#         # Locate this state in the global 'state' buffer\n",
    "#         indices = torch.where((state_card == single_frame_state).all(dim=1))[0]\n",
    "#         if len(indices) == 0:\n",
    "#             # Not found => treat as terminal\n",
    "#             # Q(current) = 0, Q(next) = 0, reward=0, done=1\n",
    "#             q_current_list.append(torch.tensor(0.0))\n",
    "#             q_nextmax_list.append(torch.tensor(0.0))\n",
    "#             reward_list.append(torch.tensor(0.0))\n",
    "#             done_list.append(torch.tensor(1.0))\n",
    "#             continue\n",
    "\n",
    "#         state_index = indices[0].item()\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # 3A) BACKWARD chunk for \"current\" state\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         cur_seq_frames = []\n",
    "#         back_idx = state_index\n",
    "\n",
    "#         # Move backward until done=1 or out of array\n",
    "#         while back_idx >= 0 and done[back_idx] != 1:\n",
    "#             cur_seq_frames.insert(0, state_card[back_idx])  # insert at front => chronological order\n",
    "#             back_idx -= 1\n",
    "\n",
    "#         if len(cur_seq_frames) == 0:\n",
    "#             # fallback to just the single frame\n",
    "#             cur_seq_frames.append(state_card[state_index])\n",
    "\n",
    "#         # Turn into shape (1, seq_len, 12) for the RNN\n",
    "#         cur_seq_tensor = torch.stack(cur_seq_frames, dim=0).unsqueeze(0)\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # 3B) FORWARD chunk for \"next\" state, until:\n",
    "#         #     - done=1\n",
    "#         #     - Bot 0's cards change (state[..., 7:9] differs from next step)\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         fwd_seq_frames = []\n",
    "#         rewards_for_seq = []\n",
    "#         current_idx = state_index + 1\n",
    "\n",
    "#         while current_idx < len(state):\n",
    "\n",
    "#             # Append the current frame\n",
    "#             fwd_seq_frames.append(state_card[current_idx])\n",
    "#             rewards_for_seq.append(rewards[current_idx])\n",
    "\n",
    "#             # Check if the *next* step changes Bot 0's cards\n",
    "#             # so we break *after* including the current frame if it\n",
    "#             # leads to a change in [7:9].\n",
    "#             if current_idx + 1 < len(state):\n",
    "#                 # Compare the slice [7:9] of the current vs. the next\n",
    "#                 bot0_current_cards = state_card[current_idx][7:9]\n",
    "#                 bot0_next_cards    = state_card[current_idx+1][7:9]\n",
    "#                 if not torch.equal(bot0_current_cards, bot0_next_cards):\n",
    "#                     # Bot 0's cards changed => break\n",
    "#                     current_idx += 1  # increment so we include the reward\n",
    "#                     break\n",
    "\n",
    "#             # Stop if done=1\n",
    "#             if done[current_idx] == 1:\n",
    "#                 break\n",
    "\n",
    "#             # If no change, keep going\n",
    "#             current_idx += 1\n",
    "\n",
    "#             # Also break if weve run out of array (the while condition checks that anyway)\n",
    "#             # but well rely on the loop condition.\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # Forward pass in the RNN for the \"current\" sequence\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         out_current = bot.card_q(cur_seq_tensor)     # shape: (1, num_actions)\n",
    "#         # pick the Q-value for the chosen action\n",
    "#         chosen_action = batch_actions_card[idx]\n",
    "#         q_val_current = out_current[0, chosen_action] # => scalar\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # Forward pass for the \"next\" sequence\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         if len(fwd_seq_frames) == 0:\n",
    "#             # Terminal if we got no next frames\n",
    "#             q_val_nextmax = torch.tensor(0.0)\n",
    "#             # Reward\n",
    "#             if current_idx < len(state):\n",
    "#                 r = rewards[current_idx]\n",
    "#             else:\n",
    "#                 r = 0.0\n",
    "#             done_flag = 1.0\n",
    "#         else:\n",
    "#             # Non-terminal\n",
    "#             fwd_seq_tensor = torch.stack(fwd_seq_frames, dim=0).unsqueeze(0)\n",
    "#             out_next       = bot.card_q(fwd_seq_tensor)  # shape (1, num_actions)\n",
    "#             q_val_nextmax  = out_next.max(dim=1)[0].squeeze()      # => scalar\n",
    "\n",
    "#             # Could do average or sum of rewards\n",
    "#             if len(rewards_for_seq) > 0:\n",
    "#                 r = sum(rewards_for_seq) / len(rewards_for_seq)\n",
    "#             else:\n",
    "#                 r = 0.0\n",
    "\n",
    "#             # done flag\n",
    "#             if current_idx < len(done):\n",
    "#                 done_flag = float(done[current_idx])\n",
    "#             else:\n",
    "#                 done_flag = 1.0\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # Collect results for the DQN update\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         q_current_list.append(q_val_current)\n",
    "#         q_nextmax_list.append(q_val_nextmax)\n",
    "#         reward_list.append(torch.tensor(r, dtype=torch.float32))\n",
    "#         done_list.append(torch.tensor(done_flag, dtype=torch.float32))\n",
    "\n",
    "#     # ------------------------------------------------------------------\n",
    "#     # 4) Convert to Tensors & compute the DQN target\n",
    "#     # ------------------------------------------------------------------\n",
    "#     q_current_t = torch.stack(q_current_list)   # (batch_size,)\n",
    "#     q_nextmax_t = torch.stack(q_nextmax_list)   # (batch_size,)\n",
    "#     reward_t    = torch.stack(reward_list)      # (batch_size,)\n",
    "#     done_t      = torch.stack(done_list)        # (batch_size,)\n",
    "\n",
    "#     # DQN target: r + gamma * max(Q(next)) * (1 - done)\n",
    "#     target_q = reward_t + gamma * q_nextmax_t * (1.0 - done_t)\n",
    "\n",
    "#     # ------------------------------------------------------------------\n",
    "#     # 5) Compute loss & optimize\n",
    "#     # ------------------------------------------------------------------\n",
    "#     loss_card = criterion(q_current_t, target_q)\n",
    "\n",
    "#     bot.optimizer_card.zero_grad()\n",
    "#     loss_card.backward()\n",
    "#     bot.optimizer_card.step()\n",
    "\n",
    "#     # ------------------------------------------------------------------\n",
    "#     # 6) Remove these samples from the replay buffer\n",
    "#     # ------------------------------------------------------------------\n",
    "#     states_card_np   = states_card.cpu().numpy()\n",
    "#     actions_card_np  = actions_card.cpu().numpy()\n",
    "#     # If you have next_states_card, similarly convert & remove\n",
    "\n",
    "#     states_card_np   = np.delete(states_card_np,  batch_indices_card, axis=0)\n",
    "#     actions_card_np  = np.delete(actions_card_np, batch_indices_card, axis=0)\n",
    "\n",
    "#     states_card  = torch.tensor(states_card_np, dtype=torch.float32)\n",
    "#     actions_card = torch.tensor(actions_card_np, dtype=torch.int64)\n",
    "\n",
    "#     losses_card.append(loss_card.item())\n",
    "\n",
    "\n",
    "\n",
    "#   # bot.cards = bots[0].cards\n",
    "#   # bot.num_coins = bots[0].num_coins\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # i += 1\n",
    "#   epsilon *= 0.9995\n",
    "\n",
    "#   # if (episode + 1) % 100 == 0:\n",
    "#   #   data_fraction = min(data_fraction + 1/5, 1)\n",
    "#   #   epsilon = 1.0\n",
    "\n",
    "#   # gamma = min(0.99, gamma + 0.001)\n",
    "\n",
    "#   avg_losses_action.append(sum(losses_action) / len(losses_action))\n",
    "#   avg_losses_block.append(sum(losses_block) / len(losses_block))\n",
    "# #   avg_losses_challenge.append(sum(losses_challenge) / len(losses_challenge))\n",
    "#   avg_losses_card.append(sum(losses_card) / len(losses_card))\n",
    "\n",
    "#   print(f'Avg Action Loss, {num_batches_action} batches: {avg_losses_action[-1]}')\n",
    "#   print(f'Avg Block Loss, {num_batches_block} batches: {avg_losses_block[-1]}')\n",
    "# #   print(f'Avg Challenge Loss, {num_batches_challenge} batches: {avg_losses_challenge[-1]}')\n",
    "#   print(f'Avg Card Loss, {num_batches_card} batches: {avg_losses_card[-1]}')\n",
    "\n",
    "#   # Copy parameters of action_q network\n",
    "#   bots[0].action_q.load_state_dict(bot.action_q.state_dict())\n",
    "\n",
    "#   # Copy parameters of block_q network\n",
    "#   bots[0].block_q.load_state_dict(bot.block_q.state_dict())\n",
    "\n",
    "#   # Copy parameters of challenge_q network\n",
    "# #   bots[0].challenge_q.load_state_dict(bot.challenge_q.state_dict())\n",
    "\n",
    "#   # Copy parameters of card_q network\n",
    "#   bots[0].card_q.load_state_dict(bot.card_q.state_dict())\n",
    "\n",
    "#   def verify_allclose(net_a, net_b, eps=1e-6):\n",
    "#     for p_a, p_b in zip(net_a.parameters(), net_b.parameters()):\n",
    "#         if not torch.allclose(p_a, p_b, atol=eps, rtol=1e-5):\n",
    "#             return False\n",
    "#     return True\n",
    "\n",
    "#   print(\"Action Q matches?\",\n",
    "#         verify_allclose(bots[0].action_q, bot.action_q))\n",
    "#   print(\"Block Q matches?\",\n",
    "#         verify_allclose(bots[0].block_q, bot.block_q))\n",
    "# #   print(\"Challenge Q matches?\",\n",
    "# #         verify_allclose(bots[0].challenge_q, bot.challenge_q))\n",
    "#   print(\"Card Q matches?\",\n",
    "#         verify_allclose(bots[0].card_q, bot.card_q))\n",
    "\n",
    "\n",
    "#   print(bots[0].name)\n",
    "\n",
    "#   if episode % 10 == 0:\n",
    "\n",
    "#     win_rate = 0\n",
    "\n",
    "#     bot0_actions = []\n",
    "\n",
    "#     game_lengths = 0\n",
    "    \n",
    "#     coins_game = []\n",
    "#     dones_eval = []\n",
    "\n",
    "#     for i in range(100):\n",
    "\n",
    "#         discard_pile, acting_player, reacting_player, current_player, action_game, reaction_game, challenge_game, card_game, coin_game, challenge_direction, done_0, reward, card_chosen, bots_copy = game_loop_random(bots, actions, influences_reverse, 0.0)\n",
    "#         bots = bots_copy\n",
    "\n",
    "#         game_lengths += len(action_game)\n",
    "\n",
    "#         for actor, action in zip(acting_player, action_game):\n",
    "#             if actor == 0:\n",
    "#                 bot0_actions.append(action)\n",
    "\n",
    "#         if reward[-1] == 1.0:\n",
    "#             win_rate += 1\n",
    "            \n",
    "#         coins_game += coin_game\n",
    "#         dones_eval += done_0\n",
    "                \n",
    "\n",
    "#     if bot0_actions:  # ensure the list is not empty\n",
    "#         counter = Counter(bot0_actions)\n",
    "#         most_common_action, count = counter.most_common(1)[0]\n",
    "#         print(f'Most common action for Bot 0: {most_common_action} taken {count} times.')\n",
    "#         print(f'total game lengths: {game_lengths}')\n",
    "    \n",
    "#     bot0_coin_counts = []\n",
    "#     opp_coin_counts = []\n",
    "#     done_flag = 0\n",
    "#     for c in range(len(dones_eval) - 1):\n",
    "#         if dones_eval[c] == 1 and done_flag == 0:\n",
    "#             done_flag = 1\n",
    "#             bot0_coin_counts.append(coins_game[c][0])\n",
    "#             opp_coin_counts.append(coins_game[c][1])\n",
    "#         elif dones_eval[c] == 0: \n",
    "#             done_flag = 0\n",
    "\n",
    "#     win_rate = win_rate / 100\n",
    "#     win_rates.append(win_rate)\n",
    "\n",
    "#     print(f'win rate: {win_rate}')\n",
    "    \n",
    "#     bot0_avg = np.mean(bot0_coin_counts)\n",
    "#     opp_avg = np.mean(opp_coin_counts)\n",
    "    \n",
    "#     bot0_avgs.append(bot0_avg)\n",
    "#     opp_avgs.append(opp_avg)\n",
    "    \n",
    "#     print(f'bot 0 average final coin count: {bot0_avg}')\n",
    "#     print(f'opponent average final coin count: {opp_avg}')\n",
    "\n",
    "\n",
    "#   # df = pd.DataFrame(data = data)\n",
    "#   # print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "Dky8U78hbAps",
   "metadata": {
    "id": "Dky8U78hbAps"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(avg_losses_action, label='Avg Action Loss')\n",
    "# plt.plot(avg_losses_block, label='Avg Block Loss')\n",
    "# # plt.plot(avg_losses_challenge, label='Avg Challenge Loss')\n",
    "# plt.plot(avg_losses_card, label='Avg Card Loss')\n",
    "\n",
    "# plt.xlabel('Episode/Iteration')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Training Losses')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "vKOENygAg-jY",
   "metadata": {
    "id": "vKOENygAg-jY"
   },
   "outputs": [],
   "source": [
    "# plt.plot(win_rates, label='Win Rate')\n",
    "\n",
    "# plt.xlabel('Episode/Iteration')\n",
    "# plt.ylabel('Win Rate')\n",
    "# plt.title('Win Rate over Time')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "mQYFQF_TiYhE",
   "metadata": {
    "id": "mQYFQF_TiYhE"
   },
   "outputs": [],
   "source": [
    "# plt.plot(avg_game_lengths, label='Avg Game Lengths')\n",
    "\n",
    "# plt.xlabel('Episode/Iteration')\n",
    "# plt.ylabel('Avg Game Length (timesteps)')\n",
    "# plt.title('Avg Game Length over Time')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "RttgbXIgzFfB",
   "metadata": {
    "id": "RttgbXIgzFfB"
   },
   "outputs": [],
   "source": [
    "# torch.save({\n",
    "#         'action_q_state_dict': bot.action_q.state_dict(),\n",
    "#         'block_q_state_dict': bot.block_q.state_dict(),\n",
    "#         'challenge_q_state_dict': bot.challenge_q.state_dict(),\n",
    "#         'card_q_state_dict': bot.card_q.state_dict(),\n",
    "#         'optimizer_action_state_dict': bot.optimizer_action.state_dict(),\n",
    "#         'optimizer_block_state_dict': bot.optimizer_block.state_dict(),\n",
    "# #         'optimizer_challenge_state_dict': bot.optimizer_challenge.state_dict(),\n",
    "#         'optimizer_card_state_dict': bot.optimizer_card.state_dict()\n",
    "#     }, 'bot_parameters.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "wh_pxHPuzNGM",
   "metadata": {
    "id": "wh_pxHPuzNGM"
   },
   "outputs": [],
   "source": [
    "# checkpoint = torch.load('bot_parameters.pth')\n",
    "# bot.action_q.load_state_dict(checkpoint['action_q_state_dict'])\n",
    "# bot.block_q.load_state_dict(checkpoint['block_q_state_dict'])\n",
    "# # bot.challenge_q.load_state_dict(checkpoint['challenge_q_state_dict'])\n",
    "# bot.card_q.load_state_dict(checkpoint['card_q_state_dict'])\n",
    "# bot.optimizer_action.load_state_dict(checkpoint['optimizer_action_state_dict'])\n",
    "# bot.optimizer_block.load_state_dict(checkpoint['optimizer_block_state_dict'])\n",
    "# # bot.optimizer_challenge.load_state_dict(checkpoint['optimizer_challenge_state_dict'])\n",
    "# bot.optimizer_card.load_state_dict(checkpoint['optimizer_card_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "Q4YC59Kfkn0B",
   "metadata": {
    "id": "Q4YC59Kfkn0B"
   },
   "outputs": [],
   "source": [
    "# import itertools\n",
    "\n",
    "# bots[0].action_q.load_state_dict(bot.action_q.state_dict())\n",
    "\n",
    "# # Copy parameters of block_q network\n",
    "# bots[0].block_q.load_state_dict(bot.block_q.state_dict())\n",
    "\n",
    "# # Copy parameters of challenge_q network\n",
    "# # bots[0].challenge_q.load_state_dict(bot.challenge_q.state_dict())\n",
    "\n",
    "# # Copy parameters of card_q network\n",
    "# bots[0].card_q.load_state_dict(bot.card_q.state_dict())\n",
    "\n",
    "\n",
    "# win_rate = 0\n",
    "\n",
    "# card_losses_counts = {1: 0,\n",
    "#                      2: 0,\n",
    "#                      3: 0,\n",
    "#                      4: 0,\n",
    "#                      5: 0}\n",
    "\n",
    "# for i in range(100):\n",
    "\n",
    "#     # print(i)\n",
    "#     discard_pile, acting_player, reacting_player, current_player, action_game, reaction_game, challenge_game, card_game, coin_game, challenge_direction, done_0, reward, card_chosen, bots_copy = game_loop_random(bots, actions, influences_reverse, 0.0)\n",
    "#     bots = bots_copy\n",
    "#     if reward[-1] == 1:\n",
    "#         win_rate += 1\n",
    "#     else:\n",
    "#         for card in itertools.chain.from_iterable(card_game[-1]):\n",
    "#             if card in card_losses_counts:      # skips 0 / None / out-of-range\n",
    "#                 card_losses_counts[card] += 1\n",
    "            \n",
    "\n",
    "# # win_rate = win_rate / 50\n",
    "# print(f'Bot 0 Win Rate, Random Actions: {win_rate / 100}')\n",
    "# print(card_losses_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8153fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for n in bots[1:]:\n",
    "#     n.cards[0] = 'Duke'\n",
    "\n",
    "# print(bot_x.cards for bot_x in bots)\n",
    "\n",
    "# num_episodes = 100\n",
    "# max_steps_per_episode = 200\n",
    "# epsilon = 1.0\n",
    "# list_division = 4\n",
    "# gamma = 0.99\n",
    "\n",
    "# avg_losses_action = []\n",
    "# avg_losses_block = []\n",
    "# avg_losses_challenge = []\n",
    "# avg_losses_card = []\n",
    "\n",
    "# # bots.remove(bots[-1])\n",
    "# # bots.remove(bots[-1])\n",
    "\n",
    "# win_rates = []\n",
    "# avg_game_lengths = []\n",
    "# bot0_avgs = []\n",
    "# opp_avgs = []\n",
    "\n",
    "# data_fraction = 1/5\n",
    "\n",
    "# batch_size = 64\n",
    "\n",
    "# for episode in range(num_episodes):\n",
    "\n",
    "#   replay_buffer_actions = []\n",
    "#   replay_buffer_blocks = []\n",
    "#   replay_buffer_challenges = []\n",
    "#   replay_buffer_cards = []\n",
    "\n",
    "#   print(f'episode {episode} of {num_episodes}')\n",
    "#   print(f'epsilon: {epsilon}')\n",
    "#   print(f'gamma: {gamma}')\n",
    "\n",
    "#   # discard_piles, acting_players, reacting_players, current_players, actions_game, reactions_game, challenges_game, cards_game, coins_game, challenges_direction, done, rewards, cards_chosen, bots_copy = game_loop_random(bots, actions, influences_reverse, epsilon)\n",
    "#   # bots = bots_copy\n",
    "#   state = torch.empty((0, 12), dtype=torch.float32)  # Assume state_size = 25 for action network\n",
    "#   state_block = torch.empty((0, 13), dtype=torch.float32)\n",
    "#   state_challenge = torch.empty((0, 13), dtype=torch.float32)\n",
    "#   state_card = torch.empty((0, 13), dtype=torch.float32)\n",
    "#   # states_action = torch.empty((0, 24), dtype=torch.float32)  # Assume state_size = 25 for action network\n",
    "#   # next_states_action = torch.empty((0, 24), dtype=torch.float32)\n",
    "#   actions_main = torch.empty((0,), dtype=torch.int64)\n",
    "#   # states_block = torch.empty((0, 23), dtype=torch.float32)  # Assume state_size = 24 for block network\n",
    "#   # next_states_block = torch.empty((0, 23), dtype=torch.float32)\n",
    "#   actions_block = torch.empty((0,), dtype=torch.int64)\n",
    "#   # states_challenge = torch.empty((0, 24), dtype=torch.float32)  # Assume state_size = 25 for challenge network\n",
    "#   # next_states_challenge = torch.empty((0, 24), dtype=torch.float32)\n",
    "#   actions_challenge = torch.empty((0,), dtype=torch.int64)\n",
    "#   # states_card = torch.empty((0, 19), dtype=torch.float32)  # Assume state_size = 20 for card network\n",
    "#   # next_states_card = torch.empty((0, 19), dtype=torch.float32)\n",
    "#   actions_card = torch.empty((0,), dtype=torch.int64)\n",
    "#   rewards = torch.empty((0,), dtype=torch.float32)\n",
    "#   done = torch.empty((0,), dtype=torch.float32)\n",
    "#   game_length_sum = 0\n",
    "#   all_discard_piles = []\n",
    "#   acting_players = []\n",
    "#   reacting_players = []\n",
    "#   reactions_game = []\n",
    "\n",
    "#   num_games = 0\n",
    "\n",
    "\n",
    "#   while len(state) <= 75 * batch_size:\n",
    "#     # print(f'game {num_games}')\n",
    "\n",
    "#     discard_pile, acting_player, reacting_player, current_player, action_game, reaction_game, challenge_game, card_game, coin_game, challenge_direction, done_0, reward, card_chosen, bots_copy = game_loop_random(bots, actions, influences_reverse, epsilon)\n",
    "\n",
    "#     num_games += 1\n",
    "#     # print(f'Game Number {num_games}')\n",
    "\n",
    "#     # if random.random():\n",
    "#     #   print(reacting_player)\n",
    "\n",
    "#     # start_index = int(3 * len(acting_player) / list_division)\n",
    "\n",
    "#     bots = bots_copy\n",
    "#     bots[1].cards[0] = 'Duke'\n",
    "\n",
    "#     game_length_sum += len(acting_player)\n",
    "\n",
    "#     split_point = int((1 - data_fraction) * len(acting_player))\n",
    "#     acting_players += acting_player\n",
    "#     reacting_players += reacting_player\n",
    "#     current_players = current_player\n",
    "#     actions_game = action_game\n",
    "#     reactions_game += reaction_game\n",
    "#     challenges_game = challenge_game\n",
    "#     cards_game = card_game\n",
    "#     coins_game = coin_game\n",
    "#     challenges_direction = challenge_direction\n",
    "#     cards_chosen = card_chosen\n",
    "#     discard_piles = discard_pile\n",
    "#     all_discard_piles += discard_pile\n",
    "\n",
    "#     avg_game_lengths.append(game_length_sum / 100)\n",
    "\n",
    "#     # 1. Cards in play (embedded):\n",
    "#     all_cards_in_play_embedded = []\n",
    "\n",
    "#     for current_discard_pile in discard_pile:\n",
    "#         cards_in_play_embedded = []\n",
    "#         for card_name in influences.keys():\n",
    "#             num_in_discard = current_discard_pile.count(inf_map[card_name])\n",
    "#             num_in_play = 3 - num_in_discard\n",
    "#             cards_in_play_embedded.append(torch.tensor(num_in_play)) # Remove .tolist() here\n",
    "\n",
    "#         all_cards_in_play_embedded.append(torch.stack(cards_in_play_embedded)) # Stack the embedded tensors here\n",
    "\n",
    "#     # Convert to a single tensor outside the loop\n",
    "#     all_cards_in_play_embedded = torch.stack(all_cards_in_play_embedded)\n",
    "\n",
    "#     # 4. Bot 0's normalized coins:\n",
    "#     bot0_coins_normalized = torch.tensor(coin_game)[:, 0] / 12  # Get Bot 0's coins and normalize\n",
    "\n",
    "#     # 5. Average cards of other players (normalized and embedded):\n",
    "#     avg_other_cards_normalized = []\n",
    "#     for step_cards in cards_game:\n",
    "#         other_bots_cards = [len([card for card in bot_cards if card != 0])\n",
    "#                           for bot_cards in step_cards[1:]]  # Exclude Bot 0\n",
    "#         avg_other_cards = sum(other_bots_cards) / len(other_bots_cards) if other_bots_cards else 0\n",
    "#         avg_other_cards_normalized.append(avg_other_cards / 2)\n",
    "\n",
    "#     avg_other_cards_normalized = torch.tensor(avg_other_cards_normalized)\n",
    "#     # avg_other_cards_embedded = embedding_cards(torch.tensor(int(avg_other_cards_normalized))).tolist()  # Assuming embedding_cards is your embedding layer\n",
    "\n",
    "#     # 6. Bot 0's current cards (embedded):\n",
    "#     all_bot0_cards_embedded = []  # Store embedded cards for all steps\n",
    "\n",
    "#     for step_cards in cards_game:\n",
    "#         bot0_cards_embedded = []\n",
    "#         for card in step_cards[0]:  # Get Bot 0's cards for this step\n",
    "#             if card != 0:  # Assuming 0 represents the absence of a card\n",
    "#                 bot0_cards_embedded.extend(embedding_cards(torch.tensor(card)).tolist())\n",
    "\n",
    "#         # If Bot 0 has no cards, add zero embeddings for consistency\n",
    "#         while len(bot0_cards_embedded) < embedding_cards.embedding_dim * 2:  # Assuming 2 cards max\n",
    "#             bot0_cards_embedded.extend([0] * embedding_cards.embedding_dim)\n",
    "\n",
    "#         all_bot0_cards_embedded.append(torch.tensor(bot0_cards_embedded))  # Convert to tensor and store\n",
    "\n",
    "#     all_bot0_cards_embedded = torch.stack(all_bot0_cards_embedded) # Stack to create a 2D tensor\n",
    "\n",
    "#     # 7. The last action taken (embedded):\n",
    "#     all_last_action_embedded = []  # Store embedded last actions for all steps\n",
    "\n",
    "#     actions_game.insert(0, 7)  # Add a dummy action at the beginning\n",
    "#     reaction_game.insert(0, 0)  # Add a dummy reaction at the beginning\n",
    "#     challenges_game.insert(0, 0)  # Add a dummy challenge at the beginning\n",
    "\n",
    "#     for i in range(len(actions_game[:-1])):\n",
    "#         last_action = actions_game[i]  # Get the action for the current step\n",
    "#         last_action_embedded = embedding_actions(torch.tensor(last_action)).tolist()\n",
    "#         all_last_action_embedded.append(last_action_embedded)\n",
    "\n",
    "#     all_last_action_embedded = torch.tensor(all_last_action_embedded)  # Convert to a tensor\n",
    "\n",
    "#     new_state = torch.cat(([all_cards_in_play_embedded.unsqueeze(-1),\n",
    "#                         bot0_coins_normalized.unsqueeze(-1).unsqueeze(-1),\n",
    "#                         avg_other_cards_normalized.unsqueeze(-1).unsqueeze(-1),\n",
    "#                         all_bot0_cards_embedded.unsqueeze(-1),\n",
    "#                         all_last_action_embedded.unsqueeze(-1)]),\n",
    "#                       1).squeeze(2)\n",
    "#     state = torch.cat([state, new_state], 0)\n",
    "\n",
    "#     # print(new_state.shape)\n",
    "\n",
    "#     # print(torch.tensor(reactions_game[:-1]).shape)\n",
    "\n",
    "#     new_state_block = torch.cat([new_state, torch.tensor(reaction_game[:-1]).unsqueeze(1)], 1)\n",
    "#     state_block = torch.cat([state_block, new_state_block], 0)\n",
    "\n",
    "#     new_state_challenge = torch.cat([new_state, torch.tensor(challenges_game[1:]).unsqueeze(1)], 1)\n",
    "#     state_challenge = torch.cat([state_challenge, new_state_challenge], 0)\n",
    "\n",
    "#     new_state_card = torch.cat([new_state, torch.tensor(cards_chosen).unsqueeze(1)], 1)\n",
    "#     state_card = torch.cat([state_card, new_state_card], 0)\n",
    "\n",
    "#     new_actions_main = torch.tensor(actions_game[1:]).type(torch.int64)\n",
    "#     actions_main = torch.cat([actions_main, new_actions_main], 0)\n",
    "\n",
    "\n",
    "#     new_actions_block = torch.tensor(reaction_game).type(torch.int64)\n",
    "#     actions_block = torch.cat([actions_block, new_actions_block], 0)\n",
    "\n",
    "\n",
    "#     new_actions_challenge = torch.tensor(challenges_game).type(torch.int64)\n",
    "#     actions_challenge = torch.cat([actions_challenge, new_actions_challenge], 0)\n",
    "\n",
    "\n",
    "#     new_actions_card = torch.tensor(cards_chosen).type(torch.int64)\n",
    "#     actions_card = torch.cat([actions_card, new_actions_card], 0)\n",
    "\n",
    "#     new_rewards = torch.tensor(reward).type(torch.float32)\n",
    "#     rewards = torch.cat([rewards, new_rewards], 0)\n",
    "\n",
    "#     new_done = torch.tensor(done_0).type(torch.float32)\n",
    "#     done = torch.cat([done, new_done], 0)\n",
    "\n",
    "\n",
    "\n",
    "#   print(f'Number of games in episode {episode}: {num_games}')\n",
    "\n",
    "#   print(state.shape)\n",
    "#   print(state_block.shape)\n",
    "#   print(state_challenge.shape)\n",
    "#   print(state_card.shape)\n",
    "\n",
    "#   states_action = torch.empty((0, 12), dtype=torch.float32)\n",
    "#   states_block = torch.empty((0, 13), dtype=torch.float32)\n",
    "#   states_challenge = torch.empty((0, 13), dtype=torch.float32)\n",
    "#   states_card = torch.empty((0, 13), dtype=torch.float32)\n",
    "\n",
    "#   # Assuming you have a list called 'all_states' that contains all the states\n",
    "#   # generated using the 'new_state' calculation you provided\n",
    "#   # and 'acting_players' list that has acting players per state,\n",
    "#   # and 'reacting_players' list for reacting players\n",
    "\n",
    "#   # all_states = []  # Initialize with your existing state generation logic\n",
    "\n",
    "#   # print(state.shape)\n",
    "#   # print(all_bot0_cards_embedded.shape)\n",
    "\n",
    "#   # Create a dictionary to store current states and their corresponding next states\n",
    "#   state_transitions = defaultdict(list)\n",
    "\n",
    "#   for i in range(len(state) - 1):  # Iterate through all states (except the last one)\n",
    "#       # print(next_state[7:9])\n",
    "#       current_state = state[i]\n",
    "#       next_state = state[i + 1]\n",
    "\n",
    "#       # Add the next state to the list of next states for the current state\n",
    "#       state_transitions[tuple(current_state.tolist())].append(next_state)  # Convert to tuple for dictionary key\n",
    "\n",
    "#   state_indices = {}\n",
    "\n",
    "#   for i, state_tensor in enumerate(state):\n",
    "#       state_indices[tuple(state_tensor.tolist())] = i\n",
    "\n",
    "#   for i in range(len(state) - 1):  # Iterate through all states (except the last one)\n",
    "\n",
    "#       current_state_action = state[i]\n",
    "#       current_state_block = state_block[i]\n",
    "#       current_state_challenge = state_challenge[i]\n",
    "#       current_state_card = state_card[i]\n",
    "#       # next_state = state[i + 1]\n",
    "\n",
    "#       # --- Action Network ---\n",
    "#       if acting_players[i] == 0:  # Check acting player for current state\n",
    "#           states_action = torch.cat([states_action, current_state_action.unsqueeze(0)], 0)\n",
    "#       # else:\n",
    "#       #     next_states_action = torch.cat([next_states_action, next_state.unsqueeze(0)], 0)\n",
    "\n",
    "#       # --- Reaction Network & Challenge Network ---\n",
    "#       if reacting_players[i] == 0:  # Bot 0 is the reacting player\n",
    "#           states_block = torch.cat([states_block, current_state_block.unsqueeze(0)], 0)\n",
    "#       # else:\n",
    "#       #     next_states_block = torch.cat([next_states_block, next_state.unsqueeze(0)], 0)\n",
    "\n",
    "#       # --- Challenge Network ---\n",
    "#       if reacting_players[i] == 0:\n",
    "#           states_challenge = torch.cat([states_challenge, current_state_challenge.unsqueeze(0)], 0)\n",
    "#       elif acting_players[i] == 0 and reactions_game[i+1] == 1: # Check acting player for current state\n",
    "#           states_challenge = torch.cat([states_challenge, current_state_challenge.unsqueeze(0)], 0)\n",
    "#       # else:\n",
    "#       #     next_states_challenge = torch.cat([next_states_challenge, next_state.unsqueeze(0)], 0)\n",
    "\n",
    "#       # --- Card Network ---\n",
    "#       # Assuming 'all_discard_piles' contains discard piles for each state\n",
    "#       # and 'all_bot0_cards_embedded' contains Bot 0's cards for each state\n",
    "\n",
    "#       # current_discard_pile_size = len(all_discard_piles[i])\n",
    "#       # next_discard_pile_size = len(all_discard_piles[i + 1])\n",
    "\n",
    "#       # Check if Bot 0 lost a card in the transition\n",
    "#       bot0_current_cards = state_card[i][7:9]\n",
    "#       bot0_next_cards    = state_card[i+1][7:9]\n",
    "#       if not torch.equal(bot0_current_cards, bot0_next_cards):\n",
    "\n",
    "#           states_card = torch.cat([states_card, current_state_card.unsqueeze(0)], 0)\n",
    "#       # else:\n",
    "#       #     next_states_card = torch.cat([next_states_card, next_state.unsqueeze(0)], 0)\n",
    "\n",
    "\n",
    "#   # print(state.shape)\n",
    "\n",
    "#   # print(states_action.shape)\n",
    "#   # print(next_states_action.shape)\n",
    "#   # print(len(states_action) + len(next_states_action))\n",
    "\n",
    "#   # print(states_block.shape)\n",
    "#   # print(next_states_block.shape)\n",
    "#   # print(len(states_block) + len(next_states_block))\n",
    "\n",
    "#   # print(states_challenge.shape)\n",
    "#   # print(next_states_challenge.shape)\n",
    "#   # print(len(states_challenge) + len(next_states_challenge))\n",
    "\n",
    "#   # print(states_card.shape)\n",
    "#   # print(next_states_card.shape)\n",
    "#   # print(len(states_card) + len(next_states_card))\n",
    "\n",
    "\n",
    "#   losses_action = []\n",
    "#   losses_block = []\n",
    "#   losses_challenge = []\n",
    "#   losses_card = []\n",
    "\n",
    "#   ############################################\n",
    "# # Example: On-the-fly building raw sequences\n",
    "# #          and passing them into the RNN Q\n",
    "# ############################################\n",
    "#   num_batches_action = len(states_action) // batch_size\n",
    "\n",
    "#   for i in range(num_batches_action):\n",
    "#     # -------------------------------------------------\n",
    "#     # 1) Sample a batch of indices\n",
    "#     # -------------------------------------------------\n",
    "#     batch_indices_action = random.sample(\n",
    "#         range(len(states_action)),\n",
    "#         min(batch_size, len(states_action))\n",
    "#     )\n",
    "\n",
    "#     batch_states_action = torch.stack([states_action[j] for j in batch_indices_action])\n",
    "#     batch_actions_main  = actions_main[batch_indices_action]  # shape (batch_size,)\n",
    "#     # (Optional) Make sure batch_actions_main is torch.LongTensor\n",
    "#     batch_actions_main  = torch.tensor(batch_actions_main, dtype=torch.long)\n",
    "\n",
    "#     # Lists to store Q-values we compute for each item in this mini-batch\n",
    "#     q_values_current_list = []\n",
    "#     q_values_nextmax_list = []\n",
    "#     rewards_list          = []\n",
    "#     done_list             = []\n",
    "\n",
    "#     # -------------------------------------------------\n",
    "#     # 2) Process each single-frame \"state\" in the batch\n",
    "#     #    to build the raw sequences for current & next\n",
    "#     # -------------------------------------------------\n",
    "#     for idx, single_frame_state in enumerate(batch_states_action):\n",
    "#         # ------------------\n",
    "#         # Find global index\n",
    "#         # ------------------\n",
    "#         indices = torch.where((state == single_frame_state).all(dim=1))[0]\n",
    "#         if len(indices) == 0:\n",
    "#             # We did not find it => treat as terminal\n",
    "#             # => Q(s) is whatever, we can do 0\n",
    "#             # => Or we skip, but let's just do zero and done=1\n",
    "#             q_values_current_list.append(torch.tensor(0.0))  # single float\n",
    "#             q_values_nextmax_list.append(torch.tensor(0.0))\n",
    "#             rewards_list.append(torch.tensor(0.0))\n",
    "#             done_list.append(torch.tensor(1.0))\n",
    "#             continue\n",
    "\n",
    "#         state_index = indices[0].item()\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # (a) Build the \"current\" sequence: from last done to now\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         cur_seq_frames = []\n",
    "#         back_index = state_index\n",
    "#         while back_index >= 0 and done[back_index] != 1:\n",
    "#             cur_seq_frames.insert(0, state[back_index])  # front => chronological\n",
    "#             back_index -= 1\n",
    "#         # shape => (seq_len, 12)\n",
    "#         # try:\n",
    "#         if len(cur_seq_frames) == 0:\n",
    "#             cur_seq_frames.append(state[state_index])\n",
    "#         cur_seq_tensor = torch.stack(cur_seq_frames, dim=0)\n",
    "#         # except:\n",
    "#         #   cur_seq_tensor = torch.stack(single_frame_state, dim=0)\n",
    "#         # shape => (1, seq_len, 12)\n",
    "#         cur_seq_tensor = cur_seq_tensor.unsqueeze(0)\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # (b) Build the \"next\" sequence\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         next_seq_frames = []\n",
    "#         rewards_for_seq = []\n",
    "#         current_index = state_index + 1\n",
    "#         while (current_index < len(state) and acting_players[current_index] != 0):\n",
    "#             next_seq_frames.append(state[current_index])\n",
    "#             rewards_for_seq.append(rewards[current_index])\n",
    "#             if done[current_index] == 1:\n",
    "#                 break\n",
    "#             current_index += 1\n",
    "\n",
    "#         # We will decide how to handle \"no next frames\"\n",
    "#         if len(next_seq_frames) == 0:\n",
    "#             # Means terminal or no next chunk\n",
    "#             next_seq_tensor = None  # we'll handle it below\n",
    "#         else:\n",
    "#             next_seq_tensor = torch.stack(next_seq_frames, dim=0)\n",
    "#             next_seq_tensor = next_seq_tensor.unsqueeze(0)  # shape (1, seq_len2, 12)\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # (c) Forward pass for the current sequence\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # shape => (1, action_dim) if batch_size=1\n",
    "#         out_current = bot.action_q(cur_seq_tensor)\n",
    "#         # We gather the chosen actions Q\n",
    "#         chosen_action = batch_actions_main[idx]  # a single int\n",
    "#         q_val_current = out_current[0, chosen_action]  # shape => scalar\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # (d) Forward pass for the next sequence (if it exists)\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         if next_seq_tensor is None:\n",
    "#             # Terminal. Let's define next_max_Q = 0\n",
    "#             q_val_nextmax = torch.tensor(0.0)\n",
    "#             # Reward from the first step after current_index if in bounds\n",
    "#             if current_index < len(rewards):\n",
    "#                 r = rewards[current_index]\n",
    "#             else:\n",
    "#                 r = 0.0\n",
    "#             done_flag = 1.0\n",
    "#         else:\n",
    "#             out_next = bot.action_q(next_seq_tensor)        # shape => (1, action_dim)\n",
    "#             q_val_nextmax = out_next.max(dim=1)[0].squeeze() # scalar\n",
    "#             # Reward is average of rewards_for_seq\n",
    "#             if len(rewards_for_seq) > 0:\n",
    "#                 r = sum(rewards_for_seq) / len(rewards_for_seq)\n",
    "#             else:\n",
    "#                 r = 0.0\n",
    "#             # Are we done? If in range, check done[current_index]\n",
    "#             if current_index < len(done):\n",
    "#                 done_flag = float(done[current_index])\n",
    "#             else:\n",
    "#                 done_flag = 1.0\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # (e) Collect everything in lists\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         q_values_current_list.append(q_val_current)\n",
    "#         q_values_nextmax_list.append(q_val_nextmax)\n",
    "#         rewards_list.append(torch.tensor(r, dtype=torch.float32))\n",
    "#         done_list.append(torch.tensor(done_flag, dtype=torch.float32))\n",
    "\n",
    "#     # -------------------------------------------------\n",
    "#     # 3) Convert results to Tensors\n",
    "#     #    so we can do a standard DQN loss\n",
    "#     # -------------------------------------------------\n",
    "#     # shape => (batch_size,)\n",
    "#     q_current_t     = torch.stack(q_values_current_list)  # current Q chosen action\n",
    "#     q_nextmax_t     = torch.stack(q_values_nextmax_list)\n",
    "#     rewards_t       = torch.stack(rewards_list)\n",
    "#     done_t          = torch.stack(done_list)\n",
    "\n",
    "#     # DQN target\n",
    "#     target_q = rewards_t + gamma * q_nextmax_t * (1.0 - done_t)\n",
    "\n",
    "#     # -------------------------------------------------\n",
    "#     # 4) Compute loss & backprop\n",
    "#     # -------------------------------------------------\n",
    "#     loss_action = criterion(q_current_t, target_q)\n",
    "#     bot.optimizer_action.zero_grad()\n",
    "#     loss_action.backward()\n",
    "#     bot.optimizer_action.step()\n",
    "\n",
    "#     # -------------------------------------------------\n",
    "#     # 5) Remove used transitions from replay\n",
    "#     #    (Optional, as in your original code)\n",
    "#     # -------------------------------------------------\n",
    "#     states_action_np = states_action.cpu().numpy()\n",
    "#     actions_main_np  = actions_main.cpu().numpy()\n",
    "\n",
    "#     states_action_np = np.delete(states_action_np, batch_indices_action, axis=0)\n",
    "#     actions_main_np  = np.delete(actions_main_np,  batch_indices_action, axis=0)\n",
    "\n",
    "#     states_action = torch.tensor(states_action_np, dtype=torch.float32)\n",
    "#     actions_main  = torch.tensor(actions_main_np,  dtype=torch.int64)\n",
    "\n",
    "#     losses_action.append(loss_action.item())\n",
    "\n",
    "\n",
    "#     # print('action success')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#   ###############################################################################\n",
    "# # RNN-based DQN training loop for your \"block\" decision, without a summarizer\n",
    "# ###############################################################################\n",
    "#   num_batches_block = len(states_block) // batch_size\n",
    "\n",
    "#   for i in range(num_batches_block):\n",
    "#     # 1) Sample a random batch of indices\n",
    "#     batch_indices_block = random.sample(\n",
    "#         range(len(states_block)),\n",
    "#         min(batch_size, len(states_block))\n",
    "#     )\n",
    "\n",
    "#     # 2) Gather Tensors for this mini-batch\n",
    "#     batch_states_block  = torch.stack([states_block[j] for j in batch_indices_block])\n",
    "#     batch_actions_block = actions_block[batch_indices_block]  # shape (batch_size,)\n",
    "#     # Make sure actions are long-int for gather\n",
    "#     batch_actions_block = torch.tensor(batch_actions_block, dtype=torch.long)\n",
    "\n",
    "#     # Lists to store results for each item in the batch\n",
    "#     q_values_current_list = []\n",
    "#     q_values_nextmax_list = []\n",
    "#     rewards_list          = []\n",
    "#     done_list             = []\n",
    "\n",
    "#     # ------------------------------------------------------------------\n",
    "#     # 3) For each single-frame in batch_states_block:\n",
    "#     #    (a) Build \"current\" sequence (backwards)\n",
    "#     #    (b) Build \"next\" sequence (forwards)\n",
    "#     #    (c) Forward pass each through RNN\n",
    "#     #    (d) Collect chosen-action Q, next-max Q, reward, and done\n",
    "#     # ------------------------------------------------------------------\n",
    "#     for idx, single_frame_state in enumerate(batch_states_block):\n",
    "#         # 3.1) Find the global index in your big `state` array\n",
    "#         indices = torch.where((state_block == single_frame_state).all(dim=1))[0]\n",
    "\n",
    "#         if len(indices) == 0:\n",
    "#             # If we didn't find it, treat as terminal\n",
    "#             q_values_current_list.append(torch.tensor(0.0))\n",
    "#             q_values_nextmax_list.append(torch.tensor(0.0))\n",
    "#             rewards_list.append(torch.tensor(0.0))\n",
    "#             done_list.append(torch.tensor(1.0))\n",
    "#             continue\n",
    "\n",
    "#         state_index = indices[0].item()\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # (a) Build \"current\" sequence by going backward until done\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         cur_seq_frames = []\n",
    "#         back_index = state_index\n",
    "#         while back_index >= 0 and done[back_index] != 1:\n",
    "#             cur_seq_frames.insert(0, state_block[back_index])  # front => chronological\n",
    "#             back_index -= 1\n",
    "#         # If we ended up with an empty chunk, fallback to single frame\n",
    "#         if len(cur_seq_frames) == 0:\n",
    "#             cur_seq_frames.append(state_block[state_index])\n",
    "\n",
    "#         # shape: (seq_len, 12)\n",
    "#         cur_seq_tensor = torch.stack(cur_seq_frames, dim=0)\n",
    "#         # shape: (1, seq_len, 12) for the RNN\n",
    "#         cur_seq_tensor = cur_seq_tensor.unsqueeze(0)\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # (b) Build \"next\" sequence by going forward until done or not reacting\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         next_seq_frames = []\n",
    "#         rewards_for_seq = []\n",
    "#         current_idx = state_index + 1\n",
    "\n",
    "#         while (current_idx < len(state) and reacting_players[current_idx] != 0):\n",
    "#             next_seq_frames.append(state_block[current_idx])\n",
    "#             rewards_for_seq.append(rewards[current_idx])\n",
    "#             if done[current_idx] == 1:\n",
    "#                 break\n",
    "#             current_idx += 1\n",
    "\n",
    "#         # We will define \"no next frames\" => terminal\n",
    "#         if len(next_seq_frames) == 0:\n",
    "#             next_seq_tensor = None\n",
    "#         else:\n",
    "#             next_seq_tensor = torch.stack(next_seq_frames, dim=0)\n",
    "#             next_seq_tensor = next_seq_tensor.unsqueeze(0)  # (1, seq_len2, 12)\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # (c) Forward pass: current sequence\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         out_current = bot.block_q(cur_seq_tensor)  # shape => (1, num_actions)\n",
    "#         chosen_action = batch_actions_block[idx]\n",
    "#         q_val_current = out_current[0, chosen_action]  # => scalar\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # (d) Forward pass: next sequence (if exists)\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         if next_seq_tensor is None:\n",
    "#             # Terminal\n",
    "#             q_val_nextmax = torch.tensor(0.0)\n",
    "#             # Reward\n",
    "#             if current_idx < len(rewards):\n",
    "#                 r = rewards[current_idx]\n",
    "#             else:\n",
    "#                 r = 0.0\n",
    "#             done_flag = 1.0\n",
    "#         else:\n",
    "#             out_next = bot.block_q(next_seq_tensor)         # => (1, num_actions)\n",
    "#             q_val_nextmax = out_next.max(dim=1)[0].squeeze()  # => scalar\n",
    "\n",
    "#             # Reward = average (or sum) across the forward chunk\n",
    "#             if len(rewards_for_seq) > 0:\n",
    "#                 r = sum(rewards_for_seq) / len(rewards_for_seq)\n",
    "#             else:\n",
    "#                 r = 0.0\n",
    "\n",
    "#             # Done or not\n",
    "#             if current_idx < len(done):\n",
    "#                 done_flag = float(done[current_idx])\n",
    "#             else:\n",
    "#                 done_flag = 1.0\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # (e) Store in lists\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         q_values_current_list.append(q_val_current)\n",
    "#         q_values_nextmax_list.append(q_val_nextmax)\n",
    "#         rewards_list.append(torch.tensor(r, dtype=torch.float32))\n",
    "#         done_list.append(torch.tensor(done_flag, dtype=torch.float32))\n",
    "\n",
    "#     # ------------------------------------------------------------------\n",
    "#     # 4) Convert the results to Tensors for a standard DQN update\n",
    "#     # ------------------------------------------------------------------\n",
    "#     q_current_t = torch.stack(q_values_current_list)   # shape (batch_size,)\n",
    "#     q_nextmax_t = torch.stack(q_values_nextmax_list)   # shape (batch_size,)\n",
    "#     rewards_t   = torch.stack(rewards_list)            # shape (batch_size,)\n",
    "#     done_t      = torch.stack(done_list)               # shape (batch_size,)\n",
    "\n",
    "#     target_q = rewards_t + gamma * q_nextmax_t * (1.0 - done_t)\n",
    "\n",
    "#     # ------------------------------------------------------------------\n",
    "#     # 5) Compute the loss & backprop\n",
    "#     # ------------------------------------------------------------------\n",
    "#     loss_block = criterion(q_current_t, target_q)\n",
    "#     bot.optimizer_block.zero_grad()\n",
    "#     loss_block.backward()\n",
    "#     bot.optimizer_block.step()\n",
    "\n",
    "#     # ------------------------------------------------------------------\n",
    "#     # 6) Remove these samples from your replay buffer (optional)\n",
    "#     # ------------------------------------------------------------------\n",
    "#     # Convert Tensors to NumPy for deletion\n",
    "#     states_block_np  = states_block.cpu().numpy()\n",
    "#     actions_block_np = actions_block.cpu().numpy()\n",
    "\n",
    "#     states_block_np  = np.delete(states_block_np,  batch_indices_block, axis=0)\n",
    "#     actions_block_np = np.delete(actions_block_np, batch_indices_block, axis=0)\n",
    "\n",
    "#     states_block  = torch.tensor(states_block_np,  dtype=torch.float32)\n",
    "#     actions_block = torch.tensor(actions_block_np, dtype=torch.int64)\n",
    "\n",
    "#     # If you have next_states_block or others, remove them similarly...\n",
    "#     # next_states_block = np.delete(...)\n",
    "\n",
    "#     losses_block.append(loss_block.item())\n",
    "\n",
    "\n",
    "#     # print('block success')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#   ##############################################################################\n",
    "# # RNN-based DQN loop for your \"challenge\" decision, without a separate summarizer\n",
    "# ##############################################################################\n",
    "\n",
    "# #   num_batches_challenge = len(states_challenge) // batch_size\n",
    "\n",
    "# #   for i in range(num_batches_challenge):\n",
    "# #     # 1) Sample a batch of indices\n",
    "# #     batch_indices_challenge = random.sample(\n",
    "# #         range(len(states_challenge)),\n",
    "# #         min(batch_size, len(states_challenge))\n",
    "# #     )\n",
    "\n",
    "# #     # 2) Gather the states & actions for this mini-batch\n",
    "# #     batch_states_challenge  = torch.stack([states_challenge[j] for j in batch_indices_challenge])\n",
    "# #     batch_actions_challenge = actions_challenge[batch_indices_challenge]\n",
    "# #     batch_actions_challenge = torch.tensor(batch_actions_challenge, dtype=torch.long)\n",
    "\n",
    "# #     # Lists to store results for the DQN update\n",
    "# #     q_values_current_list = []\n",
    "# #     q_values_nextmax_list = []\n",
    "# #     rewards_list          = []\n",
    "# #     done_list             = []\n",
    "\n",
    "# #     # ----------------------------------------------------------------\n",
    "# #     # 3) For each single-frame in batch_states_challenge:\n",
    "# #     #    - Build backward-chunk (current)\n",
    "# #     #    - Build forward-chunk (next) per your challenge logic\n",
    "# #     #    - RNN forward pass -> Q-values\n",
    "# #     # ----------------------------------------------------------------\n",
    "# #     for idx, single_frame_state in enumerate(batch_states_challenge):\n",
    "# #         # Try to locate this state in the global `state` array\n",
    "# #         indices = torch.where((state_challenge == single_frame_state).all(dim=1))[0]\n",
    "\n",
    "# #         if len(indices) == 0:\n",
    "# #             # If not found, treat as terminal\n",
    "# #             # Q(current) = 0, Q(next_max) = 0, reward=0, done=1\n",
    "# #             q_values_current_list.append(torch.tensor(0.0))\n",
    "# #             q_values_nextmax_list.append(torch.tensor(0.0))\n",
    "# #             rewards_list.append(torch.tensor(0.0))\n",
    "# #             done_list.append(torch.tensor(1.0))\n",
    "# #             continue\n",
    "\n",
    "# #         state_index = indices[0].item()\n",
    "\n",
    "# #         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# #         # 3A) Build the backward-chunk for the current state\n",
    "# #         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# #         cur_seq_frames = []\n",
    "# #         back_index = state_index\n",
    "# #         # Move backward until we hit done=1 or index < 0\n",
    "# #         # (You could also incorporate more challenge-specific conditions\n",
    "# #         # if you want symmetrical logic with forward-chunk.)\n",
    "# #         while back_index >= 0 and done[back_index] != 1:\n",
    "# #             cur_seq_frames.insert(0, state_challenge[back_index])  # front => chronological order\n",
    "# #             back_index -= 1\n",
    "\n",
    "# #         if len(cur_seq_frames) == 0:\n",
    "# #             # fallback to single frame if we got nothing\n",
    "# #             cur_seq_frames.append(state_challenge[state_index])\n",
    "\n",
    "# #         # Shape: (seq_len, 12)\n",
    "# #         cur_seq_tensor = torch.stack(cur_seq_frames, dim=0)\n",
    "# #         # Shape: (1, seq_len, 12) for the RNN\n",
    "# #         cur_seq_tensor = cur_seq_tensor.unsqueeze(0)\n",
    "\n",
    "# #         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# #         # 3B) Build the forward-chunk for the next state\n",
    "# #         #     Use your \"challenge\" condition:\n",
    "# #         #         reacting_players[idx] != 0\n",
    "# #         #         AND (acting_players[idx] != 0 OR reactions_game[idx] != 1)\n",
    "# #         #         AND done[idx] != 1\n",
    "# #         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# #         next_seq_frames = []\n",
    "# #         rewards_for_seq = []\n",
    "# #         current_idx     = state_index + 1\n",
    "\n",
    "# #         while (\n",
    "# #             current_idx < len(state) and reacting_players[idx] != 0\n",
    "# #                 and (acting_players[idx] != 0 or reactions_game[idx] != 1)):\n",
    "# #             next_seq_frames.append(state_challenge[current_idx])\n",
    "# #             rewards_for_seq.append(rewards[current_idx])\n",
    "# #             if done[current_idx] == 1:\n",
    "# #                 break\n",
    "# #             current_idx += 1\n",
    "\n",
    "# #         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# #         # RNN Forward pass for the \"current\" sequence\n",
    "# #         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# #         out_current = bot.challenge_q(cur_seq_tensor)  # shape (1, num_actions)\n",
    "# #         chosen_action = batch_actions_challenge[idx]\n",
    "# #         q_val_current = out_current[0, chosen_action]   # => scalar\n",
    "\n",
    "# #         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# #         # RNN Forward pass for the \"next\" sequence\n",
    "# #         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# #         q_val_nextmax = torch.tensor(0.0)\n",
    "# #         r = 0.0\n",
    "# #         done_flag = 1.0\n",
    "# #         if len(next_seq_frames) == 0:\n",
    "# #             # Terminal\n",
    "# #             q_val_nextmax = torch.tensor(0.0)\n",
    "# #             # Reward:\n",
    "# #             if current_idx < len(rewards):\n",
    "# #                 r = rewards[current_idx]\n",
    "# #             else:\n",
    "# #                 r = 0.0\n",
    "# #             done_flag = 1.0\n",
    "# #         else:\n",
    "# #             # Non-terminal\n",
    "# #             next_seq_tensor = torch.stack(next_seq_frames, dim=0).unsqueeze(0)\n",
    "# #             out_next        = bot.challenge_q(next_seq_tensor)   # shape (1, num_actions)\n",
    "# #             q_val_nextmax   = out_next.max(dim=1)[0].squeeze()   # => scalar\n",
    "\n",
    "# #             # Reward as average (or adjust logic as you see fit)\n",
    "# #             if len(rewards_for_seq) > 0:\n",
    "# #                 r = sum(rewards_for_seq) / len(rewards_for_seq)\n",
    "# #             else:\n",
    "# #                 r = 0.0\n",
    "\n",
    "# #             if current_idx < len(done):\n",
    "# #                 done_flag = float(done[current_idx])\n",
    "# #             else:\n",
    "# #                 done_flag = 1.0  # out of bounds => terminal\n",
    "\n",
    "# #         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# #         # Collect everything for the DQN update\n",
    "# #         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# #         q_values_current_list.append(q_val_current)\n",
    "# #         q_values_nextmax_list.append(q_val_nextmax)\n",
    "# #         rewards_list.append(torch.tensor(r, dtype=torch.float32))\n",
    "# #         done_list.append(torch.tensor(done_flag, dtype=torch.float32))\n",
    "\n",
    "# #     # ----------------------------------------------------------------\n",
    "# #     # 4) Convert to Tensors & compute DQN loss\n",
    "# #     # ----------------------------------------------------------------\n",
    "# #     q_current_t = torch.stack(q_values_current_list)  # shape (batch_size,)\n",
    "# #     q_nextmax_t = torch.stack(q_values_nextmax_list)  # shape (batch_size,)\n",
    "# #     rewards_t   = torch.stack(rewards_list)           # shape (batch_size,)\n",
    "# #     done_t      = torch.stack(done_list)              # shape (batch_size,)\n",
    "\n",
    "# #     target_q = rewards_t + gamma * q_nextmax_t * (1.0 - done_t)\n",
    "\n",
    "# #     loss_challenge = criterion(q_current_t, target_q)\n",
    "\n",
    "# #     # ----------------------------------------------------------------\n",
    "# #     # 5) Backprop & optimize\n",
    "# #     # ----------------------------------------------------------------\n",
    "# #     bot.optimizer_challenge.zero_grad()\n",
    "# #     loss_challenge.backward()\n",
    "# #     bot.optimizer_challenge.step()\n",
    "\n",
    "# #     # ----------------------------------------------------------------\n",
    "# #     # 6) Remove these samples from your replay buffer\n",
    "# #     # ----------------------------------------------------------------\n",
    "# #     # Convert Tensors -> NumPy for np.delete\n",
    "# #     states_challenge_np  = states_challenge.cpu().numpy()\n",
    "# #     actions_challenge_np = actions_challenge.cpu().numpy()\n",
    "# #     # If you have \"next_states_challenge\" or others, similarly convert them\n",
    "\n",
    "# #     states_challenge_np  = np.delete(states_challenge_np,  batch_indices_challenge, axis=0)\n",
    "# #     actions_challenge_np = np.delete(actions_challenge_np, batch_indices_challenge, axis=0)\n",
    "\n",
    "# #     # Rebuild your PyTorch Tensors\n",
    "# #     states_challenge  = torch.tensor(states_challenge_np,  dtype=torch.float32)\n",
    "# #     actions_challenge = torch.tensor(actions_challenge_np, dtype=torch.int64)\n",
    "\n",
    "# #     # If you store next_states_challenge, remove them as well:\n",
    "# #     # next_states_challenge_np = next_states_challenge.cpu().numpy()\n",
    "# #     # next_states_challenge_np = np.delete(next_states_challenge_np, batch_indices_challenge, axis=0)\n",
    "# #     # next_states_challenge = torch.tensor(next_states_challenge_np, dtype=torch.float32)\n",
    "\n",
    "# #     losses_challenge.append(loss_challenge.item())\n",
    "\n",
    "\n",
    "#     # print('challenge success')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#   ##############################################################################\n",
    "# # RNN-based training loop for your \"card\" decision,\n",
    "# # building raw backward and forward sequences on-the-fly\n",
    "# ##############################################################################\n",
    "#   num_batches_card = len(states_card) // batch_size\n",
    "\n",
    "#   for i in range(num_batches_card):\n",
    "#     if num_batches_card == 0:\n",
    "#       raise Exception(\"Something went wrong\")\n",
    "\n",
    "#     # for i in range(num_batches_card):\n",
    "\n",
    "#     # 1) Sample batch indices\n",
    "#     batch_indices_card = random.sample(\n",
    "#         range(len(states_card)),\n",
    "#         min(batch_size, len(states_card))\n",
    "#     )\n",
    "\n",
    "#     # 2) Gather states & actions for this batch\n",
    "#     batch_states_card  = torch.stack([states_card[j] for j in batch_indices_card])  # (batch_size, 12)\n",
    "#     batch_actions_card = actions_card[batch_indices_card]                           # (batch_size,)\n",
    "#     batch_actions_card = torch.tensor(batch_actions_card, dtype=torch.long)\n",
    "\n",
    "#     # We'll collect Q-values and targets for the entire batch\n",
    "#     q_current_list = []\n",
    "#     q_nextmax_list = []\n",
    "#     reward_list    = []\n",
    "#     done_list      = []\n",
    "\n",
    "#     # ------------------------------------------------------------------------\n",
    "#     # 3) For each single-frame in batch_states_card, build backward & forward\n",
    "#     # ------------------------------------------------------------------------\n",
    "#     for idx, single_frame_state in enumerate(batch_states_card):\n",
    "#         # Locate this state in the global 'state' buffer\n",
    "#         indices = torch.where((state_card == single_frame_state).all(dim=1))[0]\n",
    "#         if len(indices) == 0:\n",
    "#             # Not found => treat as terminal\n",
    "#             # Q(current) = 0, Q(next) = 0, reward=0, done=1\n",
    "#             q_current_list.append(torch.tensor(0.0))\n",
    "#             q_nextmax_list.append(torch.tensor(0.0))\n",
    "#             reward_list.append(torch.tensor(0.0))\n",
    "#             done_list.append(torch.tensor(1.0))\n",
    "#             continue\n",
    "\n",
    "#         state_index = indices[0].item()\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # 3A) BACKWARD chunk for \"current\" state\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         cur_seq_frames = []\n",
    "#         back_idx = state_index\n",
    "\n",
    "#         # Move backward until done=1 or out of array\n",
    "#         while back_idx >= 0 and done[back_idx] != 1:\n",
    "#             cur_seq_frames.insert(0, state_card[back_idx])  # insert at front => chronological order\n",
    "#             back_idx -= 1\n",
    "\n",
    "#         if len(cur_seq_frames) == 0:\n",
    "#             # fallback to just the single frame\n",
    "#             cur_seq_frames.append(state_card[state_index])\n",
    "\n",
    "#         # Turn into shape (1, seq_len, 12) for the RNN\n",
    "#         cur_seq_tensor = torch.stack(cur_seq_frames, dim=0).unsqueeze(0)\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # 3B) FORWARD chunk for \"next\" state, until:\n",
    "#         #     - done=1\n",
    "#         #     - Bot 0's cards change (state[..., 7:9] differs from next step)\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         fwd_seq_frames = []\n",
    "#         rewards_for_seq = []\n",
    "#         current_idx = state_index + 1\n",
    "\n",
    "#         while current_idx < len(state):\n",
    "\n",
    "#             # Append the current frame\n",
    "#             fwd_seq_frames.append(state_card[current_idx])\n",
    "#             rewards_for_seq.append(rewards[current_idx])\n",
    "\n",
    "#             # Check if the *next* step changes Bot 0's cards\n",
    "#             # so we break *after* including the current frame if it\n",
    "#             # leads to a change in [7:9].\n",
    "#             if current_idx + 1 < len(state):\n",
    "#                 # Compare the slice [7:9] of the current vs. the next\n",
    "#                 bot0_current_cards = state_card[current_idx][7:9]\n",
    "#                 bot0_next_cards    = state_card[current_idx+1][7:9]\n",
    "#                 if not torch.equal(bot0_current_cards, bot0_next_cards):\n",
    "#                     # Bot 0's cards changed => break\n",
    "#                     current_idx += 1  # increment so we include the reward\n",
    "#                     break\n",
    "\n",
    "#             # Stop if done=1\n",
    "#             if done[current_idx] == 1:\n",
    "#                 break\n",
    "\n",
    "#             # If no change, keep going\n",
    "#             current_idx += 1\n",
    "\n",
    "#             # Also break if weve run out of array (the while condition checks that anyway)\n",
    "#             # but well rely on the loop condition.\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # Forward pass in the RNN for the \"current\" sequence\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         out_current = bot.card_q(cur_seq_tensor)     # shape: (1, num_actions)\n",
    "#         # pick the Q-value for the chosen action\n",
    "#         chosen_action = batch_actions_card[idx]\n",
    "#         q_val_current = out_current[0, chosen_action] # => scalar\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # Forward pass for the \"next\" sequence\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         if len(fwd_seq_frames) == 0:\n",
    "#             # Terminal if we got no next frames\n",
    "#             q_val_nextmax = torch.tensor(0.0)\n",
    "#             # Reward\n",
    "#             if current_idx < len(state):\n",
    "#                 r = rewards[current_idx]\n",
    "#             else:\n",
    "#                 r = 0.0\n",
    "#             done_flag = 1.0\n",
    "#         else:\n",
    "#             # Non-terminal\n",
    "#             fwd_seq_tensor = torch.stack(fwd_seq_frames, dim=0).unsqueeze(0)\n",
    "#             out_next       = bot.card_q(fwd_seq_tensor)  # shape (1, num_actions)\n",
    "#             q_val_nextmax  = out_next.max(dim=1)[0].squeeze()      # => scalar\n",
    "\n",
    "#             # Could do average or sum of rewards\n",
    "#             if len(rewards_for_seq) > 0:\n",
    "#                 r = sum(rewards_for_seq) / len(rewards_for_seq)\n",
    "#             else:\n",
    "#                 r = 0.0\n",
    "\n",
    "#             # done flag\n",
    "#             if current_idx < len(done):\n",
    "#                 done_flag = float(done[current_idx])\n",
    "#             else:\n",
    "#                 done_flag = 1.0\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # Collect results for the DQN update\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         q_current_list.append(q_val_current)\n",
    "#         q_nextmax_list.append(q_val_nextmax)\n",
    "#         reward_list.append(torch.tensor(r, dtype=torch.float32))\n",
    "#         done_list.append(torch.tensor(done_flag, dtype=torch.float32))\n",
    "\n",
    "#     # ------------------------------------------------------------------\n",
    "#     # 4) Convert to Tensors & compute the DQN target\n",
    "#     # ------------------------------------------------------------------\n",
    "#     q_current_t = torch.stack(q_current_list)   # (batch_size,)\n",
    "#     q_nextmax_t = torch.stack(q_nextmax_list)   # (batch_size,)\n",
    "#     reward_t    = torch.stack(reward_list)      # (batch_size,)\n",
    "#     done_t      = torch.stack(done_list)        # (batch_size,)\n",
    "\n",
    "#     # DQN target: r + gamma * max(Q(next)) * (1 - done)\n",
    "#     target_q = reward_t + gamma * q_nextmax_t * (1.0 - done_t)\n",
    "\n",
    "#     # ------------------------------------------------------------------\n",
    "#     # 5) Compute loss & optimize\n",
    "#     # ------------------------------------------------------------------\n",
    "#     loss_card = criterion(q_current_t, target_q)\n",
    "\n",
    "#     bot.optimizer_card.zero_grad()\n",
    "#     loss_card.backward()\n",
    "#     bot.optimizer_card.step()\n",
    "\n",
    "#     # ------------------------------------------------------------------\n",
    "#     # 6) Remove these samples from the replay buffer\n",
    "#     # ------------------------------------------------------------------\n",
    "#     states_card_np   = states_card.cpu().numpy()\n",
    "#     actions_card_np  = actions_card.cpu().numpy()\n",
    "#     # If you have next_states_card, similarly convert & remove\n",
    "\n",
    "#     states_card_np   = np.delete(states_card_np,  batch_indices_card, axis=0)\n",
    "#     actions_card_np  = np.delete(actions_card_np, batch_indices_card, axis=0)\n",
    "\n",
    "#     states_card  = torch.tensor(states_card_np, dtype=torch.float32)\n",
    "#     actions_card = torch.tensor(actions_card_np, dtype=torch.int64)\n",
    "\n",
    "#     losses_card.append(loss_card.item())\n",
    "\n",
    "\n",
    "\n",
    "#   # bot.cards = bots[0].cards\n",
    "#   # bot.num_coins = bots[0].num_coins\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # i += 1\n",
    "#   epsilon *= 0.9995\n",
    "\n",
    "#   # if (episode + 1) % 100 == 0:\n",
    "#   #   data_fraction = min(data_fraction + 1/5, 1)\n",
    "#   #   epsilon = 1.0\n",
    "\n",
    "#   # gamma = min(0.99, gamma + 0.001)\n",
    "\n",
    "#   avg_losses_action.append(sum(losses_action) / len(losses_action))\n",
    "#   avg_losses_block.append(sum(losses_block) / len(losses_block))\n",
    "# #   avg_losses_challenge.append(sum(losses_challenge) / len(losses_challenge))\n",
    "#   avg_losses_card.append(sum(losses_card) / len(losses_card))\n",
    "\n",
    "#   print(f'Avg Action Loss, {num_batches_action} batches: {avg_losses_action[-1]}')\n",
    "#   print(f'Avg Block Loss, {num_batches_block} batches: {avg_losses_block[-1]}')\n",
    "# #   print(f'Avg Challenge Loss, {num_batches_challenge} batches: {avg_losses_challenge[-1]}')\n",
    "#   print(f'Avg Card Loss, {num_batches_card} batches: {avg_losses_card[-1]}')\n",
    "\n",
    "#   # Copy parameters of action_q network\n",
    "#   bots[0].action_q.load_state_dict(bot.action_q.state_dict())\n",
    "\n",
    "#   # Copy parameters of block_q network\n",
    "#   bots[0].block_q.load_state_dict(bot.block_q.state_dict())\n",
    "\n",
    "#   # Copy parameters of challenge_q network\n",
    "# #   bots[0].challenge_q.load_state_dict(bot.challenge_q.state_dict())\n",
    "\n",
    "#   # Copy parameters of card_q network\n",
    "#   bots[0].card_q.load_state_dict(bot.card_q.state_dict())\n",
    "\n",
    "#   def verify_allclose(net_a, net_b, eps=1e-6):\n",
    "#     for p_a, p_b in zip(net_a.parameters(), net_b.parameters()):\n",
    "#         if not torch.allclose(p_a, p_b, atol=eps, rtol=1e-5):\n",
    "#             return False\n",
    "#     return True\n",
    "\n",
    "#   print(\"Action Q matches?\",\n",
    "#         verify_allclose(bots[0].action_q, bot.action_q))\n",
    "#   print(\"Block Q matches?\",\n",
    "#         verify_allclose(bots[0].block_q, bot.block_q))\n",
    "# #   print(\"Challenge Q matches?\",\n",
    "# #         verify_allclose(bots[0].challenge_q, bot.challenge_q))\n",
    "#   print(\"Card Q matches?\",\n",
    "#         verify_allclose(bots[0].card_q, bot.card_q))\n",
    "\n",
    "\n",
    "#   print(bots[0].name)\n",
    "\n",
    "#   if episode % 10 == 0:\n",
    "\n",
    "#     win_rate = 0\n",
    "\n",
    "#     bot0_actions = []\n",
    "\n",
    "#     game_lengths = 0\n",
    "    \n",
    "#     coins_game = []\n",
    "#     dones_eval = []\n",
    "\n",
    "#     for i in range(100):\n",
    "\n",
    "#         discard_pile, acting_player, reacting_player, current_player, action_game, reaction_game, challenge_game, card_game, coin_game, challenge_direction, done_0, reward, card_chosen, bots_copy = game_loop_random(bots, actions, influences_reverse, 0.0)\n",
    "#         bots = bots_copy\n",
    "#         for n in bots[1:]:\n",
    "#             n.cards[0] = 'Duke'\n",
    "\n",
    "#         game_lengths += len(action_game)\n",
    "\n",
    "#         for actor, action in zip(acting_player, action_game):\n",
    "#             if actor == 0:\n",
    "#                 bot0_actions.append(action)\n",
    "\n",
    "#         if reward[-1] == 1.0:\n",
    "#             win_rate += 1\n",
    "            \n",
    "#         coins_game += coin_game\n",
    "#         dones_eval += done_0\n",
    "                \n",
    "\n",
    "#     if bot0_actions:  # ensure the list is not empty\n",
    "#         counter = Counter(bot0_actions)\n",
    "#         most_common_action, count = counter.most_common(1)[0]\n",
    "#         print(f'Most common action for Bot 0: {most_common_action} taken {count} times.')\n",
    "#         print(f'total game lengths: {game_lengths}')\n",
    "    \n",
    "#     bot0_coin_counts = []\n",
    "#     opp_coin_counts = []\n",
    "#     done_flag = 0\n",
    "#     for c in range(len(dones_eval) - 1):\n",
    "#         if dones_eval[c] == 1 and done_flag == 0:\n",
    "#             done_flag = 1\n",
    "#             bot0_coin_counts.append(coins_game[c][0])\n",
    "#             opp_coin_counts.append(coins_game[c][1])\n",
    "#         elif dones_eval[c] == 0: \n",
    "#             done_flag = 0\n",
    "\n",
    "#     win_rate = win_rate / 100\n",
    "#     win_rates.append(win_rate)\n",
    "\n",
    "#     print(f'win rate: {win_rate}')\n",
    "    \n",
    "#     bot0_avg = np.mean(bot0_coin_counts)\n",
    "#     opp_avg = np.mean(opp_coin_counts)\n",
    "    \n",
    "#     bot0_avgs.append(bot0_avg)\n",
    "#     opp_avgs.append(opp_avg)\n",
    "    \n",
    "#     print(f'bot 0 average final coin count: {bot0_avg}')\n",
    "#     print(f'opponent average final coin count: {opp_avg}')\n",
    "\n",
    "\n",
    "#   # df = pd.DataFrame(data = data)\n",
    "#   # print(df.head())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14f91158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({\n",
    "#         'action_q_state_dict': bot.action_q.state_dict(),\n",
    "#         'block_q_state_dict': bot.block_q.state_dict(),\n",
    "#         'challenge_q_state_dict': bot.challenge_q.state_dict(),\n",
    "#         'card_q_state_dict': bot.card_q.state_dict(),\n",
    "#         'optimizer_action_state_dict': bot.optimizer_action.state_dict(),\n",
    "#         'optimizer_block_state_dict': bot.optimizer_block.state_dict(),\n",
    "# #         'optimizer_challenge_state_dict': bot.optimizer_challenge.state_dict(),\n",
    "#         'optimizer_card_state_dict': bot.optimizer_card.state_dict()\n",
    "#     }, 'bot_parameters.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfc666b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('bot_parameters.pth')\n",
    "bot.action_q.load_state_dict(checkpoint['action_q_state_dict'])\n",
    "bot.block_q.load_state_dict(checkpoint['block_q_state_dict'])\n",
    "# bot.challenge_q.load_state_dict(checkpoint['challenge_q_state_dict'])\n",
    "bot.card_q.load_state_dict(checkpoint['card_q_state_dict'])\n",
    "bot.optimizer_action.load_state_dict(checkpoint['optimizer_action_state_dict'])\n",
    "bot.optimizer_block.load_state_dict(checkpoint['optimizer_block_state_dict'])\n",
    "# bot.optimizer_challenge.load_state_dict(checkpoint['optimizer_challenge_state_dict'])\n",
    "bot.optimizer_card.load_state_dict(checkpoint['optimizer_card_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6561c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dx/zdhgw_j12hv1vs2r2s94qphr0000gn/T/ipykernel_19201/1706364523.py:218: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x[-1][9:12] = torch.tensor(embedding_actions(torch.tensor(action)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot 0 Win Rate, Random Actions: 0.39\n",
      "{1: 17, 2: 15, 3: 14, 4: 14, 5: 12}\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "bots[0].action_q.load_state_dict(bot.action_q.state_dict())\n",
    "\n",
    "# Copy parameters of block_q network\n",
    "bots[0].block_q.load_state_dict(bot.block_q.state_dict())\n",
    "\n",
    "# Copy parameters of challenge_q network\n",
    "# bots[0].challenge_q.load_state_dict(bot.challenge_q.state_dict())\n",
    "\n",
    "# Copy parameters of card_q network\n",
    "bots[0].card_q.load_state_dict(bot.card_q.state_dict())\n",
    "\n",
    "\n",
    "win_rate = 0\n",
    "\n",
    "card_losses_counts = {1: 0,\n",
    "                     2: 0,\n",
    "                     3: 0,\n",
    "                     4: 0,\n",
    "                     5: 0}\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    # print(i)\n",
    "    discard_pile, acting_player, reacting_player, current_player, action_game, reaction_game, challenge_game, card_game, coin_game, challenge_direction, done_0, reward, card_chosen, bots_copy = game_loop_random(bots, actions, influences_reverse, 0.0)\n",
    "    bots = bots_copy\n",
    "    if reward[-1] == 1:\n",
    "        win_rate += 1\n",
    "    else:\n",
    "        for card in itertools.chain.from_iterable(card_game[-1]):\n",
    "            if card in card_losses_counts:      # skips 0 / None / out-of-range\n",
    "                card_losses_counts[card] += 1\n",
    "            \n",
    "\n",
    "# win_rate = win_rate / 50\n",
    "print(f'Bot 0 Win Rate, Random Actions: {win_rate / 100}')\n",
    "print(card_losses_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9f7a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import stat\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "num_episodes = 10000\n",
    "max_steps_per_episode = 200\n",
    "epsilon = 1.0\n",
    "list_division = 4\n",
    "gamma = 0.99\n",
    "\n",
    "bot = copy.deepcopy(bots[0])\n",
    "\n",
    "avg_losses_action = []\n",
    "avg_losses_block = []\n",
    "avg_losses_challenge = []\n",
    "avg_losses_card = []\n",
    "\n",
    "# bots.remove(bots[-1])\n",
    "# bots.remove(bots[-1])\n",
    "\n",
    "win_rates = []\n",
    "avg_game_lengths = []\n",
    "bot0_avgs = []\n",
    "opp_avgs = []\n",
    "\n",
    "data_fraction = 1/5\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "\n",
    "  replay_buffer_actions = []\n",
    "  replay_buffer_blocks = []\n",
    "  replay_buffer_challenges = []\n",
    "  replay_buffer_cards = []\n",
    "\n",
    "  print(f'episode {episode} of {num_episodes}')\n",
    "  print(f'epsilon: {epsilon}')\n",
    "  print(f'gamma: {gamma}')\n",
    "\n",
    "  # discard_piles, acting_players, reacting_players, current_players, actions_game, reactions_game, challenges_game, cards_game, coins_game, challenges_direction, done, rewards, cards_chosen, bots_copy = game_loop_random(bots, actions, influences_reverse, epsilon)\n",
    "  # bots = bots_copy\n",
    "  state = torch.empty((0, 12), dtype=torch.float32)  # Assume state_size = 25 for action network\n",
    "  state_block = torch.empty((0, 13), dtype=torch.float32)\n",
    "  state_challenge = torch.empty((0, 13), dtype=torch.float32)\n",
    "  state_card = torch.empty((0, 13), dtype=torch.float32)\n",
    "  # states_action = torch.empty((0, 24), dtype=torch.float32)  # Assume state_size = 25 for action network\n",
    "  # next_states_action = torch.empty((0, 24), dtype=torch.float32)\n",
    "  actions_main = torch.empty((0,), dtype=torch.int64)\n",
    "  # states_block = torch.empty((0, 23), dtype=torch.float32)  # Assume state_size = 24 for block network\n",
    "  # next_states_block = torch.empty((0, 23), dtype=torch.float32)\n",
    "  actions_block = torch.empty((0,), dtype=torch.int64)\n",
    "  # states_challenge = torch.empty((0, 24), dtype=torch.float32)  # Assume state_size = 25 for challenge network\n",
    "  # next_states_challenge = torch.empty((0, 24), dtype=torch.float32)\n",
    "  actions_challenge = torch.empty((0,), dtype=torch.int64)\n",
    "  # states_card = torch.empty((0, 19), dtype=torch.float32)  # Assume state_size = 20 for card network\n",
    "  # next_states_card = torch.empty((0, 19), dtype=torch.float32)\n",
    "  actions_card = torch.empty((0,), dtype=torch.int64)\n",
    "  rewards = torch.empty((0,), dtype=torch.float32)\n",
    "  done = torch.empty((0,), dtype=torch.float32)\n",
    "  game_length_sum = 0\n",
    "  all_discard_piles = []\n",
    "  acting_players = []\n",
    "  reacting_players = []\n",
    "  reactions_game = []\n",
    "\n",
    "  num_games = 0\n",
    "\n",
    "\n",
    "  while len(state) <= 75 * batch_size:\n",
    "    # print(f'game {num_games}')\n",
    "\n",
    "    discard_pile, acting_player, reacting_player, current_player, action_game, reaction_game, challenge_game, card_game, coin_game, challenge_direction, done_0, reward, card_chosen, bots_copy = game_loop_random(bots, actions, influences_reverse, epsilon)\n",
    "\n",
    "    num_games += 1\n",
    "    # print(f'Game Number {num_games}')\n",
    "\n",
    "    # if random.random():\n",
    "    #   print(reacting_player)\n",
    "\n",
    "    # start_index = int(3 * len(acting_player) / list_division)\n",
    "\n",
    "    bots = bots_copy\n",
    "\n",
    "    game_length_sum += len(acting_player)\n",
    "\n",
    "    split_point = int((1 - data_fraction) * len(acting_player))\n",
    "    acting_players += acting_player\n",
    "    reacting_players += reacting_player\n",
    "    current_players = current_player\n",
    "    actions_game = action_game\n",
    "    reactions_game += reaction_game\n",
    "    challenges_game = challenge_game\n",
    "    cards_game = card_game\n",
    "    coins_game = coin_game\n",
    "    challenges_direction = challenge_direction\n",
    "    cards_chosen = card_chosen\n",
    "    discard_piles = discard_pile\n",
    "    all_discard_piles += discard_pile\n",
    "\n",
    "    avg_game_lengths.append(game_length_sum / 100)\n",
    "\n",
    "    # 1. Cards in play (embedded):\n",
    "    all_cards_in_play_embedded = []\n",
    "\n",
    "    for current_discard_pile in discard_pile:\n",
    "        cards_in_play_embedded = []\n",
    "        for card_name in influences.keys():\n",
    "            num_in_discard = current_discard_pile.count(inf_map[card_name])\n",
    "            num_in_play = 3 - num_in_discard\n",
    "            cards_in_play_embedded.append(torch.tensor(num_in_play)) # Remove .tolist() here\n",
    "\n",
    "        all_cards_in_play_embedded.append(torch.stack(cards_in_play_embedded)) # Stack the embedded tensors here\n",
    "\n",
    "    # Convert to a single tensor outside the loop\n",
    "    all_cards_in_play_embedded = torch.stack(all_cards_in_play_embedded)\n",
    "\n",
    "    # 4. Bot 0's normalized coins:\n",
    "    bot0_coins_normalized = torch.tensor(coin_game)[:, 0] / 12  # Get Bot 0's coins and normalize\n",
    "\n",
    "    # 5. Average cards of other players (normalized and embedded):\n",
    "    avg_other_cards_normalized = []\n",
    "    for step_cards in cards_game:\n",
    "        other_bots_cards = [len([card for card in bot_cards if card != 0])\n",
    "                          for bot_cards in step_cards[1:]]  # Exclude Bot 0\n",
    "        avg_other_cards = sum(other_bots_cards) / len(other_bots_cards) if other_bots_cards else 0\n",
    "        avg_other_cards_normalized.append(avg_other_cards / 2)\n",
    "\n",
    "    avg_other_cards_normalized = torch.tensor(avg_other_cards_normalized)\n",
    "    # avg_other_cards_embedded = embedding_cards(torch.tensor(int(avg_other_cards_normalized))).tolist()  # Assuming embedding_cards is your embedding layer\n",
    "\n",
    "    # 6. Bot 0's current cards (embedded):\n",
    "    all_bot0_cards_embedded = []  # Store embedded cards for all steps\n",
    "\n",
    "    for step_cards in cards_game:\n",
    "        bot0_cards_embedded = []\n",
    "        for card in step_cards[0]:  # Get Bot 0's cards for this step\n",
    "            if card != 0:  # Assuming 0 represents the absence of a card\n",
    "                bot0_cards_embedded.extend(embedding_cards(torch.tensor(card)).tolist())\n",
    "\n",
    "        # If Bot 0 has no cards, add zero embeddings for consistency\n",
    "        while len(bot0_cards_embedded) < embedding_cards.embedding_dim * 2:  # Assuming 2 cards max\n",
    "            bot0_cards_embedded.extend([0] * embedding_cards.embedding_dim)\n",
    "\n",
    "        all_bot0_cards_embedded.append(torch.tensor(bot0_cards_embedded))  # Convert to tensor and store\n",
    "\n",
    "    all_bot0_cards_embedded = torch.stack(all_bot0_cards_embedded) # Stack to create a 2D tensor\n",
    "\n",
    "    # 7. The last action taken (embedded):\n",
    "    all_last_action_embedded = []  # Store embedded last actions for all steps\n",
    "\n",
    "    actions_game.insert(0, 7)  # Add a dummy action at the beginning\n",
    "    reaction_game.insert(0, 0)  # Add a dummy reaction at the beginning\n",
    "    challenges_game.insert(0, 0)  # Add a dummy challenge at the beginning\n",
    "\n",
    "    for i in range(len(actions_game[:-1])):\n",
    "        last_action = actions_game[i]  # Get the action for the current step\n",
    "        last_action_embedded = embedding_actions(torch.tensor(last_action)).tolist()\n",
    "        all_last_action_embedded.append(last_action_embedded)\n",
    "\n",
    "    all_last_action_embedded = torch.tensor(all_last_action_embedded)  # Convert to a tensor\n",
    "\n",
    "    new_state = torch.cat(([all_cards_in_play_embedded.unsqueeze(-1),\n",
    "                        bot0_coins_normalized.unsqueeze(-1).unsqueeze(-1),\n",
    "                        avg_other_cards_normalized.unsqueeze(-1).unsqueeze(-1),\n",
    "                        all_bot0_cards_embedded.unsqueeze(-1),\n",
    "                        all_last_action_embedded.unsqueeze(-1)]),\n",
    "                      1).squeeze(2)\n",
    "    state = torch.cat([state, new_state], 0)\n",
    "\n",
    "    # print(new_state.shape)\n",
    "\n",
    "    # print(torch.tensor(reactions_game[:-1]).shape)\n",
    "\n",
    "    new_state_block = torch.cat([new_state, torch.tensor(reaction_game[:-1]).unsqueeze(1)], 1)\n",
    "    state_block = torch.cat([state_block, new_state_block], 0)\n",
    "\n",
    "    new_state_challenge = torch.cat([new_state, torch.tensor(challenges_game[1:]).unsqueeze(1)], 1)\n",
    "    state_challenge = torch.cat([state_challenge, new_state_challenge], 0)\n",
    "\n",
    "    new_state_card = torch.cat([new_state, torch.tensor(cards_chosen).unsqueeze(1)], 1)\n",
    "    state_card = torch.cat([state_card, new_state_card], 0)\n",
    "\n",
    "    new_actions_main = torch.tensor(actions_game[1:]).type(torch.int64)\n",
    "    actions_main = torch.cat([actions_main, new_actions_main], 0)\n",
    "\n",
    "\n",
    "    new_actions_block = torch.tensor(reaction_game).type(torch.int64)\n",
    "    actions_block = torch.cat([actions_block, new_actions_block], 0)\n",
    "\n",
    "\n",
    "    new_actions_challenge = torch.tensor(challenges_game).type(torch.int64)\n",
    "    actions_challenge = torch.cat([actions_challenge, new_actions_challenge], 0)\n",
    "\n",
    "\n",
    "    new_actions_card = torch.tensor(cards_chosen).type(torch.int64)\n",
    "    actions_card = torch.cat([actions_card, new_actions_card], 0)\n",
    "\n",
    "    new_rewards = torch.tensor(reward).type(torch.float32)\n",
    "    rewards = torch.cat([rewards, new_rewards], 0)\n",
    "\n",
    "    new_done = torch.tensor(done_0).type(torch.float32)\n",
    "    done = torch.cat([done, new_done], 0)\n",
    "\n",
    "\n",
    "\n",
    "  print(f'Number of games in episode {episode}: {num_games}')\n",
    "\n",
    "  print(state.shape)\n",
    "  print(state_block.shape)\n",
    "  print(state_challenge.shape)\n",
    "  print(state_card.shape)\n",
    "\n",
    "  states_action = torch.empty((0, 12), dtype=torch.float32)\n",
    "  states_block = torch.empty((0, 13), dtype=torch.float32)\n",
    "  states_challenge = torch.empty((0, 13), dtype=torch.float32)\n",
    "  states_card = torch.empty((0, 13), dtype=torch.float32)\n",
    "\n",
    "  # Assuming you have a list called 'all_states' that contains all the states\n",
    "  # generated using the 'new_state' calculation you provided\n",
    "  # and 'acting_players' list that has acting players per state,\n",
    "  # and 'reacting_players' list for reacting players\n",
    "\n",
    "  # all_states = []  # Initialize with your existing state generation logic\n",
    "\n",
    "  # print(state.shape)\n",
    "  # print(all_bot0_cards_embedded.shape)\n",
    "\n",
    "  # Create a dictionary to store current states and their corresponding next states\n",
    "  state_transitions = defaultdict(list)\n",
    "\n",
    "  for i in range(len(state) - 1):  # Iterate through all states (except the last one)\n",
    "      # print(next_state[7:9])\n",
    "      current_state = state[i]\n",
    "      next_state = state[i + 1]\n",
    "\n",
    "      # Add the next state to the list of next states for the current state\n",
    "      state_transitions[tuple(current_state.tolist())].append(next_state)  # Convert to tuple for dictionary key\n",
    "\n",
    "  state_indices = {}\n",
    "\n",
    "  for i, state_tensor in enumerate(state):\n",
    "      state_indices[tuple(state_tensor.tolist())] = i\n",
    "\n",
    "  for i in range(len(state) - 1):  # Iterate through all states (except the last one)\n",
    "\n",
    "      current_state_action = state[i]\n",
    "      current_state_block = state_block[i]\n",
    "      current_state_challenge = state_challenge[i]\n",
    "      current_state_card = state_card[i]\n",
    "      # next_state = state[i + 1]\n",
    "\n",
    "      # --- Action Network ---\n",
    "      if acting_players[i] == 0:  # Check acting player for current state\n",
    "          states_action = torch.cat([states_action, current_state_action.unsqueeze(0)], 0)\n",
    "      # else:\n",
    "      #     next_states_action = torch.cat([next_states_action, next_state.unsqueeze(0)], 0)\n",
    "\n",
    "      # --- Reaction Network & Challenge Network ---\n",
    "      if reacting_players[i] == 0:  # Bot 0 is the reacting player\n",
    "          states_block = torch.cat([states_block, current_state_block.unsqueeze(0)], 0)\n",
    "      # else:\n",
    "      #     next_states_block = torch.cat([next_states_block, next_state.unsqueeze(0)], 0)\n",
    "\n",
    "      # --- Challenge Network ---\n",
    "      if reacting_players[i] == 0:\n",
    "          states_challenge = torch.cat([states_challenge, current_state_challenge.unsqueeze(0)], 0)\n",
    "      elif acting_players[i] == 0 and reactions_game[i+1] == 1: # Check acting player for current state\n",
    "          states_challenge = torch.cat([states_challenge, current_state_challenge.unsqueeze(0)], 0)\n",
    "      # else:\n",
    "      #     next_states_challenge = torch.cat([next_states_challenge, next_state.unsqueeze(0)], 0)\n",
    "\n",
    "      # --- Card Network ---\n",
    "      # Assuming 'all_discard_piles' contains discard piles for each state\n",
    "      # and 'all_bot0_cards_embedded' contains Bot 0's cards for each state\n",
    "\n",
    "      # current_discard_pile_size = len(all_discard_piles[i])\n",
    "      # next_discard_pile_size = len(all_discard_piles[i + 1])\n",
    "\n",
    "      # Check if Bot 0 lost a card in the transition\n",
    "      bot0_current_cards = state_card[i][7:9]\n",
    "      bot0_next_cards    = state_card[i+1][7:9]\n",
    "      if not torch.equal(bot0_current_cards, bot0_next_cards):\n",
    "\n",
    "          states_card = torch.cat([states_card, current_state_card.unsqueeze(0)], 0)\n",
    "      # else:\n",
    "      #     next_states_card = torch.cat([next_states_card, next_state.unsqueeze(0)], 0)\n",
    "\n",
    "\n",
    "  # print(state.shape)\n",
    "\n",
    "  # print(states_action.shape)\n",
    "  # print(next_states_action.shape)\n",
    "  # print(len(states_action) + len(next_states_action))\n",
    "\n",
    "  # print(states_block.shape)\n",
    "  # print(next_states_block.shape)\n",
    "  # print(len(states_block) + len(next_states_block))\n",
    "\n",
    "  # print(states_challenge.shape)\n",
    "  # print(next_states_challenge.shape)\n",
    "  # print(len(states_challenge) + len(next_states_challenge))\n",
    "\n",
    "  # print(states_card.shape)\n",
    "  # print(next_states_card.shape)\n",
    "  # print(len(states_card) + len(next_states_card))\n",
    "\n",
    "\n",
    "  losses_action = []\n",
    "  losses_block = []\n",
    "  losses_challenge = []\n",
    "  losses_card = []\n",
    "\n",
    "  ############################################\n",
    "# Example: On-the-fly building raw sequences\n",
    "#          and passing them into the RNN Q\n",
    "############################################\n",
    "  num_batches_action = len(states_action) // batch_size\n",
    "\n",
    "  for i in range(num_batches_action):\n",
    "    # -------------------------------------------------\n",
    "    # 1) Sample a batch of indices\n",
    "    # -------------------------------------------------\n",
    "    batch_indices_action = random.sample(\n",
    "        range(len(states_action)),\n",
    "        min(batch_size, len(states_action))\n",
    "    )\n",
    "\n",
    "    batch_states_action = torch.stack([states_action[j] for j in batch_indices_action])\n",
    "    batch_actions_main  = actions_main[batch_indices_action]  # shape (batch_size,)\n",
    "    # (Optional) Make sure batch_actions_main is torch.LongTensor\n",
    "    batch_actions_main  = torch.tensor(batch_actions_main, dtype=torch.long)\n",
    "\n",
    "    # Lists to store Q-values we compute for each item in this mini-batch\n",
    "    q_values_current_list = []\n",
    "    q_values_nextmax_list = []\n",
    "    rewards_list          = []\n",
    "    done_list             = []\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 2) Process each single-frame \"state\" in the batch\n",
    "    #    to build the raw sequences for current & next\n",
    "    # -------------------------------------------------\n",
    "    for idx, single_frame_state in enumerate(batch_states_action):\n",
    "        # ------------------\n",
    "        # Find global index\n",
    "        # ------------------\n",
    "        indices = torch.where((state == single_frame_state).all(dim=1))[0]\n",
    "        if len(indices) == 0:\n",
    "            # We did not find it => treat as terminal\n",
    "            # => Q(s) is whatever, we can do 0\n",
    "            # => Or we skip, but let's just do zero and done=1\n",
    "            q_values_current_list.append(torch.tensor(0.0))  # single float\n",
    "            q_values_nextmax_list.append(torch.tensor(0.0))\n",
    "            rewards_list.append(torch.tensor(0.0))\n",
    "            done_list.append(torch.tensor(1.0))\n",
    "            continue\n",
    "\n",
    "        state_index = indices[0].item()\n",
    "\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # (a) Build the \"current\" sequence: from last done to now\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        cur_seq_frames = []\n",
    "        back_index = state_index\n",
    "        while back_index >= 0 and done[back_index] != 1:\n",
    "            cur_seq_frames.insert(0, state[back_index])  # front => chronological\n",
    "            back_index -= 1\n",
    "        # shape => (seq_len, 12)\n",
    "        # try:\n",
    "        if len(cur_seq_frames) == 0:\n",
    "            cur_seq_frames.append(state[state_index])\n",
    "        cur_seq_tensor = torch.stack(cur_seq_frames, dim=0)\n",
    "        # except:\n",
    "        #   cur_seq_tensor = torch.stack(single_frame_state, dim=0)\n",
    "        # shape => (1, seq_len, 12)\n",
    "        cur_seq_tensor = cur_seq_tensor.unsqueeze(0)\n",
    "\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # (b) Build the \"next\" sequence\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        next_seq_frames = []\n",
    "        rewards_for_seq = []\n",
    "        current_index = state_index + 1\n",
    "        while (current_index < len(state) and acting_players[current_index] != 0):\n",
    "            next_seq_frames.append(state[current_index])\n",
    "            rewards_for_seq.append(rewards[current_index])\n",
    "            if done[current_index] == 1:\n",
    "                break\n",
    "            current_index += 1\n",
    "\n",
    "        # We will decide how to handle \"no next frames\"\n",
    "        if len(next_seq_frames) == 0:\n",
    "            # Means terminal or no next chunk\n",
    "            next_seq_tensor = None  # we'll handle it below\n",
    "        else:\n",
    "            next_seq_tensor = torch.stack(next_seq_frames, dim=0)\n",
    "            next_seq_tensor = next_seq_tensor.unsqueeze(0)  # shape (1, seq_len2, 12)\n",
    "\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # (c) Forward pass for the current sequence\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # shape => (1, action_dim) if batch_size=1\n",
    "        out_current = bot.action_q(cur_seq_tensor)\n",
    "        # We gather the chosen actions Q\n",
    "        chosen_action = batch_actions_main[idx]  # a single int\n",
    "        q_val_current = out_current[0, chosen_action]  # shape => scalar\n",
    "\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # (d) Forward pass for the next sequence (if it exists)\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        if next_seq_tensor is None:\n",
    "            # Terminal. Let's define next_max_Q = 0\n",
    "            q_val_nextmax = torch.tensor(0.0)\n",
    "            # Reward from the first step after current_index if in bounds\n",
    "            if current_index < len(rewards):\n",
    "                r = rewards[current_index]\n",
    "            else:\n",
    "                r = 0.0\n",
    "            done_flag = 1.0\n",
    "        else:\n",
    "            out_next = bot.action_q(next_seq_tensor)        # shape => (1, action_dim)\n",
    "            q_val_nextmax = out_next.max(dim=1)[0].squeeze() # scalar\n",
    "            # Reward is average of rewards_for_seq\n",
    "            if len(rewards_for_seq) > 0:\n",
    "                r = sum(rewards_for_seq) / len(rewards_for_seq)\n",
    "            else:\n",
    "                r = 0.0\n",
    "            # Are we done? If in range, check done[current_index]\n",
    "            if current_index < len(done):\n",
    "                done_flag = float(done[current_index])\n",
    "            else:\n",
    "                done_flag = 1.0\n",
    "\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # (e) Collect everything in lists\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        q_values_current_list.append(q_val_current)\n",
    "        q_values_nextmax_list.append(q_val_nextmax)\n",
    "        rewards_list.append(torch.tensor(r, dtype=torch.float32))\n",
    "        done_list.append(torch.tensor(done_flag, dtype=torch.float32))\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 3) Convert results to Tensors\n",
    "    #    so we can do a standard DQN loss\n",
    "    # -------------------------------------------------\n",
    "    # shape => (batch_size,)\n",
    "    q_current_t     = torch.stack(q_values_current_list)  # current Q chosen action\n",
    "    q_nextmax_t     = torch.stack(q_values_nextmax_list)\n",
    "    rewards_t       = torch.stack(rewards_list)\n",
    "    done_t          = torch.stack(done_list)\n",
    "\n",
    "    # DQN target\n",
    "    target_q = rewards_t + gamma * q_nextmax_t * (1.0 - done_t)\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 4) Compute loss & backprop\n",
    "    # -------------------------------------------------\n",
    "    loss_action = criterion(q_current_t, target_q)\n",
    "    bot.optimizer_action.zero_grad()\n",
    "    loss_action.backward()\n",
    "    bot.optimizer_action.step()\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 5) Remove used transitions from replay\n",
    "    #    (Optional, as in your original code)\n",
    "    # -------------------------------------------------\n",
    "    states_action_np = states_action.cpu().numpy()\n",
    "    actions_main_np  = actions_main.cpu().numpy()\n",
    "\n",
    "    states_action_np = np.delete(states_action_np, batch_indices_action, axis=0)\n",
    "    actions_main_np  = np.delete(actions_main_np,  batch_indices_action, axis=0)\n",
    "\n",
    "    states_action = torch.tensor(states_action_np, dtype=torch.float32)\n",
    "    actions_main  = torch.tensor(actions_main_np,  dtype=torch.int64)\n",
    "\n",
    "    losses_action.append(loss_action.item())\n",
    "\n",
    "\n",
    "    # print('action success')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  ###############################################################################\n",
    "# RNN-based DQN training loop for your \"block\" decision, without a summarizer\n",
    "###############################################################################\n",
    "  num_batches_block = len(states_block) // batch_size\n",
    "\n",
    "  for i in range(num_batches_block):\n",
    "    # 1) Sample a random batch of indices\n",
    "    batch_indices_block = random.sample(\n",
    "        range(len(states_block)),\n",
    "        min(batch_size, len(states_block))\n",
    "    )\n",
    "\n",
    "    # 2) Gather Tensors for this mini-batch\n",
    "    batch_states_block  = torch.stack([states_block[j] for j in batch_indices_block])\n",
    "    batch_actions_block = actions_block[batch_indices_block]  # shape (batch_size,)\n",
    "    # Make sure actions are long-int for gather\n",
    "    batch_actions_block = torch.tensor(batch_actions_block, dtype=torch.long)\n",
    "\n",
    "    # Lists to store results for each item in the batch\n",
    "    q_values_current_list = []\n",
    "    q_values_nextmax_list = []\n",
    "    rewards_list          = []\n",
    "    done_list             = []\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 3) For each single-frame in batch_states_block:\n",
    "    #    (a) Build \"current\" sequence (backwards)\n",
    "    #    (b) Build \"next\" sequence (forwards)\n",
    "    #    (c) Forward pass each through RNN\n",
    "    #    (d) Collect chosen-action Q, next-max Q, reward, and done\n",
    "    # ------------------------------------------------------------------\n",
    "    for idx, single_frame_state in enumerate(batch_states_block):\n",
    "        # 3.1) Find the global index in your big `state` array\n",
    "        indices = torch.where((state_block == single_frame_state).all(dim=1))[0]\n",
    "\n",
    "        if len(indices) == 0:\n",
    "            # If we didn't find it, treat as terminal\n",
    "            q_values_current_list.append(torch.tensor(0.0))\n",
    "            q_values_nextmax_list.append(torch.tensor(0.0))\n",
    "            rewards_list.append(torch.tensor(0.0))\n",
    "            done_list.append(torch.tensor(1.0))\n",
    "            continue\n",
    "\n",
    "        state_index = indices[0].item()\n",
    "\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # (a) Build \"current\" sequence by going backward until done\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        cur_seq_frames = []\n",
    "        back_index = state_index\n",
    "        while back_index >= 0 and done[back_index] != 1:\n",
    "            cur_seq_frames.insert(0, state_block[back_index])  # front => chronological\n",
    "            back_index -= 1\n",
    "        # If we ended up with an empty chunk, fallback to single frame\n",
    "        if len(cur_seq_frames) == 0:\n",
    "            cur_seq_frames.append(state_block[state_index])\n",
    "\n",
    "        # shape: (seq_len, 12)\n",
    "        cur_seq_tensor = torch.stack(cur_seq_frames, dim=0)\n",
    "        # shape: (1, seq_len, 12) for the RNN\n",
    "        cur_seq_tensor = cur_seq_tensor.unsqueeze(0)\n",
    "\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # (b) Build \"next\" sequence by going forward until done or not reacting\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        next_seq_frames = []\n",
    "        rewards_for_seq = []\n",
    "        current_idx = state_index + 1\n",
    "\n",
    "        while (current_idx < len(state) and reacting_players[current_idx] != 0):\n",
    "            next_seq_frames.append(state_block[current_idx])\n",
    "            rewards_for_seq.append(rewards[current_idx])\n",
    "            if done[current_idx] == 1:\n",
    "                break\n",
    "            current_idx += 1\n",
    "\n",
    "        # We will define \"no next frames\" => terminal\n",
    "        if len(next_seq_frames) == 0:\n",
    "            next_seq_tensor = None\n",
    "        else:\n",
    "            next_seq_tensor = torch.stack(next_seq_frames, dim=0)\n",
    "            next_seq_tensor = next_seq_tensor.unsqueeze(0)  # (1, seq_len2, 12)\n",
    "\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # (c) Forward pass: current sequence\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        out_current = bot.block_q(cur_seq_tensor)  # shape => (1, num_actions)\n",
    "        chosen_action = batch_actions_block[idx]\n",
    "        q_val_current = out_current[0, chosen_action]  # => scalar\n",
    "\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # (d) Forward pass: next sequence (if exists)\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        if next_seq_tensor is None:\n",
    "            # Terminal\n",
    "            q_val_nextmax = torch.tensor(0.0)\n",
    "            # Reward\n",
    "            if current_idx < len(rewards):\n",
    "                r = rewards[current_idx]\n",
    "            else:\n",
    "                r = 0.0\n",
    "            done_flag = 1.0\n",
    "        else:\n",
    "            out_next = bot.block_q(next_seq_tensor)         # => (1, num_actions)\n",
    "            q_val_nextmax = out_next.max(dim=1)[0].squeeze()  # => scalar\n",
    "\n",
    "            # Reward = average (or sum) across the forward chunk\n",
    "            if len(rewards_for_seq) > 0:\n",
    "                r = sum(rewards_for_seq) / len(rewards_for_seq)\n",
    "            else:\n",
    "                r = 0.0\n",
    "\n",
    "            # Done or not\n",
    "            if current_idx < len(done):\n",
    "                done_flag = float(done[current_idx])\n",
    "            else:\n",
    "                done_flag = 1.0\n",
    "\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # (e) Store in lists\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        q_values_current_list.append(q_val_current)\n",
    "        q_values_nextmax_list.append(q_val_nextmax)\n",
    "        rewards_list.append(torch.tensor(r, dtype=torch.float32))\n",
    "        done_list.append(torch.tensor(done_flag, dtype=torch.float32))\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 4) Convert the results to Tensors for a standard DQN update\n",
    "    # ------------------------------------------------------------------\n",
    "    q_current_t = torch.stack(q_values_current_list)   # shape (batch_size,)\n",
    "    q_nextmax_t = torch.stack(q_values_nextmax_list)   # shape (batch_size,)\n",
    "    rewards_t   = torch.stack(rewards_list)            # shape (batch_size,)\n",
    "    done_t      = torch.stack(done_list)               # shape (batch_size,)\n",
    "\n",
    "    target_q = rewards_t + gamma * q_nextmax_t * (1.0 - done_t)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 5) Compute the loss & backprop\n",
    "    # ------------------------------------------------------------------\n",
    "    loss_block = criterion(q_current_t, target_q)\n",
    "    bot.optimizer_block.zero_grad()\n",
    "    loss_block.backward()\n",
    "    bot.optimizer_block.step()\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 6) Remove these samples from your replay buffer (optional)\n",
    "    # ------------------------------------------------------------------\n",
    "    # Convert Tensors to NumPy for deletion\n",
    "    states_block_np  = states_block.cpu().numpy()\n",
    "    actions_block_np = actions_block.cpu().numpy()\n",
    "\n",
    "    states_block_np  = np.delete(states_block_np,  batch_indices_block, axis=0)\n",
    "    actions_block_np = np.delete(actions_block_np, batch_indices_block, axis=0)\n",
    "\n",
    "    states_block  = torch.tensor(states_block_np,  dtype=torch.float32)\n",
    "    actions_block = torch.tensor(actions_block_np, dtype=torch.int64)\n",
    "\n",
    "    # If you have next_states_block or others, remove them similarly...\n",
    "    # next_states_block = np.delete(...)\n",
    "\n",
    "    losses_block.append(loss_block.item())\n",
    "\n",
    "\n",
    "    # print('block success')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  ##############################################################################\n",
    "# RNN-based DQN loop for your \"challenge\" decision, without a separate summarizer\n",
    "##############################################################################\n",
    "\n",
    "#   num_batches_challenge = len(states_challenge) // batch_size\n",
    "\n",
    "#   for i in range(num_batches_challenge):\n",
    "#     # 1) Sample a batch of indices\n",
    "#     batch_indices_challenge = random.sample(\n",
    "#         range(len(states_challenge)),\n",
    "#         min(batch_size, len(states_challenge))\n",
    "#     )\n",
    "\n",
    "#     # 2) Gather the states & actions for this mini-batch\n",
    "#     batch_states_challenge  = torch.stack([states_challenge[j] for j in batch_indices_challenge])\n",
    "#     batch_actions_challenge = actions_challenge[batch_indices_challenge]\n",
    "#     batch_actions_challenge = torch.tensor(batch_actions_challenge, dtype=torch.long)\n",
    "\n",
    "#     # Lists to store results for the DQN update\n",
    "#     q_values_current_list = []\n",
    "#     q_values_nextmax_list = []\n",
    "#     rewards_list          = []\n",
    "#     done_list             = []\n",
    "\n",
    "#     # ----------------------------------------------------------------\n",
    "#     # 3) For each single-frame in batch_states_challenge:\n",
    "#     #    - Build backward-chunk (current)\n",
    "#     #    - Build forward-chunk (next) per your challenge logic\n",
    "#     #    - RNN forward pass -> Q-values\n",
    "#     # ----------------------------------------------------------------\n",
    "#     for idx, single_frame_state in enumerate(batch_states_challenge):\n",
    "#         # Try to locate this state in the global `state` array\n",
    "#         indices = torch.where((state_challenge == single_frame_state).all(dim=1))[0]\n",
    "\n",
    "#         if len(indices) == 0:\n",
    "#             # If not found, treat as terminal\n",
    "#             # Q(current) = 0, Q(next_max) = 0, reward=0, done=1\n",
    "#             q_values_current_list.append(torch.tensor(0.0))\n",
    "#             q_values_nextmax_list.append(torch.tensor(0.0))\n",
    "#             rewards_list.append(torch.tensor(0.0))\n",
    "#             done_list.append(torch.tensor(1.0))\n",
    "#             continue\n",
    "\n",
    "#         state_index = indices[0].item()\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # 3A) Build the backward-chunk for the current state\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         cur_seq_frames = []\n",
    "#         back_index = state_index\n",
    "#         # Move backward until we hit done=1 or index < 0\n",
    "#         # (You could also incorporate more challenge-specific conditions\n",
    "#         # if you want symmetrical logic with forward-chunk.)\n",
    "#         while back_index >= 0 and done[back_index] != 1:\n",
    "#             cur_seq_frames.insert(0, state_challenge[back_index])  # front => chronological order\n",
    "#             back_index -= 1\n",
    "\n",
    "#         if len(cur_seq_frames) == 0:\n",
    "#             # fallback to single frame if we got nothing\n",
    "#             cur_seq_frames.append(state_challenge[state_index])\n",
    "\n",
    "#         # Shape: (seq_len, 12)\n",
    "#         cur_seq_tensor = torch.stack(cur_seq_frames, dim=0)\n",
    "#         # Shape: (1, seq_len, 12) for the RNN\n",
    "#         cur_seq_tensor = cur_seq_tensor.unsqueeze(0)\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # 3B) Build the forward-chunk for the next state\n",
    "#         #     Use your \"challenge\" condition:\n",
    "#         #         reacting_players[idx] != 0\n",
    "#         #         AND (acting_players[idx] != 0 OR reactions_game[idx] != 1)\n",
    "#         #         AND done[idx] != 1\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         next_seq_frames = []\n",
    "#         rewards_for_seq = []\n",
    "#         current_idx     = state_index + 1\n",
    "\n",
    "#         while (\n",
    "#             current_idx < len(state) and reacting_players[idx] != 0\n",
    "#                 and (acting_players[idx] != 0 or reactions_game[idx] != 1)):\n",
    "#             next_seq_frames.append(state_challenge[current_idx])\n",
    "#             rewards_for_seq.append(rewards[current_idx])\n",
    "#             if done[current_idx] == 1:\n",
    "#                 break\n",
    "#             current_idx += 1\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # RNN Forward pass for the \"current\" sequence\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         out_current = bot.challenge_q(cur_seq_tensor)  # shape (1, num_actions)\n",
    "#         chosen_action = batch_actions_challenge[idx]\n",
    "#         q_val_current = out_current[0, chosen_action]   # => scalar\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # RNN Forward pass for the \"next\" sequence\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         q_val_nextmax = torch.tensor(0.0)\n",
    "#         r = 0.0\n",
    "#         done_flag = 1.0\n",
    "#         if len(next_seq_frames) == 0:\n",
    "#             # Terminal\n",
    "#             q_val_nextmax = torch.tensor(0.0)\n",
    "#             # Reward:\n",
    "#             if current_idx < len(rewards):\n",
    "#                 r = rewards[current_idx]\n",
    "#             else:\n",
    "#                 r = 0.0\n",
    "#             done_flag = 1.0\n",
    "#         else:\n",
    "#             # Non-terminal\n",
    "#             next_seq_tensor = torch.stack(next_seq_frames, dim=0).unsqueeze(0)\n",
    "#             out_next        = bot.challenge_q(next_seq_tensor)   # shape (1, num_actions)\n",
    "#             q_val_nextmax   = out_next.max(dim=1)[0].squeeze()   # => scalar\n",
    "\n",
    "#             # Reward as average (or adjust logic as you see fit)\n",
    "#             if len(rewards_for_seq) > 0:\n",
    "#                 r = sum(rewards_for_seq) / len(rewards_for_seq)\n",
    "#             else:\n",
    "#                 r = 0.0\n",
    "\n",
    "#             if current_idx < len(done):\n",
    "#                 done_flag = float(done[current_idx])\n",
    "#             else:\n",
    "#                 done_flag = 1.0  # out of bounds => terminal\n",
    "\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         # Collect everything for the DQN update\n",
    "#         # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#         q_values_current_list.append(q_val_current)\n",
    "#         q_values_nextmax_list.append(q_val_nextmax)\n",
    "#         rewards_list.append(torch.tensor(r, dtype=torch.float32))\n",
    "#         done_list.append(torch.tensor(done_flag, dtype=torch.float32))\n",
    "\n",
    "#     # ----------------------------------------------------------------\n",
    "#     # 4) Convert to Tensors & compute DQN loss\n",
    "#     # ----------------------------------------------------------------\n",
    "#     q_current_t = torch.stack(q_values_current_list)  # shape (batch_size,)\n",
    "#     q_nextmax_t = torch.stack(q_values_nextmax_list)  # shape (batch_size,)\n",
    "#     rewards_t   = torch.stack(rewards_list)           # shape (batch_size,)\n",
    "#     done_t      = torch.stack(done_list)              # shape (batch_size,)\n",
    "\n",
    "#     target_q = rewards_t + gamma * q_nextmax_t * (1.0 - done_t)\n",
    "\n",
    "#     loss_challenge = criterion(q_current_t, target_q)\n",
    "\n",
    "#     # ----------------------------------------------------------------\n",
    "#     # 5) Backprop & optimize\n",
    "#     # ----------------------------------------------------------------\n",
    "#     bot.optimizer_challenge.zero_grad()\n",
    "#     loss_challenge.backward()\n",
    "#     bot.optimizer_challenge.step()\n",
    "\n",
    "#     # ----------------------------------------------------------------\n",
    "#     # 6) Remove these samples from your replay buffer\n",
    "#     # ----------------------------------------------------------------\n",
    "#     # Convert Tensors -> NumPy for np.delete\n",
    "#     states_challenge_np  = states_challenge.cpu().numpy()\n",
    "#     actions_challenge_np = actions_challenge.cpu().numpy()\n",
    "#     # If you have \"next_states_challenge\" or others, similarly convert them\n",
    "\n",
    "#     states_challenge_np  = np.delete(states_challenge_np,  batch_indices_challenge, axis=0)\n",
    "#     actions_challenge_np = np.delete(actions_challenge_np, batch_indices_challenge, axis=0)\n",
    "\n",
    "#     # Rebuild your PyTorch Tensors\n",
    "#     states_challenge  = torch.tensor(states_challenge_np,  dtype=torch.float32)\n",
    "#     actions_challenge = torch.tensor(actions_challenge_np, dtype=torch.int64)\n",
    "\n",
    "#     # If you store next_states_challenge, remove them as well:\n",
    "#     # next_states_challenge_np = next_states_challenge.cpu().numpy()\n",
    "#     # next_states_challenge_np = np.delete(next_states_challenge_np, batch_indices_challenge, axis=0)\n",
    "#     # next_states_challenge = torch.tensor(next_states_challenge_np, dtype=torch.float32)\n",
    "\n",
    "#     losses_challenge.append(loss_challenge.item())\n",
    "\n",
    "\n",
    "    # print('challenge success')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  ##############################################################################\n",
    "# RNN-based training loop for your \"card\" decision,\n",
    "# building raw backward and forward sequences on-the-fly\n",
    "##############################################################################\n",
    "  num_batches_card = len(states_card) // batch_size\n",
    "\n",
    "  for i in range(num_batches_card):\n",
    "    if num_batches_card == 0:\n",
    "      raise Exception(\"Something went wrong\")\n",
    "\n",
    "    # for i in range(num_batches_card):\n",
    "\n",
    "    # 1) Sample batch indices\n",
    "    batch_indices_card = random.sample(\n",
    "        range(len(states_card)),\n",
    "        min(batch_size, len(states_card))\n",
    "    )\n",
    "\n",
    "    # 2) Gather states & actions for this batch\n",
    "    batch_states_card  = torch.stack([states_card[j] for j in batch_indices_card])  # (batch_size, 12)\n",
    "    batch_actions_card = actions_card[batch_indices_card]                           # (batch_size,)\n",
    "    batch_actions_card = torch.tensor(batch_actions_card, dtype=torch.long)\n",
    "\n",
    "    # We'll collect Q-values and targets for the entire batch\n",
    "    q_current_list = []\n",
    "    q_nextmax_list = []\n",
    "    reward_list    = []\n",
    "    done_list      = []\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # 3) For each single-frame in batch_states_card, build backward & forward\n",
    "    # ------------------------------------------------------------------------\n",
    "    for idx, single_frame_state in enumerate(batch_states_card):\n",
    "        # Locate this state in the global 'state' buffer\n",
    "        indices = torch.where((state_card == single_frame_state).all(dim=1))[0]\n",
    "        if len(indices) == 0:\n",
    "            # Not found => treat as terminal\n",
    "            # Q(current) = 0, Q(next) = 0, reward=0, done=1\n",
    "            q_current_list.append(torch.tensor(0.0))\n",
    "            q_nextmax_list.append(torch.tensor(0.0))\n",
    "            reward_list.append(torch.tensor(0.0))\n",
    "            done_list.append(torch.tensor(1.0))\n",
    "            continue\n",
    "\n",
    "        state_index = indices[0].item()\n",
    "\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # 3A) BACKWARD chunk for \"current\" state\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        cur_seq_frames = []\n",
    "        back_idx = state_index\n",
    "\n",
    "        # Move backward until done=1 or out of array\n",
    "        while back_idx >= 0 and done[back_idx] != 1:\n",
    "            cur_seq_frames.insert(0, state_card[back_idx])  # insert at front => chronological order\n",
    "            back_idx -= 1\n",
    "\n",
    "        if len(cur_seq_frames) == 0:\n",
    "            # fallback to just the single frame\n",
    "            cur_seq_frames.append(state_card[state_index])\n",
    "\n",
    "        # Turn into shape (1, seq_len, 12) for the RNN\n",
    "        cur_seq_tensor = torch.stack(cur_seq_frames, dim=0).unsqueeze(0)\n",
    "\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # 3B) FORWARD chunk for \"next\" state, until:\n",
    "        #     - done=1\n",
    "        #     - Bot 0's cards change (state[..., 7:9] differs from next step)\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        fwd_seq_frames = []\n",
    "        rewards_for_seq = []\n",
    "        current_idx = state_index + 1\n",
    "\n",
    "        while current_idx < len(state):\n",
    "\n",
    "            # Append the current frame\n",
    "            fwd_seq_frames.append(state_card[current_idx])\n",
    "            rewards_for_seq.append(rewards[current_idx])\n",
    "\n",
    "            # Check if the *next* step changes Bot 0's cards\n",
    "            # so we break *after* including the current frame if it\n",
    "            # leads to a change in [7:9].\n",
    "            if current_idx + 1 < len(state):\n",
    "                # Compare the slice [7:9] of the current vs. the next\n",
    "                bot0_current_cards = state_card[current_idx][7:9]\n",
    "                bot0_next_cards    = state_card[current_idx+1][7:9]\n",
    "                if not torch.equal(bot0_current_cards, bot0_next_cards):\n",
    "                    # Bot 0's cards changed => break\n",
    "                    current_idx += 1  # increment so we include the reward\n",
    "                    break\n",
    "\n",
    "            # Stop if done=1\n",
    "            if done[current_idx] == 1:\n",
    "                break\n",
    "\n",
    "            # If no change, keep going\n",
    "            current_idx += 1\n",
    "\n",
    "            # Also break if weve run out of array (the while condition checks that anyway)\n",
    "            # but well rely on the loop condition.\n",
    "\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # Forward pass in the RNN for the \"current\" sequence\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        out_current = bot.card_q(cur_seq_tensor)     # shape: (1, num_actions)\n",
    "        # pick the Q-value for the chosen action\n",
    "        chosen_action = batch_actions_card[idx]\n",
    "        q_val_current = out_current[0, chosen_action] # => scalar\n",
    "\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # Forward pass for the \"next\" sequence\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        if len(fwd_seq_frames) == 0:\n",
    "            # Terminal if we got no next frames\n",
    "            q_val_nextmax = torch.tensor(0.0)\n",
    "            # Reward\n",
    "            if current_idx < len(state):\n",
    "                r = rewards[current_idx]\n",
    "            else:\n",
    "                r = 0.0\n",
    "            done_flag = 1.0\n",
    "        else:\n",
    "            # Non-terminal\n",
    "            fwd_seq_tensor = torch.stack(fwd_seq_frames, dim=0).unsqueeze(0)\n",
    "            out_next       = bot.card_q(fwd_seq_tensor)  # shape (1, num_actions)\n",
    "            q_val_nextmax  = out_next.max(dim=1)[0].squeeze()      # => scalar\n",
    "\n",
    "            # Could do average or sum of rewards\n",
    "            if len(rewards_for_seq) > 0:\n",
    "                r = sum(rewards_for_seq) / len(rewards_for_seq)\n",
    "            else:\n",
    "                r = 0.0\n",
    "\n",
    "            # done flag\n",
    "            if current_idx < len(done):\n",
    "                done_flag = float(done[current_idx])\n",
    "            else:\n",
    "                done_flag = 1.0\n",
    "\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        # Collect results for the DQN update\n",
    "        # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "        q_current_list.append(q_val_current)\n",
    "        q_nextmax_list.append(q_val_nextmax)\n",
    "        reward_list.append(torch.tensor(r, dtype=torch.float32))\n",
    "        done_list.append(torch.tensor(done_flag, dtype=torch.float32))\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 4) Convert to Tensors & compute the DQN target\n",
    "    # ------------------------------------------------------------------\n",
    "    q_current_t = torch.stack(q_current_list)   # (batch_size,)\n",
    "    q_nextmax_t = torch.stack(q_nextmax_list)   # (batch_size,)\n",
    "    reward_t    = torch.stack(reward_list)      # (batch_size,)\n",
    "    done_t      = torch.stack(done_list)        # (batch_size,)\n",
    "\n",
    "    # DQN target: r + gamma * max(Q(next)) * (1 - done)\n",
    "    target_q = reward_t + gamma * q_nextmax_t * (1.0 - done_t)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 5) Compute loss & optimize\n",
    "    # ------------------------------------------------------------------\n",
    "    loss_card = criterion(q_current_t, target_q)\n",
    "\n",
    "    bot.optimizer_card.zero_grad()\n",
    "    loss_card.backward()\n",
    "    bot.optimizer_card.step()\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 6) Remove these samples from the replay buffer\n",
    "    # ------------------------------------------------------------------\n",
    "    states_card_np   = states_card.cpu().numpy()\n",
    "    actions_card_np  = actions_card.cpu().numpy()\n",
    "    # If you have next_states_card, similarly convert & remove\n",
    "\n",
    "    states_card_np   = np.delete(states_card_np,  batch_indices_card, axis=0)\n",
    "    actions_card_np  = np.delete(actions_card_np, batch_indices_card, axis=0)\n",
    "\n",
    "    states_card  = torch.tensor(states_card_np, dtype=torch.float32)\n",
    "    actions_card = torch.tensor(actions_card_np, dtype=torch.int64)\n",
    "\n",
    "    losses_card.append(loss_card.item())\n",
    "\n",
    "\n",
    "\n",
    "  # bot.cards = bots[0].cards\n",
    "  # bot.num_coins = bots[0].num_coins\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # i += 1\n",
    "  epsilon *= 0.9995\n",
    "\n",
    "  # if (episode + 1) % 100 == 0:\n",
    "  #   data_fraction = min(data_fraction + 1/5, 1)\n",
    "  #   epsilon = 1.0\n",
    "\n",
    "  # gamma = min(0.99, gamma + 0.001)\n",
    "\n",
    "  avg_losses_action.append(sum(losses_action) / len(losses_action))\n",
    "  avg_losses_block.append(sum(losses_block) / len(losses_block))\n",
    "#   avg_losses_challenge.append(sum(losses_challenge) / len(losses_challenge))\n",
    "  avg_losses_card.append(sum(losses_card) / len(losses_card))\n",
    "\n",
    "  print(f'Avg Action Loss, {num_batches_action} batches: {avg_losses_action[-1]}')\n",
    "  print(f'Avg Block Loss, {num_batches_block} batches: {avg_losses_block[-1]}')\n",
    "#   print(f'Avg Challenge Loss, {num_batches_challenge} batches: {avg_losses_challenge[-1]}')\n",
    "  print(f'Avg Card Loss, {num_batches_card} batches: {avg_losses_card[-1]}')\n",
    "\n",
    "  # Copy parameters of action_q network\n",
    "  bots[0].action_q.load_state_dict(bot.action_q.state_dict())\n",
    "\n",
    "  # Copy parameters of block_q network\n",
    "  bots[0].block_q.load_state_dict(bot.block_q.state_dict())\n",
    "\n",
    "  # Copy parameters of challenge_q network\n",
    "#   bots[0].challenge_q.load_state_dict(bot.challenge_q.state_dict())\n",
    "\n",
    "  # Copy parameters of card_q network\n",
    "  bots[0].card_q.load_state_dict(bot.card_q.state_dict())\n",
    "\n",
    "  def verify_allclose(net_a, net_b, eps=1e-6):\n",
    "    for p_a, p_b in zip(net_a.parameters(), net_b.parameters()):\n",
    "        if not torch.allclose(p_a, p_b, atol=eps, rtol=1e-5):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "  print(\"Action Q matches?\",\n",
    "        verify_allclose(bots[0].action_q, bot.action_q))\n",
    "  print(\"Block Q matches?\",\n",
    "        verify_allclose(bots[0].block_q, bot.block_q))\n",
    "#   print(\"Challenge Q matches?\",\n",
    "#         verify_allclose(bots[0].challenge_q, bot.challenge_q))\n",
    "  print(\"Card Q matches?\",\n",
    "        verify_allclose(bots[0].card_q, bot.card_q))\n",
    "\n",
    "\n",
    "  print(bots[0].name)\n",
    "\n",
    "  if episode % 10 == 0:\n",
    "\n",
    "    win_rate = 0\n",
    "\n",
    "    bot0_actions = []\n",
    "\n",
    "    game_lengths = 0\n",
    "    \n",
    "    coins_game = []\n",
    "    dones_eval = []\n",
    "\n",
    "    for i in range(100):\n",
    "\n",
    "        discard_pile, acting_player, reacting_player, current_player, action_game, reaction_game, challenge_game, card_game, coin_game, challenge_direction, done_0, reward, card_chosen, bots_copy = game_loop_random(bots, actions, influences_reverse, 0.0)\n",
    "        bots = bots_copy\n",
    "\n",
    "        game_lengths += len(action_game)\n",
    "\n",
    "        for actor, action in zip(acting_player, action_game):\n",
    "            if actor == 0:\n",
    "                bot0_actions.append(action)\n",
    "\n",
    "        if reward[-1] == 1.0:\n",
    "            win_rate += 1\n",
    "            \n",
    "        coins_game += coin_game\n",
    "        dones_eval += done_0\n",
    "                \n",
    "\n",
    "    if bot0_actions:  # ensure the list is not empty\n",
    "        counter = Counter(bot0_actions)\n",
    "        most_common_action, count = counter.most_common(1)[0]\n",
    "        print(f'Most common action for Bot 0: {most_common_action} taken {count} times.')\n",
    "        print(f'total game lengths: {game_lengths}')\n",
    "    \n",
    "    bot0_coin_counts = []\n",
    "    opp_coin_counts = []\n",
    "    done_flag = 0\n",
    "    for c in range(len(dones_eval) - 1):\n",
    "        if dones_eval[c] == 1 and done_flag == 0:\n",
    "            done_flag = 1\n",
    "            bot0_coin_counts.append(coins_game[c][0])\n",
    "            opp_coin_counts.append(coins_game[c][1])\n",
    "        elif dones_eval[c] == 0: \n",
    "            done_flag = 0\n",
    "\n",
    "    win_rate = win_rate / 100\n",
    "    win_rates.append(win_rate)\n",
    "\n",
    "    print(f'win rate: {win_rate}')\n",
    "    \n",
    "    bot0_avg = np.mean(bot0_coin_counts)\n",
    "    opp_avg = np.mean(opp_coin_counts)\n",
    "    \n",
    "    bot0_avgs.append(bot0_avg)\n",
    "    opp_avgs.append(opp_avg)\n",
    "    \n",
    "    print(f'bot 0 average final coin count: {bot0_avg}')\n",
    "    print(f'opponent average final coin count: {opp_avg}')\n",
    "\n",
    "\n",
    "  # df = pd.DataFrame(data = data)\n",
    "  # print(df.head())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
