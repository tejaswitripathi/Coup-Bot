{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 450,
      "id": "17ca9a8f",
      "metadata": {
        "id": "17ca9a8f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 451,
      "id": "6a2f9eb3",
      "metadata": {
        "id": "6a2f9eb3"
      },
      "outputs": [],
      "source": [
        "actions_map = {\n",
        "    0: 'take 1 coin',\n",
        "    1: 'coup',\n",
        "    2: 'take 2 coins',\n",
        "    3: 'take 3 coins',\n",
        "    4: 'steal 2 coins',\n",
        "    5: 'assassinate',\n",
        "    6: 'exchange',\n",
        "    7: 'challenge',\n",
        "    8: 'block foreign aid',\n",
        "    9: 'block stealing',\n",
        "    10: 'block assassination'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 452,
      "id": "4d15621e",
      "metadata": {
        "id": "4d15621e"
      },
      "outputs": [],
      "source": [
        "class Action:\n",
        "    def __init__(self, name, challengeable, response_card, response_action,\n",
        "                 p1_net_coins, p2_net_coins, p1_net_cards, p2_net_cards, vector):\n",
        "        self.name = name\n",
        "        self.challengeable = challengeable\n",
        "        self.response_card = response_card\n",
        "        self.response_action = response_action\n",
        "        self.p1_net_coins = p1_net_coins\n",
        "        self.p2_net_coins = p2_net_coins\n",
        "        self.p1_net_cards = p1_net_cards\n",
        "        self.p2_net_cards = p2_net_cards\n",
        "#         self.base_utility = base_utility\n",
        "#         self.p_bluff = p_bluff\n",
        "        self.vector = vector\n",
        "\n",
        "    def update_responses(self, response_card, response_action):\n",
        "        self.response_card = response_card\n",
        "        self.response_action = response_action\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 453,
      "id": "baef8cbb",
      "metadata": {
        "id": "baef8cbb"
      },
      "outputs": [],
      "source": [
        "take_1 = Action(actions_map[0], False, None, None, 1, 0, 0, 0, [0])\n",
        "\n",
        "coup = Action(actions_map[1], False, None, None, -7, 0, 0, -1, [1])\n",
        "\n",
        "take_2 = Action(actions_map[2], True, 'Duke', actions_map[8], 2, 0, 0, 0, [2])\n",
        "\n",
        "take_3 = Action(actions_map[3], True, None, actions_map[7], 3, 0, 0, 0, [3])\n",
        "\n",
        "steal_2 = Action(actions_map[4], True, ['Captain', 'Ambassador'], actions_map[9], 2, -2, 0, 0,[4])\n",
        "\n",
        "assassinate = Action(actions_map[5], True, 'Contessa', actions_map[10], -3, 0, 0, -1, [5])\n",
        "\n",
        "exchange = Action(actions_map[6], True, None, actions_map[7], 0, 0, 0, 0,[6])\n",
        "\n",
        "# challenge = Action(actions_map[7], False, None, None, 0, 0, -1, -1, 1, 0)\n",
        "\n",
        "block_take_2 = Action(actions_map[8], True, None, actions_map[7], 0, -2, 0, 0, [7])\n",
        "\n",
        "block_steal = Action(actions_map[9], True, None, actions_map[7], 2, -2, 0, 0, [8])\n",
        "\n",
        "block_assassination = Action(actions_map[10], True, None, actions_map[7], 0, 0, 1, 0, [9])\n",
        "\n",
        "# challenge =\n",
        "\n",
        "actions = {\n",
        "    0: take_1,\n",
        "    1: coup,\n",
        "    2: take_2,\n",
        "    3: take_3,\n",
        "    4: steal_2,\n",
        "    5: assassinate,\n",
        "    6: exchange,\n",
        "    7: block_take_2,\n",
        "    8: block_steal,\n",
        "    9: block_assassination\n",
        "}\n",
        "\n",
        "take_2.response_action = actions[7]\n",
        "steal_2.response_action = actions[8]\n",
        "assassinate.response_action = actions[9]\n",
        "\n",
        "influences = {\n",
        "    'Duke': [take_3, block_take_2, take_1, coup],\n",
        "    'Captain': [steal_2, block_steal, take_2, take_1, coup],\n",
        "    'Assassin': [assassinate, take_2, take_1, coup],\n",
        "    'Contessa': [take_2, block_assassination, take_1, coup],\n",
        "    'Ambassador': [exchange, block_steal, take_2, take_1, coup]\n",
        "    }\n",
        "\n",
        "inf_map = {\n",
        "    'Dead': 0,\n",
        "    'Duke': 1,\n",
        "    'Captain': 2,\n",
        "    'Assassin': 3,\n",
        "    'Contessa': 4,\n",
        "    'Ambassador': 5,\n",
        "    'Hidden': 6\n",
        "}\n",
        "\n",
        "\n",
        "influences_reverse = {\n",
        "    take_1: ['Duke', 'Captain', 'Assassin', 'Contessa', 'Ambassador'],\n",
        "    coup: ['Duke', 'Captain', 'Assassin', 'Contessa', 'Ambassador'],\n",
        "    take_2: ['Captain', 'Assassin', 'Contessa', 'Ambassador'],\n",
        "    take_3: ['Duke'],\n",
        "    steal_2: ['Captain'],\n",
        "    assassinate: ['Assassin'],\n",
        "    exchange: ['Ambassador'],\n",
        "    block_take_2: ['Duke'],\n",
        "    block_steal: ['Captain','Ambassador'],\n",
        "    block_assassination: ['Contessa']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 454,
      "id": "d617da4f",
      "metadata": {
        "id": "d617da4f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size, hidden_size=128):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        # self.dropout = nn.Dropout(0.2)\n",
        "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc3 = nn.Linear(hidden_size, 64)\n",
        "        self.fc4 = nn.Linear(64, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: shape (batch_size, seq_len, state_size)\n",
        "        returns: shape (batch_size, action_size)\n",
        "        \"\"\"\n",
        "        # 1) Pass each frame in the sequence through a linear + ReLU\n",
        "        #    => shape (batch_size, seq_len, hidden_size)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        # x = self.dropout(x)\n",
        "\n",
        "        # 2) Pass entire sequence through RNN\n",
        "        #    => out: shape (batch_size, seq_len, hidden_size)\n",
        "        out, _ = self.rnn(x)\n",
        "\n",
        "        # 3) We want the LAST hidden vector from the sequence (i.e. out[:, -1, :])\n",
        "        last_out = out[:, -1, :]  # shape (batch_size, hidden_size)\n",
        "\n",
        "        # 4) Pass to fully connected heads for Q-values\n",
        "        x = self.fc3(last_out)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc4(x)  # shape (batch_size, action_size)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "embedding_cards = nn.Embedding(7, 1)\n",
        "cards_tens = torch.tensor([0,1,2,3,4,5,6])\n",
        "cards_emb = embedding_cards(cards_tens)\n",
        "\n",
        "# actions = {\n",
        "#     0: take_1,\n",
        "#     1: coup,\n",
        "#     2: take_2,\n",
        "#     3: take_3,\n",
        "#     4: steal_2,\n",
        "#     5: assassinate,\n",
        "#     6: exchange,\n",
        "#     7: block_take_2,\n",
        "#     8: block_steal,\n",
        "#     9: block_assassination\n",
        "# }\n",
        "\n",
        "embedding_actions = nn.Embedding(8, 3)\n",
        "actions_tens = torch.tensor([0,1,2,3,4,5,6,7])\n",
        "actions_emb = embedding_actions(actions_tens)\n",
        "\n",
        "embedding_coins = nn.Embedding(13, 2)\n",
        "coins_tens = torch.tensor([0,1,2,3,4,5,6,7,8,9,10,11,12])\n",
        "coins_emb = embedding_coins(coins_tens)\n",
        "\n",
        "embedding_players = nn.Embedding(5, 3)\n",
        "players_tens = torch.tensor([0,1,2,3,4])\n",
        "players_emb = embedding_players(players_tens)\n",
        "\n",
        "state_size_a = 12\n",
        "state_size_b = 13\n",
        "action_size = 16\n",
        "block_size = 2\n",
        "challenge_size = 2\n",
        "card_size = 2\n",
        "\n",
        "criterion = nn.HuberLoss(delta=1.0)\n",
        "learning_rate = 0.001\n",
        "gamma = 0.99\n",
        "epsilon = 1.0\n",
        "epsilon_decay = 0.005\n",
        "min_epsilon = 0.01\n",
        "# batch_size = 64\n",
        "# replay_buffer_size = 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 455,
      "id": "EvwXxvtVf4_3",
      "metadata": {
        "id": "EvwXxvtVf4_3"
      },
      "outputs": [],
      "source": [
        "class StateSummarizer(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, embedding_size):\n",
        "        super(StateSummarizer, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, embedding_size)\n",
        "\n",
        "    def forward(self, next_states):\n",
        "        # Initialize hidden state and cell state\n",
        "        h0 = torch.zeros(1, next_states.shape[0], self.hidden_size).to(next_states.device)\n",
        "        c0 = torch.zeros(1, next_states.shape[0], self.hidden_size).to(next_states.device)\n",
        "\n",
        "        # Pass the sequence of next states through the LSTM\n",
        "        out, _ = self.lstm(next_states, (h0, c0))\n",
        "\n",
        "        # print(f'out.shape: {out.shape}')\n",
        "\n",
        "        # Take the last hidden state as the summary\n",
        "        summary = torch.mean(out, dim=1)  # Average across the sequence dimension (dim=1)\n",
        "\n",
        "        # Project the summary to the desired embedding size\n",
        "        embedding = self.fc(summary)\n",
        "        return embedding\n",
        "\n",
        "# summarizer = StateSummarizer(12, 64, 12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 456,
      "id": "952658b8",
      "metadata": {
        "id": "952658b8"
      },
      "outputs": [],
      "source": [
        "class Bot:\n",
        "    def __init__(self, cards, num_coins, hostility, name, action_q, block_q, challenge_q, card_q,\n",
        "                 optimizer_action, optimizer_block, optimizer_challenge, optimizer_card,\n",
        "                 summarizer):\n",
        "        self.cards = cards\n",
        "        self.num_coins = num_coins\n",
        "        self.hostility = hostility\n",
        "        self.name = name\n",
        "        self.action_q = action_q\n",
        "        self.block_q = block_q\n",
        "        self.challenge_q = challenge_q\n",
        "        self.card_q = card_q\n",
        "        self.optimizer_action = optimizer_action\n",
        "        self.optimizer_block = optimizer_block\n",
        "        self.optimizer_challenge = optimizer_challenge\n",
        "        self.optimizer_card = optimizer_card\n",
        "        self.summarizer = summarizer\n",
        "\n",
        "    def num_coins_adj(self, n):\n",
        "        self.num_coins += n\n",
        "\n",
        "    def cards_adj(self, card):\n",
        "        self.cards.remove(card)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 457,
      "id": "7826ddbf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7826ddbf",
        "outputId": "7b4bc5ff-8c12-4788-d548-6e38dd302ef9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Contessa', 'Duke']\n",
            "['Ambassador', 'Duke']\n",
            "['Ambassador', 'Captain']\n",
            "['Contessa', 'Assassin']\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "bag = ['Duke', 'Captain', 'Assassin', 'Contessa', 'Ambassador'] * 3\n",
        "random.shuffle(bag)\n",
        "\n",
        "bots = []\n",
        "bluff_degree = 0\n",
        "\n",
        "for i in range(4):\n",
        "    cards = random.sample(bag, 2)\n",
        "    for card in cards:\n",
        "        bag.remove(card)\n",
        "#     kb = []\n",
        "\n",
        "    action_q = QNetwork(state_size_a, action_size)\n",
        "    optimizer_action = optim.Adam(action_q.parameters(), lr=learning_rate)\n",
        "\n",
        "    block_q = QNetwork(state_size_b, block_size)\n",
        "    optimizer_block = optim.Adam(block_q.parameters(), lr=learning_rate)\n",
        "\n",
        "    challenge_q = QNetwork(state_size_b, challenge_size)\n",
        "    optimizer_challenge = optim.Adam(challenge_q.parameters(), lr=learning_rate)\n",
        "\n",
        "    card_q = QNetwork(state_size_b, card_size)\n",
        "    optimizer_card = optim.Adam(card_q.parameters(), lr=learning_rate)\n",
        "\n",
        "    summarizer = StateSummarizer(12, 64, 12)\n",
        "\n",
        "    bots.append(Bot(cards, 2, None, i, action_q, block_q, challenge_q, card_q,\n",
        "                    optimizer_action, optimizer_block, optimizer_challenge, optimizer_card, summarizer))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for bot in bots:\n",
        "    print(bot.cards)\n",
        "print(bots[0].name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 458,
      "id": "av5X5iZs2UJQ",
      "metadata": {
        "id": "av5X5iZs2UJQ"
      },
      "outputs": [],
      "source": [
        "replay_buffer = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 459,
      "id": "TadCAcsPTrIz",
      "metadata": {
        "id": "TadCAcsPTrIz"
      },
      "outputs": [],
      "source": [
        "# actions = {\n",
        "#     0: take_1,\n",
        "#     1: coup,\n",
        "#     2: take_2,\n",
        "#     3: take_3,\n",
        "#     4: steal_2,\n",
        "#     5: assassinate,\n",
        "#     6: exchange,\n",
        "#     7: block_take_2,\n",
        "#     8: block_steal,\n",
        "#     9: block_assassination\n",
        "# }\n",
        "\n",
        "def get_legal_actions(bot, bots):\n",
        "  \"\"\"Returns a list of legal action indices for the given bot.\"\"\"\n",
        "  legal_actions = []\n",
        "\n",
        "  if bot.num_coins >= 10:\n",
        "    return [1]\n",
        "\n",
        "  # Always legal actions\n",
        "  legal_actions.extend([0, 2, 3, 6])  # Income, Foreign Aid, Tax, Exchange are always legal\n",
        "\n",
        "  # Conditional actions\n",
        "  if bot.num_coins >= 7:\n",
        "    legal_actions.append(1)  # Coup\n",
        "  if bot.num_coins >= 3:\n",
        "    legal_actions.append(5)  # Assassinate (if enough coins)\n",
        "\n",
        "  # Actions that target other players\n",
        "  for other_bot in bots:\n",
        "    if other_bot != bot and other_bot.num_coins > 1 : #Can't steal from players with no coins\n",
        "      legal_actions.append(4) #Steal\n",
        "      break  # Only need to add steal once if there's a valid target\n",
        "\n",
        "  return legal_actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 460,
      "id": "0cc07371",
      "metadata": {
        "id": "0cc07371"
      },
      "outputs": [],
      "source": [
        "def action_selection(i, bots, actions_vector, actions, epsilon, state):\n",
        "\n",
        "    legal_actions = get_legal_actions(bots[i], bots)  # Get list of legal actions\n",
        "\n",
        "    if int(bots[i].name) == 0:\n",
        "      if random.random() >= epsilon:\n",
        "          q_values = bots[i].action_q(state)\n",
        "          max_q_value = float('-inf')\n",
        "          best_action = None\n",
        "\n",
        "          # Iterate through Q-values and find the maximum for legal actions\n",
        "          for action_idx, q_value in enumerate(q_values[0]):  # q_values[0] is assumed to be a 1D tensor\n",
        "              if action_idx in legal_actions and q_value.item() > max_q_value:\n",
        "                  max_q_value = q_value.item()\n",
        "                  best_action = action_idx\n",
        "\n",
        "          # If no best action was selected from Q-values, choose randomly from legal actions\n",
        "          if best_action is None:\n",
        "              best_action = random.choice(legal_actions)\n",
        "\n",
        "          # Now determine the target bot: choose the bot (other than self) with the highest number of cards,\n",
        "          # and if there's a tie, choose the one with the most coins.\n",
        "          target = None\n",
        "          max_cards = -1\n",
        "          max_coins = -1\n",
        "          for bot_idx, other_bot in enumerate(bots):\n",
        "              if int(other_bot.name) != 0:  # Skip self (bot 0)\n",
        "                  num_coins = other_bot.num_coins  # Assuming 'cards' is a list of the bot's cards\n",
        "                  if num_coins > max_coins:\n",
        "                      max_coins = num_coins  # Assuming num_coins is a numeric attribute\n",
        "                      target = bots[bot_idx]\n",
        "\n",
        "          # Return the chosen action and the target bot index\n",
        "          return [best_action, target]\n",
        "      else:\n",
        "        # Random action selection:\n",
        "        action = None\n",
        "        if bots[i].num_coins >= 10:\n",
        "            action = 1  # Coup\n",
        "        else:\n",
        "            # Choose a random action from the legal actions\n",
        "            action = random.choice(legal_actions)\n",
        "\n",
        "        target = None\n",
        "        if (actions[action].p2_net_coins != 0 or actions[action].p2_net_cards != 0) and actions[action].response_action != 'challenge':\n",
        "            targets = bots[:i] + bots[i+1:]\n",
        "            valid_targets = [bot for bot in targets if bot.num_coins >= -actions[action].p2_net_coins]\n",
        "            if valid_targets:\n",
        "                target = random.choice(valid_targets)\n",
        "\n",
        "        return [action, target]\n",
        "\n",
        "# actions = {\n",
        "#     0: take_1,\n",
        "#     1: coup,\n",
        "#     2: take_2,\n",
        "#     3: take_3,\n",
        "#     4: steal_2,\n",
        "#     5: assassinate,\n",
        "#     6: exchange,\n",
        "#     7: block_take_2,\n",
        "#     8: block_steal,\n",
        "#     9: block_assassination\n",
        "# }\n",
        "\n",
        "    else:\n",
        "\n",
        "      target = None\n",
        "      action = None\n",
        "      targets = bots[:i] + bots[i+1:]\n",
        "\n",
        "      # Play truthfully:\n",
        "      # bot = bots[i]\n",
        "      if bots[i].num_coins >= 10:  # Coup if possible\n",
        "          action = 1  # Coup action index\n",
        "          target = random.choice(targets)  # Choose a random target\n",
        "      else:\n",
        "          # Prioritize actions based on cards and coins:\n",
        "          if 'Duke' in bots[i].cards:  # Take 3 coins if Duke\n",
        "              action = 3  # Take 3 coins action index\n",
        "          elif 'Captain' in bots[i].cards and 4 in legal_actions:\n",
        "              action = 4  # Steal action index\n",
        "              valid_targets = [bot for bot in targets if bot.num_coins >= 2]\n",
        "              if valid_targets:\n",
        "                  target = random.choice(valid_targets)\n",
        "              # target = bots.index(random.choice([other_bot for other_bot in bots if other_bot != bots[i] and other_bot.num_coins > 0]))\n",
        "          elif 'Assassin' in bots[i].cards and 5 in legal_actions:\n",
        "              action = 5  # Assassinate action index\n",
        "              target = random.choice(targets)  # Choose a random target\n",
        "          elif 'Ambassador' in bots[i].cards:  # Exchange if Ambassador\n",
        "              action = 6  # Exchange action index\n",
        "          else:  # Otherwise, take income\n",
        "              action = 0  # Take 1 coin action index\n",
        "          if bots[i].num_coins >= 7:\n",
        "            action = 1 # Coup if previous action was not legal and can coup\n",
        "            target = random.choice(targets)\n",
        "          elif 'Duke' not in bots[i].cards:\n",
        "            action = 2 # foreign aid if previous action was not legal and can foreign aid\n",
        "          # else:\n",
        "          #   action = 0 # income if previous action was not legal and can only income\n",
        "\n",
        "      # If action requires a target and none is selected yet:\n",
        "      # if (actions[action].p2_net_coins != 0 or actions[action].p2_net_cards != 0) and actions[action].response_action != 'challenge' and target is None:\n",
        "      #     target = bots.index(random.choice(bots[:i] + bots[i+1:]))\n",
        "\n",
        "      return [action, target]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def reaction_selection(i, bots, target, response_action, epsilon, state):\n",
        "\n",
        "  if target is None:\n",
        "\n",
        "    target = random.choice(bots[:i] + bots[i+1:])\n",
        "\n",
        "  if random.random() >= epsilon and int(target.name) == 0:\n",
        "\n",
        "    # state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "    q_values = target.block_q(state)\n",
        "    return torch.argmax(q_values).item(), target\n",
        "\n",
        "  elif int(target.name) != 0:\n",
        "\n",
        "    # Play truthfully - block only if has the card\n",
        "    if response_action.name == actions_map[8]:  # Block foreign aid\n",
        "        if 'Duke' in target.cards:\n",
        "            return 1, target  # Block\n",
        "        else:\n",
        "            return 0, target  # Pass\n",
        "    elif response_action.name == actions_map[9]:  # Block stealing\n",
        "        if 'Captain' in target.cards or 'Ambassador' in target.cards:\n",
        "            return 1, target  # Block\n",
        "        else:\n",
        "            return 0, target  # Pass\n",
        "    elif response_action.name == actions_map[10]:  # Block assassination\n",
        "        if 'Contessa' in target.cards:\n",
        "            return 1, target  # Block\n",
        "        else:\n",
        "            return 0, target  # Pass\n",
        "    else:  # Other actions (cannot be blocked truthfully)\n",
        "        return 0, target  # Pass\n",
        "\n",
        "  else:\n",
        "\n",
        "    return random.choice([0, 1]), target\n",
        "\n",
        "\n",
        "def challenge_selection(epsilon, state, bot):\n",
        "\n",
        "  if random.random() >= epsilon and int(bot.name) == 0:\n",
        "\n",
        "    return 0\n",
        "\n",
        "    # state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "    # q_values = bot.challenge_q(state_tensor)\n",
        "    # return torch.argmax(q_values).item()\n",
        "\n",
        "  else:\n",
        "\n",
        "    return 0\n",
        "\n",
        "\n",
        "def card_selection(bot, cards, epsilon, state, action):\n",
        "\n",
        "  if random.random() >= epsilon and len(cards) > 1 and int(bot.name) == 0:\n",
        "\n",
        "    # state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "    q_values = bot.card_q(state)\n",
        "    card_index = torch.argmax(q_values).item()  # Get index (0 or 1)\n",
        "    return card_index  # Return the index directly\n",
        "\n",
        "  else:\n",
        "\n",
        "    c = random.choice(cards)\n",
        "\n",
        "    c = cards.index(c)\n",
        "\n",
        "    return c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 461,
      "id": "af20170f",
      "metadata": {
        "id": "af20170f"
      },
      "outputs": [],
      "source": [
        "def perform_action(bot, target, action, discard_pile, state, card_chosen, epsilon, bag):\n",
        "\n",
        "    if target is not None:\n",
        "\n",
        "        target.num_coins += action.p2_net_coins\n",
        "\n",
        "        if action.p2_net_cards < 0 and len(target.cards) > 0:\n",
        "\n",
        "            card = card_selection(target, target.cards, epsilon, state, action)\n",
        "            # print(card)\n",
        "\n",
        "            x = target.cards[card]\n",
        "\n",
        "            discard_pile.append(inf_map[x])\n",
        "\n",
        "            card_chosen = card\n",
        "\n",
        "            target.cards.remove(x)\n",
        "\n",
        "    bot.num_coins += action.p1_net_coins\n",
        "\n",
        "    if action == exchange:\n",
        "\n",
        "        card = card_selection(bot, bot.cards, epsilon, state, action)\n",
        "\n",
        "        x = bot.cards[card]\n",
        "\n",
        "        c = random.sample(bag, 2)\n",
        "\n",
        "        # arr = [x] + c\n",
        "\n",
        "        next_choice = card_selection(bot, c, epsilon, state, action)\n",
        "\n",
        "        next_choice = c[next_choice]\n",
        "\n",
        "        arr = [x] + [next_choice]\n",
        "\n",
        "        final_choice = card_selection(bot, arr, epsilon, state, action)\n",
        "\n",
        "        card_chosen = final_choice\n",
        "\n",
        "        final_choice = arr[final_choice]\n",
        "\n",
        "        arr.remove(final_choice)\n",
        "\n",
        "        for i in arr:\n",
        "            bag.insert(-1, i)\n",
        "\n",
        "        random.shuffle(bag)\n",
        "        bot.cards.insert(-1, final_choice)\n",
        "        bot.cards.remove(x)\n",
        "\n",
        "    return bot, target, discard_pile, card_chosen, bag\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 462,
      "id": "LCgzznC-DY3Y",
      "metadata": {
        "id": "LCgzznC-DY3Y"
      },
      "outputs": [],
      "source": [
        "def reset_game(bots_copy):\n",
        "  bag = ['Duke', 'Captain', 'Assassin', 'Contessa', 'Ambassador'] * 3\n",
        "  random.shuffle(bag)\n",
        "  new_bots = []  # Create a new list\n",
        "  for i, bot in enumerate(bots_copy):\n",
        "      cards = random.sample(bag, 2)\n",
        "      for card in cards:\n",
        "          bag.remove(card)\n",
        "      new_bots.append(Bot(cards, 2, None, f'{i}', bot.action_q, bot.block_q, bot.challenge_q, bot.card_q,\n",
        "                          bot.optimizer_action, bot.optimizer_block, bot.optimizer_challenge, bot.optimizer_card))\n",
        "  return new_bots  # Return the new list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 463,
      "id": "3c6e59e2",
      "metadata": {
        "id": "3c6e59e2"
      },
      "outputs": [],
      "source": [
        "# Base Game Loop\n",
        "\n",
        "def game_loop_random(bots, actions, influences_reverse, epsilon):\n",
        "    bag = ['Duke', 'Captain', 'Assassin', 'Contessa', 'Ambassador'] * 3\n",
        "    random.shuffle(bag)\n",
        "\n",
        "    bots_copy = copy.deepcopy(bots)\n",
        "\n",
        "    states = torch.empty((0, 12), dtype=torch.float32)\n",
        "    states_block = torch.empty((0, 13), dtype=torch.float32)\n",
        "    states_challenge = torch.empty((0, 13), dtype=torch.float32)\n",
        "    states_card = torch.empty((0, 13), dtype=torch.float32)\n",
        "\n",
        "    discard_piles = []\n",
        "    discard_piles.append([])\n",
        "    acting_players = []\n",
        "    reacting_players = []\n",
        "    current_players = [[1,1,1,1]]\n",
        "    actions_game = [7]\n",
        "    reactions_game = [0]\n",
        "    challenges_game = [0]\n",
        "    challenges_direction = []\n",
        "    cards_game = []\n",
        "    coins_game = []\n",
        "\n",
        "    # state_tuples = []\n",
        "\n",
        "    rewards = [0]\n",
        "\n",
        "    action_history = [7]\n",
        "    reaction_history = [0]\n",
        "    challenge_history = [0]\n",
        "    card_history = [0]\n",
        "\n",
        "    cards_turn = [[inf_map[c] for c in bots[0].cards],\n",
        "                  [6,6],\n",
        "                  [6,6],\n",
        "                  [6,6]]\n",
        "    # for i in range(3):\n",
        "\n",
        "    #     cards_ind = [inf_map[c] for c in bot.cards]\n",
        "    #     cards_turn.append(cards_ind)\n",
        "\n",
        "    cards_game.append(cards_turn)\n",
        "\n",
        "    coins_turn = [2,2,2,2]\n",
        "    coins_game.append(coins_turn)\n",
        "    done = []\n",
        "    cards_chosen = [0]\n",
        "\n",
        "    t = 0\n",
        "\n",
        "\n",
        "#     print(cards_game[-1])\n",
        "#     print(coins_game[-1])\n",
        "\n",
        "    while len(bots) > 1:\n",
        "\n",
        "        if t > 100 and bots[0].name == 0:\n",
        "          done.append(1)\n",
        "          rewards.append(1.0)\n",
        "          break\n",
        "\n",
        "\n",
        "#         for bot in bots:\n",
        "#             print(f'{bot.name}')\n",
        "        i = 0\n",
        "        while i < len(bots):\n",
        "\n",
        "            # print(i)\n",
        "\n",
        "            # for bot in bots:\n",
        "            #   print(bot.cards)\n",
        "\n",
        "            card_chosen = 0\n",
        "#             print(i)\n",
        "\n",
        "            if len(bots) == 1:\n",
        "                # done.append(1)\n",
        "                break\n",
        "\n",
        "\n",
        "            done.append(0)\n",
        "            rewards.append(0)\n",
        "\n",
        "            challenge_dir = 2\n",
        "\n",
        "            discard_pile = copy.deepcopy(discard_piles[-1])\n",
        "\n",
        "            curr = None\n",
        "            try:\n",
        "                curr = bots[i]\n",
        "            except:\n",
        "                i = 0\n",
        "                curr = bots[i]\n",
        "\n",
        "            acting_players.append(int(curr.name))\n",
        "\n",
        "            # cards_state = cards_game[-1]\n",
        "            # coins_state = coins_game[-1]\n",
        "            # current_players_state = current_players[-1]\n",
        "\n",
        "            # cards_game: N x 4 x 2 -> 8 tensors of size N\n",
        "            cards_game_tensors = [\n",
        "                torch.tensor([cards_game[i][j][k] for i in range(len(cards_game))] )\n",
        "                for j in range(4) for k in range(2)\n",
        "            ]\n",
        "\n",
        "            # current_players: N x 4 -> 1 tensor of size N\n",
        "            current_players_tensors = torch.tensor([sum(row) for row in current_players])\n",
        "\n",
        "            coins_game_tensors = [\n",
        "                      torch.tensor([coins_game[i][j] for i in range(len(coins_game))] )\n",
        "                      for j in range(4)\n",
        "            ]\n",
        "\n",
        "            # discard_piles: N x y (max y = 7) -> 7 tensors of size N\n",
        "            max_discard_len = 7  # Maximum possible length of discard_piles\n",
        "            discard_piles_tensors = [\n",
        "                torch.tensor([discard_piles[i][j] if j < len(discard_piles[i]) else 0\n",
        "                              for i in range(len(discard_piles))] )\n",
        "                for j in range(max_discard_len)\n",
        "            ]\n",
        "\n",
        "            # Concatenate tensors for states_action\n",
        "            # states_action = torch.cat(([\n",
        "            #     torch.tensor(acting_players[1:]).unsqueeze(1), # unsqueeze to add a dimension\n",
        "            #     torch.tensor(reacting_players).unsqueeze(1),\n",
        "            #     torch.tensor(reactions_game).unsqueeze(1),\n",
        "            #     torch.tensor(challenges_game).unsqueeze(1),\n",
        "            #     torch.tensor(current_players_tensors).unsqueeze(1),\n",
        "            #     *[t.unsqueeze(1) for t in cards_game_tensors],\n",
        "            #     *[t.unsqueeze(1) for t in discard_piles_tensors],\n",
        "            #     *[t.unsqueeze(1) for t in coins_game_tensors],\n",
        "            #     torch.tensor(done).unsqueeze(1)\n",
        "            # ]), 1)  # changed dim to 1\n",
        "\n",
        "            state = None\n",
        "\n",
        "            if int(bots[0].name) == 0:\n",
        "\n",
        "              # 1. Cards in play (embedded):\n",
        "              cards_in_play_embedded = []\n",
        "              for card_name in influences.keys():\n",
        "                  num_in_discard = discard_piles[-1].count(inf_map[card_name])\n",
        "                  num_in_play = 3 - num_in_discard  # Assuming 3 of each card initially\n",
        "                  cards_in_play_embedded.append(torch.tensor(num_in_play))  # Keep as tensor\n",
        "\n",
        "              # Stack the embeddings to create a 2D tensor\n",
        "              cards_in_play_embedded = torch.stack(cards_in_play_embedded).squeeze()\n",
        "\n",
        "              # 4. Bot 0's normalized coins:\n",
        "              bot0_coins_normalized = bots[0].num_coins / 12 # Normalize to 0-1 range (assuming max coins is 12)\n",
        "\n",
        "              # 5. Average cards of other players (normalized and embedded):\n",
        "              other_bots_cards = [len(bot.cards) for bot in bots if bot != bots[0]]\n",
        "              avg_other_cards = sum(other_bots_cards) / len(other_bots_cards) if other_bots_cards else 0  # Avoid division by zero\n",
        "              avg_other_cards_normalized = avg_other_cards / 2  # Normalize to 0-1 range (assuming max cards per bot is 2)\n",
        "              # avg_other_cards_embedded = embedding_cards(torch.tensor(int(avg_other_cards_normalized))).tolist()  # Assuming embedding_cards is your embedding layer\n",
        "\n",
        "              # 6. Bot 0's current cards (embedded):\n",
        "              bot0_cards_embedded = []\n",
        "              for card in bots[0].cards:\n",
        "                  bot0_cards_embedded.append(embedding_cards(torch.tensor(inf_map[card])))  # Keep as tensor\n",
        "\n",
        "              # If the bot has cards, concatenate the embeddings. Otherwise, create a zero tensor\n",
        "              if bot0_cards_embedded:\n",
        "                  bot0_cards_embedded = torch.cat(bot0_cards_embedded)\n",
        "              else:\n",
        "                  # Create a zero tensor with the expected shape if bot has no cards\n",
        "                  bot0_cards_embedded = torch.cat((embedding_cards(torch.tensor(0)), embedding_cards(torch.tensor(0))))\n",
        "\n",
        "              if len(bot0_cards_embedded) == 1:\n",
        "                  bot0_cards_embedded = torch.cat((bot0_cards_embedded, embedding_cards(torch.tensor(0))))\n",
        "\n",
        "              # 7. The last action taken (embedded):\n",
        "              last_action = actions_game[-1]\n",
        "              last_action_embedded = embedding_actions(torch.tensor(last_action))  # Keep as a tensor\n",
        "\n",
        "              state = torch.cat(([cards_in_play_embedded,\n",
        "                                  torch.tensor([bot0_coins_normalized]),\n",
        "                                  torch.tensor([avg_other_cards_normalized]),\n",
        "                                  bot0_cards_embedded,\n",
        "                                  last_action_embedded])\n",
        "                                ).type(torch.float32)\n",
        "\n",
        "              state_block = torch.cat([state, torch.tensor([reactions_game[-1]])], 0)\n",
        "              state_challenge = torch.cat([state, torch.tensor([challenges_game[-1]])], 0)\n",
        "              state_card = torch.cat([state, torch.tensor([cards_chosen[-1]])], 0)\n",
        "\n",
        "              # states_temp = torch.cat((states_temp, state.unsqueeze(0)), 0)\n",
        "              # states_summarized = bot.summarizer(states_temp)\n",
        "              states = torch.cat([states, state.unsqueeze(0)], 0)\n",
        "              states_block = torch.cat([states_block, state_block.unsqueeze(0)], 0)\n",
        "              states_challenge = torch.cat([states_challenge, state_challenge.unsqueeze(0)], 0)\n",
        "              states_card = torch.cat([states_card, state_card.unsqueeze(0)], 0)\n",
        "\n",
        "\n",
        "            action_stack = []\n",
        "\n",
        "            action_vector = [0,1,2,3,4,5,6]\n",
        "            for j in action_vector:\n",
        "                if actions[j].p1_net_coins * (-1) > bots[i].num_coins:\n",
        "                    action_vector.remove(j)\n",
        "\n",
        "            # state = None\n",
        "            # state_tensor = None\n",
        "            # if bots[0].name == 0:\n",
        "            #   state = get_state(bots, discard_pile, action_history, reaction_history, challenge_history, card_history, bots[i], network_type=\"action\")\n",
        "            #   state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            #   print(len(state_tensor[0]))\n",
        "\n",
        "            action_selection_output = action_selection(i, bots, action_vector, actions, epsilon, states.unsqueeze(0))\n",
        "\n",
        "            action = action_selection_output[0]\n",
        "\n",
        "            for x in [states, states_block, states_challenge, states_card]:\n",
        "              x[-1][9:12] = torch.tensor(embedding_actions(torch.tensor(action)))\n",
        "\n",
        "            try:\n",
        "              last_state = states[-2]\n",
        "            except:\n",
        "              last_state = None\n",
        "\n",
        "            # state_tuples.append((states[-1], actions[action], rewards[-1], last_state, done[-1]))\n",
        "\n",
        "            if actions_game[-1] == 7:\n",
        "              actions_game[-1] = action\n",
        "\n",
        "            else:\n",
        "              actions_game.append(action)\n",
        "\n",
        "            action_e = actions_emb[action]\n",
        "\n",
        "            action = actions[action]\n",
        "    #         print(action_selection_output[1])\n",
        "\n",
        "            # print(f'bot {bots[i].name} is performing action {action.name}')\n",
        "            # print(f'target is {action_selection_output[1]}')\n",
        "\n",
        "            target = None\n",
        "            reacting_player = 4\n",
        "            challenge = 0\n",
        "            reaction = 0\n",
        "            try:\n",
        "              target = action_selection_output[1]\n",
        "              reacting_player = int(target.name)\n",
        "            except:\n",
        "              target = None\n",
        "              # reacting_player = 4\n",
        "                # print(\"no target\")\n",
        "            # if target is not None:\n",
        "            #     print(f'target is {target.name}')\n",
        "\n",
        "            action_stack.append(action)\n",
        "\n",
        "            if action.response_action is not None and target is None:\n",
        "              target = random.choice(bots)\n",
        "              reacting_player = int(target.name)\n",
        "            reacting_players.append(reacting_player)\n",
        "\n",
        "            # state_reaction = copy.deepcopy(state_action)\n",
        "            # state_reaction.append(action)\n",
        "\n",
        "            # state_challenge = copy.deepcopy(state_action)\n",
        "            # state_challenge.append(action)\n",
        "\n",
        "            # state_card = copy.deepcopy(state_action)\n",
        "            # state_card.append(action)\n",
        "\n",
        "            if action.response_action is not None and action.response_action != 'challenge':  # is blockable?\n",
        "\n",
        "                response, target = reaction_selection(i, bots, target, action.response_action, epsilon, states_block.unsqueeze(0))\n",
        "\n",
        "                # reacting_player = int(target.name)\n",
        "\n",
        "#                 try:\n",
        "#                     print(f'bot {target.name} is considering blocking')\n",
        "#                 except:\n",
        "#                     print(\"no target, check reaction selection\")\n",
        "\n",
        "                if response == 1:\n",
        "\n",
        "                    reaction = 1\n",
        "\n",
        "                    reactions_game.append(1)\n",
        "\n",
        "#                     reacting_players.append(int(target.name))\n",
        "\n",
        "                    action_stack.append(action.response_action)\n",
        "                    # state_challenge.append(reaction)\n",
        "                    # state_card.append(reaction)\n",
        "\n",
        "                    # print(f'bot {target.name} is performing action {action.response_action.name} against bot {bots[i].name}')\n",
        "\n",
        "                else:\n",
        "\n",
        "                    reactions_game.append(0)\n",
        "\n",
        "            else:\n",
        "\n",
        "                reactions_game.append(0)\n",
        "\n",
        "#                     print(f'target will not block')\n",
        "\n",
        "            if action_stack[-1].response_action == 'challenge':  # is challengeable?\n",
        "\n",
        "                response = challenge_selection(epsilon, states_challenge.unsqueeze(0), target if len(action_stack) == 3 else bots[i])\n",
        "\n",
        "                if response == 1:\n",
        "\n",
        "                    challenge = 1\n",
        "\n",
        "                    challenges_game.append(1)\n",
        "\n",
        "                    action_stack.append('challenge')\n",
        "\n",
        "                    if len(action_stack) == 3:\n",
        "                        challenge_dir = 1\n",
        "\n",
        "                    else:\n",
        "                        challenge_dir = 0\n",
        "                    challenges_direction.append(challenge_dir)\n",
        "\n",
        "                else:\n",
        "\n",
        "                    challenges_game.append(0)\n",
        "                    challenges_direction.append(challenge_dir)\n",
        "\n",
        "#                     print('no challenge')\n",
        "\n",
        "            else:\n",
        "\n",
        "                challenges_game.append(0)\n",
        "                challenges_direction.append(2)\n",
        "\n",
        "            # challenges_game.append(0)\n",
        "            # challenges_direction.append(2)\n",
        "\n",
        "            while len(action_stack) != 0:\n",
        "\n",
        "                # state_card.append(challenge)\n",
        "                # state_card.append(challenge_dir)\n",
        "\n",
        "                a = action_stack.pop()\n",
        "                # if a != 'challenge':\n",
        "                #   print(a.name)\n",
        "\n",
        "                if int(bots[0].name) != 0:\n",
        "                  rewards[-1] = -1.0\n",
        "                  done[-1] = 1\n",
        "\n",
        "                if a == 'challenge':\n",
        "\n",
        "                    print('error!')\n",
        "\n",
        "                    if len(action_stack) > 1:\n",
        "\n",
        "                        if influences_reverse[action_stack[-1]] in target.cards:\n",
        "\n",
        "                            # print(f'bot {bots[i].name} has lost the challenge')\n",
        "\n",
        "                            card = 0\n",
        "                            if len(bots[i].cards) > 1:\n",
        "\n",
        "                              card = card_selection(bots[i], bots[i].cards, epsilon, states_card.unsqueeze(0), a)\n",
        "\n",
        "                            # print(card)\n",
        "                            x = bots[i].cards[card]\n",
        "\n",
        "                            discard_pile.append(inf_map[x])\n",
        "\n",
        "                            card_chosen = card\n",
        "                            # card = inf_map.get(card)\n",
        "\n",
        "                            bots[i].cards.remove(x)\n",
        "\n",
        "                            if len(bots[i].cards) == 0:\n",
        "\n",
        "                                print (f'bot {bots[i].name} is out!')\n",
        "\n",
        "                                bots.remove(bots[i])\n",
        "\n",
        "                                i -= 1\n",
        "\n",
        "                            target.cards.remove(influences_reverse[action_stack[-1]])\n",
        "                            bag.insert(influences_reverse[action_stack[-1]])\n",
        "                            random.shuffle(bag)\n",
        "                            c = random.sample(bag, 1)\n",
        "                            bag.remove(c)\n",
        "                            target.cards.insert(c)\n",
        "\n",
        "                            action_stack.clear()\n",
        "\n",
        "                        else:\n",
        "\n",
        "                            # print(f'bot {target.name} has lost the challenge')\n",
        "\n",
        "                            card = 0\n",
        "                            if len(target.cards) > 1:\n",
        "                              card = card_selection(target, target.cards, epsilon, states_card.unsqueeze(0), a)\n",
        "\n",
        "                            # print(card)\n",
        "                            x = target.cards[card]\n",
        "\n",
        "                            discard_pile.append(inf_map[x])\n",
        "\n",
        "                            card_chosen = card\n",
        "\n",
        "                            # card = inf_map.get(card)\n",
        "\n",
        "                            target.cards.remove(x)\n",
        "\n",
        "                            if len(target.cards) == 0:\n",
        "\n",
        "                                bots.remove(target)\n",
        "\n",
        "                                # print (f'bot {target.name} is out!')\n",
        "\n",
        "                            action_stack.pop()\n",
        "\n",
        "                    else:\n",
        "\n",
        "                        if influences_reverse[action_stack[-1]] in bots[i].cards:\n",
        "\n",
        "                            # print(f'bot {target.name} has lost the challenge')\n",
        "\n",
        "                            card = 0\n",
        "                            if len(target.cards) > 1:\n",
        "                              card = card_selection(target, target.cards, epsilon, states_card.unsqueeze(0), a)\n",
        "\n",
        "                            # print(card)\n",
        "                            x = target.cards[card]\n",
        "\n",
        "                            discard_pile.append(inf_map[x])\n",
        "\n",
        "                            card_chosen = card\n",
        "                            # card = inf_map.get(card)\n",
        "\n",
        "                            target.cards.remove(x)\n",
        "\n",
        "                            if len(target.cards) == 0:\n",
        "\n",
        "                                # print (f'bot {target.name} is out!')\n",
        "\n",
        "                                bots.remove(target)\n",
        "\n",
        "                            bots[i].cards.remove(influences_reverse[action_stack[-1]])\n",
        "                            bag.insert(influences_reverse[action_stack[-1]])\n",
        "                            random.shuffle(bag)\n",
        "                            c = random.sample(bag, 1)\n",
        "                            bag.remove(c)\n",
        "                            bots[i].cards.insert(c)\n",
        "\n",
        "                        else:\n",
        "\n",
        "                            # print(f'bot {bots[i].name} has lost the challenge')\n",
        "\n",
        "                            card = 0\n",
        "                            if len(bots[i].cards) > 1:\n",
        "                              card = card_selection(bots[i], bots[i].cards, epsilon, states_card.unsqueeze(0), a)\n",
        "                            # print(card)\n",
        "                            x = bots[i].cards[card]\n",
        "\n",
        "                            discard_pile.append(inf_map[x])\n",
        "\n",
        "                            card_chosen = card\n",
        "                            # card = inf_map.get(card)\n",
        "\n",
        "                            bots[i].cards.remove(x)\n",
        "\n",
        "                            if len(bots[i].cards) == 0:\n",
        "\n",
        "                                # print (f'bot {bots[i].name} is out!')\n",
        "\n",
        "                                bots.remove(bots[i])\n",
        "\n",
        "                                i -= 1\n",
        "\n",
        "                            action_stack.pop()\n",
        "\n",
        "                else:\n",
        "\n",
        "                    # print(f'current action: {a.name}')\n",
        "\n",
        "                    if len(action_stack) == 1:\n",
        "\n",
        "                        target, curr, discard_pile, card_chosen, bag = perform_action(target, curr, a, discard_pile, states_card.unsqueeze(0), card_chosen, epsilon, bag)\n",
        "                        if curr in bots:\n",
        "                            if len(curr.cards) == 0:\n",
        "                                if curr in bots:\n",
        "                                    bots.remove(curr)\n",
        "                                    # print(f'{curr.name} is out!')\n",
        "                                    i -= 1\n",
        "\n",
        "                    else:\n",
        "\n",
        "                        curr, target, discard_pile, card_chosen, bag = perform_action(curr, target, a, discard_pile, states_card.unsqueeze(0), card_chosen, epsilon, bag)\n",
        "                        if target in bots:\n",
        "                            if len(target.cards) == 0:\n",
        "                                if target in bots:\n",
        "                                    bots.remove(target)\n",
        "                                    # print(f'{target.name} is out!')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # print(f'bot {curr.name} has {curr.num_coins} coins.')\n",
        "            # if target is not None:\n",
        "            #     print(f'bot {target.name} has {target.num_coins} coins.')\n",
        "\n",
        "            # print(f'bot {curr.name} has {len(curr.cards)} cards.')\n",
        "            # if target is not None:\n",
        "            #     print(f'bot {target.name} has {len(target.cards)} cards.')\n",
        "\n",
        "\n",
        "            curr_players = [0,0,0,0]\n",
        "            for bot in bots:\n",
        "#                 print(int(bot.name))\n",
        "                curr_players[int(bot.name)] = 1\n",
        "            # print(curr_players)\n",
        "            current_players.append(curr_players)\n",
        "\n",
        "            cards_turn = [[0,0],\n",
        "                          [0,0],\n",
        "                          [0,0],\n",
        "                          [0,0]]\n",
        "            coins_turn = [0,0,0,0]\n",
        "\n",
        "            # if len(bots[0].cards) == 0:\n",
        "            #     cards_turn[0] = [0,0]\n",
        "            # if len(bots[0].cards) == 1:\n",
        "            #     cards_turn[0].append(0)\n",
        "\n",
        "            for bot in bots:\n",
        "\n",
        "                bot_index = int(bot.name)\n",
        "\n",
        "                cards_ind = []\n",
        "\n",
        "                cards_turn[bot_index] = [inf_map[c] for c in bot.cards]\n",
        "\n",
        "                if len(bot.cards) == 0:\n",
        "                    cards_turn[bot_index] = [0, 0]\n",
        "                if len(bot.cards) == 1:\n",
        "                    cards_turn[bot_index].append(0)\n",
        "\n",
        "                coins_turn[bot_index] = bot.num_coins\n",
        "\n",
        "            cards_game.append(cards_turn)\n",
        "            coins_game.append(coins_turn)\n",
        "\n",
        "            discard_piles.append(discard_pile)\n",
        "\n",
        "            # reacting_players.append(reacting_player)\n",
        "            # reactions_game.append(reaction)\n",
        "            # challenges_game.append(challenge)\n",
        "            # challenges_direction.append(challenge_dir)\n",
        "            cards_chosen.append(card_chosen)\n",
        "\n",
        "            action_history.append(action)\n",
        "            reaction_history.append(reaction)\n",
        "            challenge_history.append(challenge)\n",
        "            card_history.append(card_chosen)\n",
        "\n",
        "            # print(actions_game[-1])\n",
        "\n",
        "            i += 1\n",
        "            t += 1\n",
        "#             print(cards_turn)\n",
        "#             print(coins_turn)\n",
        "# #             print(curr_players)\n",
        "#             print(discard_pile)\n",
        "\n",
        "\n",
        "    # print(f'bot {bots[0].name} wins!')\n",
        "    acting_players.append(4)\n",
        "    reacting_players.append(4)\n",
        "    actions_game.append(7)\n",
        "    # reactions_game.append(0)\n",
        "    # challenges_game.append(0)\n",
        "    challenges_direction.append(2)\n",
        "    done.append(1)\n",
        "    # rewards = copy.deepcopy(done)\n",
        "    if int(bots[0].name) != 0:\n",
        "      rewards[-1] = -1.0\n",
        "    else:\n",
        "      rewards[-1] = 1.0\n",
        "\n",
        "\n",
        "    # Reset Game\n",
        "\n",
        "    bag = ['Duke', 'Captain', 'Assassin', 'Contessa', 'Ambassador'] * 3\n",
        "    random.shuffle(bag)\n",
        "    # new_bots = []  # Create a new list\n",
        "\n",
        "    for bot in bots_copy:\n",
        "        bot.cards = random.sample(bag, 2)\n",
        "        for card in bot.cards:\n",
        "            bag.remove(card)\n",
        "        bot.num_coins = 2\n",
        "\n",
        "    bots = bots_copy\n",
        "\n",
        "\n",
        "    return discard_piles, acting_players, reacting_players, current_players, actions_game, reactions_game, challenges_game, cards_game, coins_game, challenges_direction, done, rewards, cards_chosen, bots_copy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 464,
      "id": "pV7TxFDjnEC9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV7TxFDjnEC9",
        "outputId": "bb8ee1d3-beb5-44f8-d1d6-83ac4b380074"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35\n",
            "35\n",
            "35\n",
            "35\n",
            "35\n",
            "35\n",
            "35\n",
            "35\n",
            "   acting_players  reacting_players  actions_game  reactions_game  \\\n",
            "0               0                 1             0               0   \n",
            "1               1                 1             3               0   \n",
            "2               2                 3             2               0   \n",
            "\n",
            "   challenges_game  challenges_direction                        cards_game  \\\n",
            "0                0                     2  [[4, 1], [6, 6], [6, 6], [6, 6]]   \n",
            "1                0                     2  [[4, 1], [5, 1], [5, 2], [4, 3]]   \n",
            "2                0                     2  [[4, 1], [5, 1], [5, 2], [4, 3]]   \n",
            "\n",
            "   card_chosen    coins_game  done  rewards  \n",
            "0            0  [2, 2, 2, 2]     0      0.0  \n",
            "1            0  [3, 2, 2, 2]     0      0.0  \n",
            "2            0  [3, 5, 2, 2]     0      0.0  \n",
            "\n",
            "    acting_players  reacting_players  actions_game  reactions_game  \\\n",
            "32               3                 1             2               0   \n",
            "33               1                 3             1               1   \n",
            "34               4                 4             7               0   \n",
            "\n",
            "    challenges_game  challenges_direction                        cards_game  \\\n",
            "32                0                     2  [[0, 0], [5, 1], [0, 0], [3, 0]]   \n",
            "33                0                     2  [[0, 0], [5, 1], [0, 0], [3, 0]]   \n",
            "34                0                     2  [[0, 0], [5, 1], [0, 0], [0, 0]]   \n",
            "\n",
            "    card_chosen    coins_game  done  rewards  \n",
            "32            0  [0, 8, 0, 3]     1     -1.0  \n",
            "33            0  [0, 8, 0, 3]     1     -1.0  \n",
            "34            0  [0, 1, 0, 0]     1     -1.0  \n",
            "\n",
            "    acting_players  reacting_players  actions_game  reactions_game  \\\n",
            "10               2                 2             2               0   \n",
            "11               3                 2             2               0   \n",
            "12               0                 2             0               0   \n",
            "13               1                 3             3               0   \n",
            "14               2                 0             1               0   \n",
            "\n",
            "    challenges_game  challenges_direction                        cards_game  \\\n",
            "10                0                     2  [[4, 1], [5, 1], [5, 0], [4, 3]]   \n",
            "11                0                     2  [[4, 1], [5, 1], [5, 0], [4, 3]]   \n",
            "12                0                     2  [[4, 1], [5, 1], [5, 0], [4, 3]]   \n",
            "13                0                     2  [[4, 1], [5, 1], [5, 0], [4, 3]]   \n",
            "14                0                     2  [[4, 1], [5, 1], [5, 0], [4, 3]]   \n",
            "\n",
            "    card_chosen    coins_game  done  rewards  \n",
            "10            1  [5, 1, 6, 6]     0      0.0  \n",
            "11            0  [5, 1, 8, 6]     0      0.0  \n",
            "12            0  [5, 1, 8, 8]     0      0.0  \n",
            "13            0  [6, 1, 8, 8]     0      0.0  \n",
            "14            0  [6, 4, 8, 8]     0      0.0  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-463-42d77421dad5>:218: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x[-1][9:12] = torch.tensor(embedding_actions(torch.tensor(action)))\n"
          ]
        }
      ],
      "source": [
        "### Test ###\n",
        "\n",
        "win_rate = 0.0\n",
        "\n",
        "# for i in range(50):\n",
        "\n",
        "discard_piles, acting_players, reacting_players, current_players, actions_game, reactions_game, challenges_game, cards_game, coins_game, challenges_direction, done, rewards, cards_chosen, bots_copy = game_loop_random(bots, actions, influences_reverse, 0.0)\n",
        "\n",
        "bots = bots_copy\n",
        "\n",
        "  # if rewards[-1] == 1.0:\n",
        "  #   win_rate += 1.0\n",
        "\n",
        "# print(win_rate/50)\n",
        "print(len(acting_players))\n",
        "print(len(reacting_players))\n",
        "print(len(actions_game))\n",
        "print(len(reactions_game))\n",
        "print(len(challenges_game))\n",
        "print(len(cards_game))\n",
        "print(len(coins_game))\n",
        "print(len(done))\n",
        "\n",
        "\n",
        "data = {\n",
        "    'acting_players': acting_players,\n",
        "    'reacting_players': reacting_players,\n",
        "    'actions_game': actions_game,\n",
        "    'reactions_game': reactions_game,\n",
        "    'challenges_game': challenges_game,\n",
        "    'challenges_direction': challenges_direction,\n",
        "    'cards_game': cards_game,\n",
        "    'card_chosen': cards_chosen,\n",
        "    'coins_game': coins_game,\n",
        "    'done': done,\n",
        "    'rewards': rewards\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data = data)\n",
        "\n",
        "print(df.head(3))\n",
        "print()\n",
        "print(df.tail(3))\n",
        "print()\n",
        "\n",
        "print(df[10:15])\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 465,
      "id": "be4160d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "be4160d0",
        "outputId": "43aaa280-9432-43a9-b7a5-be5286b12edd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode 0 of 1000\n",
            "epsilon: 1.0\n",
            "gamma: 0.99\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-463-42d77421dad5>:218: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x[-1][9:12] = torch.tensor(embedding_actions(torch.tensor(action)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of games in episode 0: 42\n",
            "torch.Size([1648, 12])\n",
            "torch.Size([1648, 13])\n",
            "torch.Size([1648, 13])\n",
            "torch.Size([1648, 13])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-465-4fff822dc34b>:329: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_actions_main  = torch.tensor(batch_actions_main, dtype=torch.long)\n",
            "<ipython-input-465-4fff822dc34b>:435: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  rewards_list.append(torch.tensor(r, dtype=torch.float32))\n",
            "<ipython-input-465-4fff822dc34b>:496: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_actions_block = torch.tensor(batch_actions_block, dtype=torch.long)\n",
            "<ipython-input-465-4fff822dc34b>:603: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  rewards_list.append(torch.tensor(r, dtype=torch.float32))\n",
            "<ipython-input-465-4fff822dc34b>:664: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_actions_challenge = torch.tensor(batch_actions_challenge, dtype=torch.long)\n",
            "<ipython-input-465-4fff822dc34b>:777: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  rewards_list.append(torch.tensor(r, dtype=torch.float32))\n",
            "<ipython-input-465-4fff822dc34b>:847: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_actions_card = torch.tensor(batch_actions_card, dtype=torch.long)\n",
            "<ipython-input-465-4fff822dc34b>:970: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  reward_list.append(torch.tensor(r, dtype=torch.float32))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg Action Loss, 7 batches: 0.015562765300273895\n",
            "Avg Block Loss, 7 batches: 0.013180936686694622\n",
            "Avg Challenge Loss, 8 batches: 0.02278696931898594\n",
            "Avg Card Loss, 2 batches: 0.046754177659749985\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "Most common action for Bot 0: 2 taken 634 times.\n",
            "total game lengths: 3791\n",
            "win rate: 0.24\n",
            "episode 1 of 1000\n",
            "epsilon: 0.995\n",
            "gamma: 0.99\n",
            "Number of games in episode 1: 42\n",
            "torch.Size([1606, 12])\n",
            "torch.Size([1606, 13])\n",
            "torch.Size([1606, 13])\n",
            "torch.Size([1606, 13])\n",
            "Avg Action Loss, 6 batches: 0.0151097122579813\n",
            "Avg Block Loss, 5 batches: 0.017018742859363556\n",
            "Avg Challenge Loss, 6 batches: 0.04598613828420639\n",
            "Avg Card Loss, 2 batches: 0.0480349138379097\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 2 of 1000\n",
            "epsilon: 0.990025\n",
            "gamma: 0.99\n",
            "Number of games in episode 2: 42\n",
            "torch.Size([1638, 12])\n",
            "torch.Size([1638, 13])\n",
            "torch.Size([1638, 13])\n",
            "torch.Size([1638, 13])\n",
            "Avg Action Loss, 6 batches: 0.007424647454172373\n",
            "Avg Block Loss, 6 batches: 0.01130324974656105\n",
            "Avg Challenge Loss, 7 batches: 0.04008883982896805\n",
            "Avg Card Loss, 2 batches: 0.058137986809015274\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 3 of 1000\n",
            "epsilon: 0.985074875\n",
            "gamma: 0.99\n",
            "Number of games in episode 3: 43\n",
            "torch.Size([1631, 12])\n",
            "torch.Size([1631, 13])\n",
            "torch.Size([1631, 13])\n",
            "torch.Size([1631, 13])\n",
            "Avg Action Loss, 6 batches: 0.028104180470108986\n",
            "Avg Block Loss, 6 batches: 0.021266140043735504\n",
            "Avg Challenge Loss, 7 batches: 0.026321642100811005\n",
            "Avg Card Loss, 2 batches: 0.03777129203081131\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 4 of 1000\n",
            "epsilon: 0.9801495006250001\n",
            "gamma: 0.99\n",
            "Number of games in episode 4: 43\n",
            "torch.Size([1623, 12])\n",
            "torch.Size([1623, 13])\n",
            "torch.Size([1623, 13])\n",
            "torch.Size([1623, 13])\n",
            "Avg Action Loss, 6 batches: 0.021270103752613068\n",
            "Avg Block Loss, 6 batches: 0.02957306057214737\n",
            "Avg Challenge Loss, 6 batches: 0.022293366491794586\n",
            "Avg Card Loss, 2 batches: 0.031634487211704254\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 5 of 1000\n",
            "epsilon: 0.9752487531218751\n",
            "gamma: 0.99\n",
            "Number of games in episode 5: 42\n",
            "torch.Size([1626, 12])\n",
            "torch.Size([1626, 13])\n",
            "torch.Size([1626, 13])\n",
            "torch.Size([1626, 13])\n",
            "Avg Action Loss, 6 batches: 0.008531955070793629\n",
            "Avg Block Loss, 6 batches: 0.020163660869002342\n",
            "Avg Challenge Loss, 7 batches: 0.02109876275062561\n",
            "Avg Card Loss, 2 batches: 0.04647407680749893\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 6 of 1000\n",
            "epsilon: 0.9703725093562657\n",
            "gamma: 0.99\n",
            "Number of games in episode 6: 42\n",
            "torch.Size([1604, 12])\n",
            "torch.Size([1604, 13])\n",
            "torch.Size([1604, 13])\n",
            "torch.Size([1604, 13])\n",
            "Avg Action Loss, 5 batches: 0.012618190608918667\n",
            "Avg Block Loss, 5 batches: 0.016501009464263916\n",
            "Avg Challenge Loss, 6 batches: 0.008611614815890789\n",
            "Avg Card Loss, 2 batches: 0.04453973099589348\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 7 of 1000\n",
            "epsilon: 0.9655206468094844\n",
            "gamma: 0.99\n",
            "Number of games in episode 7: 41\n",
            "torch.Size([1626, 12])\n",
            "torch.Size([1626, 13])\n",
            "torch.Size([1626, 13])\n",
            "torch.Size([1626, 13])\n",
            "Avg Action Loss, 6 batches: 0.006936111953109503\n",
            "Avg Block Loss, 6 batches: 0.019548730924725533\n",
            "Avg Challenge Loss, 7 batches: 0.005986202508211136\n",
            "Avg Card Loss, 2 batches: 0.026108261197805405\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 8 of 1000\n",
            "epsilon: 0.960693043575437\n",
            "gamma: 0.99\n",
            "Number of games in episode 8: 44\n",
            "torch.Size([1606, 12])\n",
            "torch.Size([1606, 13])\n",
            "torch.Size([1606, 13])\n",
            "torch.Size([1606, 13])\n",
            "Avg Action Loss, 6 batches: 0.012270447798073292\n",
            "Avg Block Loss, 6 batches: 0.010130820795893669\n",
            "Avg Challenge Loss, 7 batches: 0.02687215805053711\n",
            "Avg Card Loss, 2 batches: 0.027887217700481415\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 9 of 1000\n",
            "epsilon: 0.9558895783575597\n",
            "gamma: 0.99\n",
            "Number of games in episode 9: 39\n",
            "torch.Size([1605, 12])\n",
            "torch.Size([1605, 13])\n",
            "torch.Size([1605, 13])\n",
            "torch.Size([1605, 13])\n",
            "Avg Action Loss, 6 batches: 0.009626025334000587\n",
            "Avg Block Loss, 6 batches: 0.027548838406801224\n",
            "Avg Challenge Loss, 7 batches: 0.016243737190961838\n",
            "Avg Card Loss, 2 batches: 0.02912520244717598\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 10 of 1000\n",
            "epsilon: 0.9511101304657719\n",
            "gamma: 0.99\n",
            "Number of games in episode 10: 43\n",
            "torch.Size([1627, 12])\n",
            "torch.Size([1627, 13])\n",
            "torch.Size([1627, 13])\n",
            "torch.Size([1627, 13])\n",
            "Avg Action Loss, 6 batches: 0.015427968464791775\n",
            "Avg Block Loss, 6 batches: 0.010850952006876469\n",
            "Avg Challenge Loss, 7 batches: 0.008612409234046936\n",
            "Avg Card Loss, 2 batches: 0.04429996386170387\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "Most common action for Bot 0: 0 taken 1030 times.\n",
            "total game lengths: 4307\n",
            "win rate: 0.41\n",
            "episode 11 of 1000\n",
            "epsilon: 0.946354579813443\n",
            "gamma: 0.99\n",
            "Number of games in episode 11: 43\n",
            "torch.Size([1613, 12])\n",
            "torch.Size([1613, 13])\n",
            "torch.Size([1613, 13])\n",
            "torch.Size([1613, 13])\n",
            "Avg Action Loss, 6 batches: 0.02453993819653988\n",
            "Avg Block Loss, 6 batches: 0.013784624636173248\n",
            "Avg Challenge Loss, 7 batches: 0.026696497574448586\n",
            "Avg Card Loss, 2 batches: 0.039086438715457916\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 12 of 1000\n",
            "epsilon: 0.9416228069143757\n",
            "gamma: 0.99\n",
            "Number of games in episode 12: 41\n",
            "torch.Size([1617, 12])\n",
            "torch.Size([1617, 13])\n",
            "torch.Size([1617, 13])\n",
            "torch.Size([1617, 13])\n",
            "Avg Action Loss, 6 batches: 0.02014729380607605\n",
            "Avg Block Loss, 6 batches: 0.03421587496995926\n",
            "Avg Challenge Loss, 7 batches: 0.028127770870923996\n",
            "Avg Card Loss, 2 batches: 0.05315583571791649\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 13 of 1000\n",
            "epsilon: 0.9369146928798039\n",
            "gamma: 0.99\n",
            "Number of games in episode 13: 41\n",
            "torch.Size([1639, 12])\n",
            "torch.Size([1639, 13])\n",
            "torch.Size([1639, 13])\n",
            "torch.Size([1639, 13])\n",
            "Avg Action Loss, 7 batches: 0.014683299697935581\n",
            "Avg Block Loss, 6 batches: 0.019069289788603783\n",
            "Avg Challenge Loss, 7 batches: 0.016354605555534363\n",
            "Avg Card Loss, 2 batches: 0.058751802891492844\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 14 of 1000\n",
            "epsilon: 0.9322301194154049\n",
            "gamma: 0.99\n",
            "Number of games in episode 14: 41\n",
            "torch.Size([1606, 12])\n",
            "torch.Size([1606, 13])\n",
            "torch.Size([1606, 13])\n",
            "torch.Size([1606, 13])\n",
            "Avg Action Loss, 6 batches: 0.022560209035873413\n",
            "Avg Block Loss, 6 batches: 0.007442571222782135\n",
            "Avg Challenge Loss, 7 batches: 0.024659289047122\n",
            "Avg Card Loss, 2 batches: 0.03848917782306671\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 15 of 1000\n",
            "epsilon: 0.9275689688183278\n",
            "gamma: 0.99\n",
            "Number of games in episode 15: 43\n",
            "torch.Size([1642, 12])\n",
            "torch.Size([1642, 13])\n",
            "torch.Size([1642, 13])\n",
            "torch.Size([1642, 13])\n",
            "Avg Action Loss, 6 batches: 0.005591583903878927\n",
            "Avg Block Loss, 6 batches: 0.025931306183338165\n",
            "Avg Challenge Loss, 7 batches: 0.02380974031984806\n",
            "Avg Card Loss, 2 batches: 0.031490497291088104\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 16 of 1000\n",
            "epsilon: 0.9229311239742362\n",
            "gamma: 0.99\n",
            "Number of games in episode 16: 42\n",
            "torch.Size([1633, 12])\n",
            "torch.Size([1633, 13])\n",
            "torch.Size([1633, 13])\n",
            "torch.Size([1633, 13])\n",
            "Avg Action Loss, 6 batches: 0.01623990386724472\n",
            "Avg Block Loss, 5 batches: 0.02796485647559166\n",
            "Avg Challenge Loss, 7 batches: 0.022908858954906464\n",
            "Avg Card Loss, 2 batches: 0.03834182769060135\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 17 of 1000\n",
            "epsilon: 0.918316468354365\n",
            "gamma: 0.99\n",
            "Number of games in episode 17: 42\n",
            "torch.Size([1636, 12])\n",
            "torch.Size([1636, 13])\n",
            "torch.Size([1636, 13])\n",
            "torch.Size([1636, 13])\n",
            "Avg Action Loss, 6 batches: 0.021571394056081772\n",
            "Avg Block Loss, 6 batches: 0.0292016863822937\n",
            "Avg Challenge Loss, 7 batches: 0.013563310727477074\n",
            "Avg Card Loss, 2 batches: 0.01864299736917019\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 18 of 1000\n",
            "epsilon: 0.9137248860125932\n",
            "gamma: 0.99\n",
            "Number of games in episode 18: 42\n",
            "torch.Size([1637, 12])\n",
            "torch.Size([1637, 13])\n",
            "torch.Size([1637, 13])\n",
            "torch.Size([1637, 13])\n",
            "Avg Action Loss, 6 batches: 0.010943098925054073\n",
            "Avg Block Loss, 5 batches: 0.005724064540117979\n",
            "Avg Challenge Loss, 6 batches: 0.0069118160754442215\n",
            "Avg Card Loss, 2 batches: 0.01872088573873043\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 19 of 1000\n",
            "epsilon: 0.9091562615825302\n",
            "gamma: 0.99\n",
            "Number of games in episode 19: 41\n",
            "torch.Size([1602, 12])\n",
            "torch.Size([1602, 13])\n",
            "torch.Size([1602, 13])\n",
            "torch.Size([1602, 13])\n",
            "Avg Action Loss, 6 batches: 0.006225753575563431\n",
            "Avg Block Loss, 5 batches: 0.009500696323812008\n",
            "Avg Challenge Loss, 6 batches: 0.03185632824897766\n",
            "Avg Card Loss, 2 batches: 0.031218675896525383\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 20 of 1000\n",
            "epsilon: 0.9046104802746175\n",
            "gamma: 0.99\n",
            "Number of games in episode 20: 40\n",
            "torch.Size([1623, 12])\n",
            "torch.Size([1623, 13])\n",
            "torch.Size([1623, 13])\n",
            "torch.Size([1623, 13])\n",
            "Avg Action Loss, 6 batches: 0.008439837023615837\n",
            "Avg Block Loss, 6 batches: 0.013102704659104347\n",
            "Avg Challenge Loss, 7 batches: 0.03467009589076042\n",
            "Avg Card Loss, 2 batches: 0.025168336927890778\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "Most common action for Bot 0: 6 taken 790 times.\n",
            "total game lengths: 4334\n",
            "win rate: 0.11\n",
            "episode 21 of 1000\n",
            "epsilon: 0.9000874278732445\n",
            "gamma: 0.99\n",
            "Number of games in episode 21: 44\n",
            "torch.Size([1611, 12])\n",
            "torch.Size([1611, 13])\n",
            "torch.Size([1611, 13])\n",
            "torch.Size([1611, 13])\n",
            "Avg Action Loss, 6 batches: 0.020222971215844154\n",
            "Avg Block Loss, 6 batches: 0.025012750178575516\n",
            "Avg Challenge Loss, 7 batches: 0.03502410650253296\n",
            "Avg Card Loss, 2 batches: 0.040740855038166046\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 22 of 1000\n",
            "epsilon: 0.8955869907338783\n",
            "gamma: 0.99\n",
            "Number of games in episode 22: 44\n",
            "torch.Size([1640, 12])\n",
            "torch.Size([1640, 13])\n",
            "torch.Size([1640, 13])\n",
            "torch.Size([1640, 13])\n",
            "Avg Action Loss, 6 batches: 0.039212923496961594\n",
            "Avg Block Loss, 6 batches: 0.014430799521505833\n",
            "Avg Challenge Loss, 6 batches: 0.02053314447402954\n",
            "Avg Card Loss, 2 batches: 0.032153062522411346\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 23 of 1000\n",
            "epsilon: 0.8911090557802088\n",
            "gamma: 0.99\n",
            "Number of games in episode 23: 41\n",
            "torch.Size([1607, 12])\n",
            "torch.Size([1607, 13])\n",
            "torch.Size([1607, 13])\n",
            "torch.Size([1607, 13])\n",
            "Avg Action Loss, 6 batches: 0.01522780954837799\n",
            "Avg Block Loss, 6 batches: 0.024043269455432892\n",
            "Avg Challenge Loss, 7 batches: 0.01366593036800623\n",
            "Avg Card Loss, 2 batches: 0.02638857439160347\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 24 of 1000\n",
            "epsilon: 0.8866535105013078\n",
            "gamma: 0.99\n",
            "Number of games in episode 24: 41\n",
            "torch.Size([1615, 12])\n",
            "torch.Size([1615, 13])\n",
            "torch.Size([1615, 13])\n",
            "torch.Size([1615, 13])\n",
            "Avg Action Loss, 6 batches: 0.0039941417053341866\n",
            "Avg Block Loss, 5 batches: 0.028981242328882217\n",
            "Avg Challenge Loss, 6 batches: 0.02335984632372856\n",
            "Avg Card Loss, 2 batches: 0.040921974927186966\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 25 of 1000\n",
            "epsilon: 0.8822202429488013\n",
            "gamma: 0.99\n",
            "Number of games in episode 25: 40\n",
            "torch.Size([1635, 12])\n",
            "torch.Size([1635, 13])\n",
            "torch.Size([1635, 13])\n",
            "torch.Size([1635, 13])\n",
            "Avg Action Loss, 6 batches: 0.004155979957431555\n",
            "Avg Block Loss, 6 batches: 0.020244769752025604\n",
            "Avg Challenge Loss, 7 batches: 0.019527744501829147\n",
            "Avg Card Loss, 2 batches: 0.02837473340332508\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 26 of 1000\n",
            "epsilon: 0.8778091417340573\n",
            "gamma: 0.99\n",
            "Number of games in episode 26: 42\n",
            "torch.Size([1633, 12])\n",
            "torch.Size([1633, 13])\n",
            "torch.Size([1633, 13])\n",
            "torch.Size([1633, 13])\n",
            "Avg Action Loss, 6 batches: 0.01761540398001671\n",
            "Avg Block Loss, 6 batches: 0.02488286793231964\n",
            "Avg Challenge Loss, 7 batches: 0.022000156342983246\n",
            "Avg Card Loss, 2 batches: 0.03485943377017975\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 27 of 1000\n",
            "epsilon: 0.8734200960253871\n",
            "gamma: 0.99\n",
            "Number of games in episode 27: 41\n",
            "torch.Size([1631, 12])\n",
            "torch.Size([1631, 13])\n",
            "torch.Size([1631, 13])\n",
            "torch.Size([1631, 13])\n",
            "Avg Action Loss, 6 batches: 0.005933333188295364\n",
            "Avg Block Loss, 6 batches: 0.010231029242277145\n",
            "Avg Challenge Loss, 7 batches: 0.007035338785499334\n",
            "Avg Card Loss, 2 batches: 0.036756761372089386\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 28 of 1000\n",
            "epsilon: 0.8690529955452602\n",
            "gamma: 0.99\n",
            "Number of games in episode 28: 40\n",
            "torch.Size([1622, 12])\n",
            "torch.Size([1622, 13])\n",
            "torch.Size([1622, 13])\n",
            "torch.Size([1622, 13])\n",
            "Avg Action Loss, 6 batches: 0.019584618508815765\n",
            "Avg Block Loss, 6 batches: 0.0034391917288303375\n",
            "Avg Challenge Loss, 6 batches: 0.016142573207616806\n",
            "Avg Card Loss, 2 batches: 0.01615476794540882\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 29 of 1000\n",
            "epsilon: 0.8647077305675338\n",
            "gamma: 0.99\n",
            "Number of games in episode 29: 43\n",
            "torch.Size([1626, 12])\n",
            "torch.Size([1626, 13])\n",
            "torch.Size([1626, 13])\n",
            "torch.Size([1626, 13])\n",
            "Avg Action Loss, 6 batches: 0.008838017471134663\n",
            "Avg Block Loss, 5 batches: 0.007012789603322744\n",
            "Avg Challenge Loss, 7 batches: 0.008182741701602936\n",
            "Avg Card Loss, 2 batches: 0.015310395509004593\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 30 of 1000\n",
            "epsilon: 0.8603841919146962\n",
            "gamma: 0.99\n",
            "Number of games in episode 30: 41\n",
            "torch.Size([1606, 12])\n",
            "torch.Size([1606, 13])\n",
            "torch.Size([1606, 13])\n",
            "torch.Size([1606, 13])\n",
            "Avg Action Loss, 6 batches: 0.0024196647573262453\n",
            "Avg Block Loss, 5 batches: 0.0033038074616342783\n",
            "Avg Challenge Loss, 6 batches: 0.01547627616673708\n",
            "Avg Card Loss, 1 batches: 0.03155505284667015\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "Most common action for Bot 0: 4 taken 930 times.\n",
            "total game lengths: 4958\n",
            "win rate: 0.4\n",
            "episode 31 of 1000\n",
            "epsilon: 0.8560822709551227\n",
            "gamma: 0.99\n",
            "Number of games in episode 31: 39\n",
            "torch.Size([1604, 12])\n",
            "torch.Size([1604, 13])\n",
            "torch.Size([1604, 13])\n",
            "torch.Size([1604, 13])\n",
            "Avg Action Loss, 7 batches: 0.026059405878186226\n",
            "Avg Block Loss, 6 batches: 0.005687226541340351\n",
            "Avg Challenge Loss, 8 batches: 0.01217626128345728\n",
            "Avg Card Loss, 1 batches: 0.026420995593070984\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 32 of 1000\n",
            "epsilon: 0.851801859600347\n",
            "gamma: 0.99\n",
            "Number of games in episode 32: 43\n",
            "torch.Size([1614, 12])\n",
            "torch.Size([1614, 13])\n",
            "torch.Size([1614, 13])\n",
            "torch.Size([1614, 13])\n",
            "Avg Action Loss, 6 batches: 0.03096032328903675\n",
            "Avg Block Loss, 5 batches: 0.017140384763479233\n",
            "Avg Challenge Loss, 6 batches: 0.013498157262802124\n",
            "Avg Card Loss, 2 batches: 0.034310679882764816\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 33 of 1000\n",
            "epsilon: 0.8475428503023453\n",
            "gamma: 0.99\n",
            "Number of games in episode 33: 44\n",
            "torch.Size([1632, 12])\n",
            "torch.Size([1632, 13])\n",
            "torch.Size([1632, 13])\n",
            "torch.Size([1632, 13])\n",
            "Avg Action Loss, 6 batches: 0.040423884987831116\n",
            "Avg Block Loss, 6 batches: 0.022191591560840607\n",
            "Avg Challenge Loss, 7 batches: 0.03899121657013893\n",
            "Avg Card Loss, 2 batches: 0.027072882279753685\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 34 of 1000\n",
            "epsilon: 0.8433051360508336\n",
            "gamma: 0.99\n",
            "Number of games in episode 34: 45\n",
            "torch.Size([1627, 12])\n",
            "torch.Size([1627, 13])\n",
            "torch.Size([1627, 13])\n",
            "torch.Size([1627, 13])\n",
            "Avg Action Loss, 6 batches: 0.011425605975091457\n",
            "Avg Block Loss, 5 batches: 0.04231059178709984\n",
            "Avg Challenge Loss, 6 batches: 0.01799318566918373\n",
            "Avg Card Loss, 2 batches: 0.03706483915448189\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 35 of 1000\n",
            "epsilon: 0.8390886103705794\n",
            "gamma: 0.99\n",
            "Number of games in episode 35: 45\n",
            "torch.Size([1638, 12])\n",
            "torch.Size([1638, 13])\n",
            "torch.Size([1638, 13])\n",
            "torch.Size([1638, 13])\n",
            "Avg Action Loss, 6 batches: 0.01741274818778038\n",
            "Avg Block Loss, 6 batches: 0.02904645726084709\n",
            "Avg Challenge Loss, 7 batches: 0.008682653307914734\n",
            "Avg Card Loss, 2 batches: 0.02220255881547928\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 36 of 1000\n",
            "epsilon: 0.8348931673187264\n",
            "gamma: 0.99\n",
            "Number of games in episode 36: 43\n",
            "torch.Size([1601, 12])\n",
            "torch.Size([1601, 13])\n",
            "torch.Size([1601, 13])\n",
            "torch.Size([1601, 13])\n",
            "Avg Action Loss, 6 batches: 0.015559522435069084\n",
            "Avg Block Loss, 6 batches: 0.029048539698123932\n",
            "Avg Challenge Loss, 7 batches: 0.026944773271679878\n",
            "Avg Card Loss, 2 batches: 0.030018219724297523\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 37 of 1000\n",
            "epsilon: 0.8307187014821328\n",
            "gamma: 0.99\n",
            "Number of games in episode 37: 43\n",
            "torch.Size([1620, 12])\n",
            "torch.Size([1620, 13])\n",
            "torch.Size([1620, 13])\n",
            "torch.Size([1620, 13])\n",
            "Avg Action Loss, 6 batches: 0.01799202337861061\n",
            "Avg Block Loss, 6 batches: 0.03767073526978493\n",
            "Avg Challenge Loss, 7 batches: 0.01168269757181406\n",
            "Avg Card Loss, 2 batches: 0.02186298742890358\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 38 of 1000\n",
            "epsilon: 0.8265651079747222\n",
            "gamma: 0.99\n",
            "Number of games in episode 38: 42\n",
            "torch.Size([1616, 12])\n",
            "torch.Size([1616, 13])\n",
            "torch.Size([1616, 13])\n",
            "torch.Size([1616, 13])\n",
            "Avg Action Loss, 6 batches: 0.023141512647271156\n",
            "Avg Block Loss, 5 batches: 0.014222318306565285\n",
            "Avg Challenge Loss, 7 batches: 0.011848525144159794\n",
            "Avg Card Loss, 2 batches: 0.021278273314237595\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 39 of 1000\n",
            "epsilon: 0.8224322824348486\n",
            "gamma: 0.99\n",
            "Number of games in episode 39: 41\n",
            "torch.Size([1633, 12])\n",
            "torch.Size([1633, 13])\n",
            "torch.Size([1633, 13])\n",
            "torch.Size([1633, 13])\n",
            "Avg Action Loss, 6 batches: 0.005805351305752993\n",
            "Avg Block Loss, 5 batches: 0.007542384788393974\n",
            "Avg Challenge Loss, 6 batches: 0.025921352207660675\n",
            "Avg Card Loss, 2 batches: 0.02652004174888134\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 40 of 1000\n",
            "epsilon: 0.8183201210226743\n",
            "gamma: 0.99\n",
            "Number of games in episode 40: 41\n",
            "torch.Size([1633, 12])\n",
            "torch.Size([1633, 13])\n",
            "torch.Size([1633, 13])\n",
            "torch.Size([1633, 13])\n",
            "Avg Action Loss, 6 batches: 0.01177132036536932\n",
            "Avg Block Loss, 5 batches: 0.014656558632850647\n",
            "Avg Challenge Loss, 7 batches: 0.00460557546466589\n",
            "Avg Card Loss, 2 batches: 0.027923496440052986\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "Most common action for Bot 0: 4 taken 978 times.\n",
            "total game lengths: 4973\n",
            "win rate: 0.3\n",
            "episode 41 of 1000\n",
            "epsilon: 0.8142285204175609\n",
            "gamma: 0.99\n",
            "Number of games in episode 41: 39\n",
            "torch.Size([1640, 12])\n",
            "torch.Size([1640, 13])\n",
            "torch.Size([1640, 13])\n",
            "torch.Size([1640, 13])\n",
            "Avg Action Loss, 6 batches: 0.01312053482979536\n",
            "Avg Block Loss, 6 batches: 0.020799290388822556\n",
            "Avg Challenge Loss, 7 batches: 0.020549405366182327\n",
            "Avg Card Loss, 2 batches: 0.021786630153656006\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 42 of 1000\n",
            "epsilon: 0.810157377815473\n",
            "gamma: 0.99\n",
            "Number of games in episode 42: 42\n",
            "torch.Size([1632, 12])\n",
            "torch.Size([1632, 13])\n",
            "torch.Size([1632, 13])\n",
            "torch.Size([1632, 13])\n",
            "Avg Action Loss, 6 batches: 0.03928384557366371\n",
            "Avg Block Loss, 5 batches: 0.03607823699712753\n",
            "Avg Challenge Loss, 7 batches: 0.007726520299911499\n",
            "Avg Card Loss, 2 batches: 0.023024801164865494\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 43 of 1000\n",
            "epsilon: 0.8061065909263957\n",
            "gamma: 0.99\n",
            "Number of games in episode 43: 41\n",
            "torch.Size([1619, 12])\n",
            "torch.Size([1619, 13])\n",
            "torch.Size([1619, 13])\n",
            "torch.Size([1619, 13])\n",
            "Avg Action Loss, 6 batches: 0.009416036307811737\n",
            "Avg Block Loss, 5 batches: 0.02332470566034317\n",
            "Avg Challenge Loss, 7 batches: 0.015388760715723038\n",
            "Avg Card Loss, 2 batches: 0.031168406829237938\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 44 of 1000\n",
            "epsilon: 0.8020760579717637\n",
            "gamma: 0.99\n",
            "Number of games in episode 44: 41\n",
            "torch.Size([1612, 12])\n",
            "torch.Size([1612, 13])\n",
            "torch.Size([1612, 13])\n",
            "torch.Size([1612, 13])\n",
            "Avg Action Loss, 6 batches: 0.020617995411157608\n",
            "Avg Block Loss, 5 batches: 0.039586104452610016\n",
            "Avg Challenge Loss, 6 batches: 0.014800858683884144\n",
            "Avg Card Loss, 2 batches: 0.02303551509976387\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 45 of 1000\n",
            "epsilon: 0.798065677681905\n",
            "gamma: 0.99\n",
            "Number of games in episode 45: 40\n",
            "torch.Size([1633, 12])\n",
            "torch.Size([1633, 13])\n",
            "torch.Size([1633, 13])\n",
            "torch.Size([1633, 13])\n",
            "Avg Action Loss, 6 batches: 0.022081036120653152\n",
            "Avg Block Loss, 6 batches: 0.030847856774926186\n",
            "Avg Challenge Loss, 8 batches: 0.00372130679897964\n",
            "Avg Card Loss, 1 batches: 0.025227725505828857\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 46 of 1000\n",
            "epsilon: 0.7940753492934954\n",
            "gamma: 0.99\n",
            "Number of games in episode 46: 41\n",
            "torch.Size([1646, 12])\n",
            "torch.Size([1646, 13])\n",
            "torch.Size([1646, 13])\n",
            "torch.Size([1646, 13])\n",
            "Avg Action Loss, 6 batches: 0.002153265057131648\n",
            "Avg Block Loss, 5 batches: 0.017907951027154922\n",
            "Avg Challenge Loss, 7 batches: 0.01046860869973898\n",
            "Avg Card Loss, 2 batches: 0.01801135763525963\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 47 of 1000\n",
            "epsilon: 0.7901049725470279\n",
            "gamma: 0.99\n",
            "Number of games in episode 47: 40\n",
            "torch.Size([1626, 12])\n",
            "torch.Size([1626, 13])\n",
            "torch.Size([1626, 13])\n",
            "torch.Size([1626, 13])\n",
            "Avg Action Loss, 6 batches: 0.0070167542435228825\n",
            "Avg Block Loss, 5 batches: 0.00910455547273159\n",
            "Avg Challenge Loss, 6 batches: 0.0047636013478040695\n",
            "Avg Card Loss, 1 batches: 0.016764484345912933\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 48 of 1000\n",
            "epsilon: 0.7861544476842928\n",
            "gamma: 0.99\n",
            "Number of games in episode 48: 41\n",
            "torch.Size([1607, 12])\n",
            "torch.Size([1607, 13])\n",
            "torch.Size([1607, 13])\n",
            "torch.Size([1607, 13])\n",
            "Avg Action Loss, 6 batches: 0.009225367568433285\n",
            "Avg Block Loss, 5 batches: 0.01649405062198639\n",
            "Avg Challenge Loss, 7 batches: 0.0041635483503341675\n",
            "Avg Card Loss, 2 batches: 0.03444385156035423\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 49 of 1000\n",
            "epsilon: 0.7822236754458713\n",
            "gamma: 0.99\n",
            "Number of games in episode 49: 40\n",
            "torch.Size([1610, 12])\n",
            "torch.Size([1610, 13])\n",
            "torch.Size([1610, 13])\n",
            "torch.Size([1610, 13])\n",
            "Avg Action Loss, 6 batches: 0.009105531498789787\n",
            "Avg Block Loss, 5 batches: 0.0036733164452016354\n",
            "Avg Challenge Loss, 7 batches: 0.022804800420999527\n",
            "Avg Card Loss, 1 batches: 0.014464268460869789\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 50 of 1000\n",
            "epsilon: 0.778312557068642\n",
            "gamma: 0.99\n",
            "Number of games in episode 50: 41\n",
            "torch.Size([1630, 12])\n",
            "torch.Size([1630, 13])\n",
            "torch.Size([1630, 13])\n",
            "torch.Size([1630, 13])\n",
            "Avg Action Loss, 7 batches: 0.013194617815315723\n",
            "Avg Block Loss, 6 batches: 0.026449663564562798\n",
            "Avg Challenge Loss, 7 batches: 0.02342039905488491\n",
            "Avg Card Loss, 2 batches: 0.015997685492038727\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "Most common action for Bot 0: 4 taken 956 times.\n",
            "total game lengths: 4412\n",
            "win rate: 0.2\n",
            "episode 51 of 1000\n",
            "epsilon: 0.7744209942832988\n",
            "gamma: 0.99\n",
            "Number of games in episode 51: 40\n",
            "torch.Size([1641, 12])\n",
            "torch.Size([1641, 13])\n",
            "torch.Size([1641, 13])\n",
            "torch.Size([1641, 13])\n",
            "Avg Action Loss, 6 batches: 0.03143729269504547\n",
            "Avg Block Loss, 6 batches: 0.01507662795484066\n",
            "Avg Challenge Loss, 8 batches: 0.0189349427819252\n",
            "Avg Card Loss, 1 batches: 0.022386692464351654\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 52 of 1000\n",
            "epsilon: 0.7705488893118823\n",
            "gamma: 0.99\n",
            "Number of games in episode 52: 39\n",
            "torch.Size([1619, 12])\n",
            "torch.Size([1619, 13])\n",
            "torch.Size([1619, 13])\n",
            "torch.Size([1619, 13])\n",
            "Avg Action Loss, 6 batches: 0.010308497585356236\n",
            "Avg Block Loss, 5 batches: 0.026890870183706284\n",
            "Avg Challenge Loss, 7 batches: 0.015849513933062553\n",
            "Avg Card Loss, 1 batches: 0.02062944322824478\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 53 of 1000\n",
            "epsilon: 0.7666961448653229\n",
            "gamma: 0.99\n",
            "Number of games in episode 53: 40\n",
            "torch.Size([1607, 12])\n",
            "torch.Size([1607, 13])\n",
            "torch.Size([1607, 13])\n",
            "torch.Size([1607, 13])\n",
            "Avg Action Loss, 6 batches: 0.006895211059600115\n",
            "Avg Block Loss, 5 batches: 0.00390713382512331\n",
            "Avg Challenge Loss, 6 batches: 0.010451780632138252\n",
            "Avg Card Loss, 2 batches: 0.021595485508441925\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 54 of 1000\n",
            "epsilon: 0.7628626641409962\n",
            "gamma: 0.99\n",
            "Number of games in episode 54: 42\n",
            "torch.Size([1617, 12])\n",
            "torch.Size([1617, 13])\n",
            "torch.Size([1617, 13])\n",
            "torch.Size([1617, 13])\n",
            "Avg Action Loss, 5 batches: 0.012268205173313618\n",
            "Avg Block Loss, 6 batches: 0.01009633019566536\n",
            "Avg Challenge Loss, 7 batches: 0.006333607714623213\n",
            "Avg Card Loss, 2 batches: 0.027623042464256287\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 55 of 1000\n",
            "epsilon: 0.7590483508202912\n",
            "gamma: 0.99\n",
            "Number of games in episode 55: 40\n",
            "torch.Size([1630, 12])\n",
            "torch.Size([1630, 13])\n",
            "torch.Size([1630, 13])\n",
            "torch.Size([1630, 13])\n",
            "Avg Action Loss, 6 batches: 0.011188870295882225\n",
            "Avg Block Loss, 5 batches: 0.007671622559428215\n",
            "Avg Challenge Loss, 7 batches: 0.023944344371557236\n",
            "Avg Card Loss, 1 batches: 0.017682380974292755\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 56 of 1000\n",
            "epsilon: 0.7552531090661897\n",
            "gamma: 0.99\n",
            "Number of games in episode 56: 42\n",
            "torch.Size([1608, 12])\n",
            "torch.Size([1608, 13])\n",
            "torch.Size([1608, 13])\n",
            "torch.Size([1608, 13])\n",
            "Avg Action Loss, 6 batches: 0.012660383246839046\n",
            "Avg Block Loss, 6 batches: 0.011905652470886707\n",
            "Avg Challenge Loss, 7 batches: 0.016513166949152946\n",
            "Avg Card Loss, 1 batches: 0.03589366003870964\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 57 of 1000\n",
            "epsilon: 0.7514768435208588\n",
            "gamma: 0.99\n",
            "Number of games in episode 57: 42\n",
            "torch.Size([1627, 12])\n",
            "torch.Size([1627, 13])\n",
            "torch.Size([1627, 13])\n",
            "torch.Size([1627, 13])\n",
            "Avg Action Loss, 6 batches: 0.003927074838429689\n",
            "Avg Block Loss, 6 batches: 0.020605655387043953\n",
            "Avg Challenge Loss, 6 batches: 0.01066666655242443\n",
            "Avg Card Loss, 1 batches: 0.03006218932569027\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 58 of 1000\n",
            "epsilon: 0.7477194593032545\n",
            "gamma: 0.99\n",
            "Number of games in episode 58: 44\n",
            "torch.Size([1603, 12])\n",
            "torch.Size([1603, 13])\n",
            "torch.Size([1603, 13])\n",
            "torch.Size([1603, 13])\n",
            "Avg Action Loss, 6 batches: 0.023486634716391563\n",
            "Avg Block Loss, 5 batches: 0.005694741848856211\n",
            "Avg Challenge Loss, 6 batches: 0.011847678571939468\n",
            "Avg Card Loss, 2 batches: 0.015060532838106155\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 59 of 1000\n",
            "epsilon: 0.7439808620067382\n",
            "gamma: 0.99\n",
            "Number of games in episode 59: 44\n",
            "torch.Size([1635, 12])\n",
            "torch.Size([1635, 13])\n",
            "torch.Size([1635, 13])\n",
            "torch.Size([1635, 13])\n",
            "Avg Action Loss, 6 batches: 0.03391687944531441\n",
            "Avg Block Loss, 5 batches: 0.03070799820125103\n",
            "Avg Challenge Loss, 6 batches: 0.04091809689998627\n",
            "Avg Card Loss, 1 batches: 0.04145347326993942\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 60 of 1000\n",
            "epsilon: 0.7402609576967045\n",
            "gamma: 0.99\n",
            "Number of games in episode 60: 42\n",
            "torch.Size([1609, 12])\n",
            "torch.Size([1609, 13])\n",
            "torch.Size([1609, 13])\n",
            "torch.Size([1609, 13])\n",
            "Avg Action Loss, 6 batches: 0.008585646748542786\n",
            "Avg Block Loss, 5 batches: 0.013347002677619457\n",
            "Avg Challenge Loss, 6 batches: 0.005092049948871136\n",
            "Avg Card Loss, 1 batches: 0.03131741285324097\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "Most common action for Bot 0: 3 taken 615 times.\n",
            "total game lengths: 3314\n",
            "win rate: 0.4\n",
            "episode 61 of 1000\n",
            "epsilon: 0.736559652908221\n",
            "gamma: 0.99\n",
            "Number of games in episode 61: 43\n",
            "torch.Size([1601, 12])\n",
            "torch.Size([1601, 13])\n",
            "torch.Size([1601, 13])\n",
            "torch.Size([1601, 13])\n",
            "Avg Action Loss, 6 batches: 0.038396481424570084\n",
            "Avg Block Loss, 5 batches: 0.03163081407546997\n",
            "Avg Challenge Loss, 6 batches: 0.01879606395959854\n",
            "Avg Card Loss, 2 batches: 0.0215372983366251\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 62 of 1000\n",
            "epsilon: 0.7328768546436799\n",
            "gamma: 0.99\n",
            "Number of games in episode 62: 45\n",
            "torch.Size([1612, 12])\n",
            "torch.Size([1612, 13])\n",
            "torch.Size([1612, 13])\n",
            "torch.Size([1612, 13])\n",
            "Avg Action Loss, 6 batches: 0.012134377844631672\n",
            "Avg Block Loss, 5 batches: 0.0076400237157940865\n",
            "Avg Challenge Loss, 6 batches: 0.0317477248609066\n",
            "Avg Card Loss, 1 batches: 0.017398623749613762\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 63 of 1000\n",
            "epsilon: 0.7292124703704616\n",
            "gamma: 0.99\n",
            "Number of games in episode 63: 44\n",
            "torch.Size([1615, 12])\n",
            "torch.Size([1615, 13])\n",
            "torch.Size([1615, 13])\n",
            "torch.Size([1615, 13])\n",
            "Avg Action Loss, 6 batches: 0.02660098671913147\n",
            "Avg Block Loss, 5 batches: 0.010137729346752167\n",
            "Avg Challenge Loss, 6 batches: 0.00785768311470747\n",
            "Avg Card Loss, 2 batches: 0.0368804857134819\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 64 of 1000\n",
            "epsilon: 0.7255664080186093\n",
            "gamma: 0.99\n",
            "Number of games in episode 64: 45\n",
            "torch.Size([1610, 12])\n",
            "torch.Size([1610, 13])\n",
            "torch.Size([1610, 13])\n",
            "torch.Size([1610, 13])\n",
            "Avg Action Loss, 6 batches: 0.040994662791490555\n",
            "Avg Block Loss, 5 batches: 0.021635618060827255\n",
            "Avg Challenge Loss, 6 batches: 0.009766827337443829\n",
            "Avg Card Loss, 1 batches: 0.024436356499791145\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 65 of 1000\n",
            "epsilon: 0.7219385759785162\n",
            "gamma: 0.99\n",
            "Number of games in episode 65: 42\n",
            "torch.Size([1605, 12])\n",
            "torch.Size([1605, 13])\n",
            "torch.Size([1605, 13])\n",
            "torch.Size([1605, 13])\n",
            "Avg Action Loss, 5 batches: 0.012401474639773369\n",
            "Avg Block Loss, 5 batches: 0.028186596930027008\n",
            "Avg Challenge Loss, 6 batches: 0.0114999795332551\n",
            "Avg Card Loss, 1 batches: 0.02990620583295822\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 66 of 1000\n",
            "epsilon: 0.7183288830986236\n",
            "gamma: 0.99\n",
            "Number of games in episode 66: 45\n",
            "torch.Size([1642, 12])\n",
            "torch.Size([1642, 13])\n",
            "torch.Size([1642, 13])\n",
            "torch.Size([1642, 13])\n",
            "Avg Action Loss, 6 batches: 0.01675281673669815\n",
            "Avg Block Loss, 5 batches: 0.02130710706114769\n",
            "Avg Challenge Loss, 6 batches: 0.009273890405893326\n",
            "Avg Card Loss, 2 batches: 0.01586373895406723\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 67 of 1000\n",
            "epsilon: 0.7147372386831305\n",
            "gamma: 0.99\n",
            "Number of games in episode 67: 41\n",
            "torch.Size([1601, 12])\n",
            "torch.Size([1601, 13])\n",
            "torch.Size([1601, 13])\n",
            "torch.Size([1601, 13])\n",
            "Avg Action Loss, 6 batches: 0.014202439226210117\n",
            "Avg Block Loss, 6 batches: 0.023895952850580215\n",
            "Avg Challenge Loss, 7 batches: 0.011501488275825977\n",
            "Avg Card Loss, 1 batches: 0.04702906683087349\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 68 of 1000\n",
            "epsilon: 0.7111635524897149\n",
            "gamma: 0.99\n",
            "Number of games in episode 68: 41\n",
            "torch.Size([1622, 12])\n",
            "torch.Size([1622, 13])\n",
            "torch.Size([1622, 13])\n",
            "torch.Size([1622, 13])\n",
            "Avg Action Loss, 6 batches: 0.011614271439611912\n",
            "Avg Block Loss, 6 batches: 0.01831432431936264\n",
            "Avg Challenge Loss, 8 batches: 0.024606920778751373\n",
            "Avg Card Loss, 2 batches: 0.03085504285991192\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 69 of 1000\n",
            "epsilon: 0.7076077347272662\n",
            "gamma: 0.99\n",
            "Number of games in episode 69: 41\n",
            "torch.Size([1625, 12])\n",
            "torch.Size([1625, 13])\n",
            "torch.Size([1625, 13])\n",
            "torch.Size([1625, 13])\n",
            "Avg Action Loss, 6 batches: 0.007467582821846008\n",
            "Avg Block Loss, 5 batches: 0.0309845432639122\n",
            "Avg Challenge Loss, 7 batches: 0.006933108903467655\n",
            "Avg Card Loss, 1 batches: 0.025147711858153343\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 70 of 1000\n",
            "epsilon: 0.7040696960536299\n",
            "gamma: 0.99\n",
            "Number of games in episode 70: 40\n",
            "torch.Size([1612, 12])\n",
            "torch.Size([1612, 13])\n",
            "torch.Size([1612, 13])\n",
            "torch.Size([1612, 13])\n",
            "Avg Action Loss, 6 batches: 0.018783360719680786\n",
            "Avg Block Loss, 6 batches: 0.050181176513433456\n",
            "Avg Challenge Loss, 7 batches: 0.00824928842484951\n",
            "Avg Card Loss, 1 batches: 0.018177874386310577\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "Most common action for Bot 0: 4 taken 797 times.\n",
            "total game lengths: 4275\n",
            "win rate: 0.13\n",
            "episode 71 of 1000\n",
            "epsilon: 0.7005493475733617\n",
            "gamma: 0.99\n",
            "Number of games in episode 71: 40\n",
            "torch.Size([1617, 12])\n",
            "torch.Size([1617, 13])\n",
            "torch.Size([1617, 13])\n",
            "torch.Size([1617, 13])\n",
            "Avg Action Loss, 6 batches: 0.004951922222971916\n",
            "Avg Block Loss, 5 batches: 0.016110597178339958\n",
            "Avg Challenge Loss, 7 batches: 0.02166854962706566\n",
            "Avg Card Loss, 1 batches: 0.023327216506004333\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 72 of 1000\n",
            "epsilon: 0.697046600835495\n",
            "gamma: 0.99\n",
            "Number of games in episode 72: 41\n",
            "torch.Size([1611, 12])\n",
            "torch.Size([1611, 13])\n",
            "torch.Size([1611, 13])\n",
            "torch.Size([1611, 13])\n",
            "Avg Action Loss, 6 batches: 0.010738179087638855\n",
            "Avg Block Loss, 5 batches: 0.007322852499783039\n",
            "Avg Challenge Loss, 7 batches: 0.008730962872505188\n",
            "Avg Card Loss, 2 batches: 0.01968506909906864\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 73 of 1000\n",
            "epsilon: 0.6935613678313175\n",
            "gamma: 0.99\n",
            "Number of games in episode 73: 40\n",
            "torch.Size([1617, 12])\n",
            "torch.Size([1617, 13])\n",
            "torch.Size([1617, 13])\n",
            "torch.Size([1617, 13])\n",
            "Avg Action Loss, 6 batches: 0.017331523820757866\n",
            "Avg Block Loss, 6 batches: 0.02379143424332142\n",
            "Avg Challenge Loss, 7 batches: 0.026452209800481796\n",
            "Avg Card Loss, 1 batches: 0.02985367178916931\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 74 of 1000\n",
            "epsilon: 0.6900935609921609\n",
            "gamma: 0.99\n",
            "Number of games in episode 74: 38\n",
            "torch.Size([1609, 12])\n",
            "torch.Size([1609, 13])\n",
            "torch.Size([1609, 13])\n",
            "torch.Size([1609, 13])\n",
            "Avg Action Loss, 6 batches: 0.009996981360018253\n",
            "Avg Block Loss, 5 batches: 0.0038185955490916967\n",
            "Avg Challenge Loss, 7 batches: 0.0014115814119577408\n",
            "Avg Card Loss, 1 batches: 0.012403924018144608\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 75 of 1000\n",
            "epsilon: 0.6866430931872001\n",
            "gamma: 0.99\n",
            "Number of games in episode 75: 40\n",
            "torch.Size([1621, 12])\n",
            "torch.Size([1621, 13])\n",
            "torch.Size([1621, 13])\n",
            "torch.Size([1621, 13])\n",
            "Avg Action Loss, 6 batches: 0.02551424317061901\n",
            "Avg Block Loss, 5 batches: 0.012325945310294628\n",
            "Avg Challenge Loss, 6 batches: 0.01030657347291708\n",
            "Avg Card Loss, 1 batches: 0.009499168023467064\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 76 of 1000\n",
            "epsilon: 0.6832098777212641\n",
            "gamma: 0.99\n",
            "Number of games in episode 76: 40\n",
            "torch.Size([1640, 12])\n",
            "torch.Size([1640, 13])\n",
            "torch.Size([1640, 13])\n",
            "torch.Size([1640, 13])\n",
            "Avg Action Loss, 6 batches: 0.003363664960488677\n",
            "Avg Block Loss, 5 batches: 0.015150501392781734\n",
            "Avg Challenge Loss, 7 batches: 0.010385856963694096\n",
            "Avg Card Loss, 2 batches: 0.04042944684624672\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 77 of 1000\n",
            "epsilon: 0.6797938283326578\n",
            "gamma: 0.99\n",
            "Number of games in episode 77: 37\n",
            "torch.Size([1632, 12])\n",
            "torch.Size([1632, 13])\n",
            "torch.Size([1632, 13])\n",
            "torch.Size([1632, 13])\n",
            "Avg Action Loss, 6 batches: 0.011313487775623798\n",
            "Avg Block Loss, 5 batches: 0.011424046009778976\n",
            "Avg Challenge Loss, 7 batches: 0.011472808197140694\n",
            "Avg Card Loss, 1 batches: 0.018244069069623947\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 78 of 1000\n",
            "epsilon: 0.6763948591909945\n",
            "gamma: 0.99\n",
            "Number of games in episode 78: 38\n",
            "torch.Size([1631, 12])\n",
            "torch.Size([1631, 13])\n",
            "torch.Size([1631, 13])\n",
            "torch.Size([1631, 13])\n",
            "Avg Action Loss, 6 batches: 0.012395269237458706\n",
            "Avg Block Loss, 6 batches: 0.02213512919843197\n",
            "Avg Challenge Loss, 8 batches: 0.005424097180366516\n",
            "Avg Card Loss, 2 batches: 0.039471618831157684\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 79 of 1000\n",
            "epsilon: 0.6730128848950395\n",
            "gamma: 0.99\n",
            "Number of games in episode 79: 38\n",
            "torch.Size([1602, 12])\n",
            "torch.Size([1602, 13])\n",
            "torch.Size([1602, 13])\n",
            "torch.Size([1602, 13])\n",
            "Avg Action Loss, 6 batches: 0.02853311225771904\n",
            "Avg Block Loss, 6 batches: 0.016200155019760132\n",
            "Avg Challenge Loss, 7 batches: 0.025365274399518967\n",
            "Avg Card Loss, 1 batches: 0.034004293382167816\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 80 of 1000\n",
            "epsilon: 0.6696478204705644\n",
            "gamma: 0.99\n",
            "Number of games in episode 80: 41\n",
            "torch.Size([1636, 12])\n",
            "torch.Size([1636, 13])\n",
            "torch.Size([1636, 13])\n",
            "torch.Size([1636, 13])\n",
            "Avg Action Loss, 6 batches: 0.002971733221784234\n",
            "Avg Block Loss, 5 batches: 0.019899118691682816\n",
            "Avg Challenge Loss, 7 batches: 0.035636190325021744\n",
            "Avg Card Loss, 2 batches: 0.026247870177030563\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "Most common action for Bot 0: 4 taken 900 times.\n",
            "total game lengths: 4663\n",
            "win rate: 0.27\n",
            "episode 81 of 1000\n",
            "epsilon: 0.6662995813682115\n",
            "gamma: 0.99\n",
            "Number of games in episode 81: 40\n",
            "torch.Size([1612, 12])\n",
            "torch.Size([1612, 13])\n",
            "torch.Size([1612, 13])\n",
            "torch.Size([1612, 13])\n",
            "Avg Action Loss, 6 batches: 0.013125889003276825\n",
            "Avg Block Loss, 5 batches: 0.011946329846978188\n",
            "Avg Challenge Loss, 7 batches: 0.006730723660439253\n",
            "Avg Card Loss, 1 batches: 0.025861706584692\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 82 of 1000\n",
            "epsilon: 0.6629680834613705\n",
            "gamma: 0.99\n",
            "Number of games in episode 82: 40\n",
            "torch.Size([1609, 12])\n",
            "torch.Size([1609, 13])\n",
            "torch.Size([1609, 13])\n",
            "torch.Size([1609, 13])\n",
            "Avg Action Loss, 6 batches: 0.0225728340446949\n",
            "Avg Block Loss, 5 batches: 0.015333959832787514\n",
            "Avg Challenge Loss, 7 batches: 0.02456626109778881\n",
            "Avg Card Loss, 1 batches: 0.018999500200152397\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 83 of 1000\n",
            "epsilon: 0.6596532430440636\n",
            "gamma: 0.99\n",
            "Number of games in episode 83: 42\n",
            "torch.Size([1606, 12])\n",
            "torch.Size([1606, 13])\n",
            "torch.Size([1606, 13])\n",
            "torch.Size([1606, 13])\n",
            "Avg Action Loss, 6 batches: 0.005088053178042173\n",
            "Avg Block Loss, 5 batches: 0.0269023384898901\n",
            "Avg Challenge Loss, 7 batches: 0.01603909209370613\n",
            "Avg Card Loss, 1 batches: 0.01584026776254177\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 84 of 1000\n",
            "epsilon: 0.6563549768288433\n",
            "gamma: 0.99\n",
            "Number of games in episode 84: 43\n",
            "torch.Size([1631, 12])\n",
            "torch.Size([1631, 13])\n",
            "torch.Size([1631, 13])\n",
            "torch.Size([1631, 13])\n",
            "Avg Action Loss, 6 batches: 0.01209962461143732\n",
            "Avg Block Loss, 5 batches: 0.012662865221500397\n",
            "Avg Challenge Loss, 6 batches: 0.004847586154937744\n",
            "Avg Card Loss, 2 batches: 0.042990557849407196\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 85 of 1000\n",
            "epsilon: 0.653073201944699\n",
            "gamma: 0.99\n",
            "Number of games in episode 85: 44\n",
            "torch.Size([1614, 12])\n",
            "torch.Size([1614, 13])\n",
            "torch.Size([1614, 13])\n",
            "torch.Size([1614, 13])\n",
            "Avg Action Loss, 6 batches: 0.026826905086636543\n",
            "Avg Block Loss, 5 batches: 0.025854231789708138\n",
            "Avg Challenge Loss, 6 batches: 0.03340532258152962\n",
            "Avg Card Loss, 2 batches: 0.021428097039461136\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 86 of 1000\n",
            "epsilon: 0.6498078359349755\n",
            "gamma: 0.99\n",
            "Number of games in episode 86: 45\n",
            "torch.Size([1635, 12])\n",
            "torch.Size([1635, 13])\n",
            "torch.Size([1635, 13])\n",
            "torch.Size([1635, 13])\n",
            "Avg Action Loss, 6 batches: 0.01743801310658455\n",
            "Avg Block Loss, 5 batches: 0.02116893231868744\n",
            "Avg Challenge Loss, 6 batches: 0.012740209698677063\n",
            "Avg Card Loss, 1 batches: 0.03035266511142254\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 87 of 1000\n",
            "epsilon: 0.6465587967553006\n",
            "gamma: 0.99\n",
            "Number of games in episode 87: 44\n",
            "torch.Size([1631, 12])\n",
            "torch.Size([1631, 13])\n",
            "torch.Size([1631, 13])\n",
            "torch.Size([1631, 13])\n",
            "Avg Action Loss, 6 batches: 0.013838812708854675\n",
            "Avg Block Loss, 5 batches: 0.022500047460198402\n",
            "Avg Challenge Loss, 6 batches: 0.005292700137943029\n",
            "Avg Card Loss, 1 batches: 0.03840509429574013\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 88 of 1000\n",
            "epsilon: 0.6433260027715241\n",
            "gamma: 0.99\n",
            "Number of games in episode 88: 43\n",
            "torch.Size([1602, 12])\n",
            "torch.Size([1602, 13])\n",
            "torch.Size([1602, 13])\n",
            "torch.Size([1602, 13])\n",
            "Avg Action Loss, 6 batches: 0.003993230406194925\n",
            "Avg Block Loss, 5 batches: 0.03464443236589432\n",
            "Avg Challenge Loss, 6 batches: 0.016528543084859848\n",
            "Avg Card Loss, 2 batches: 0.017743386328220367\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 89 of 1000\n",
            "epsilon: 0.6401093727576664\n",
            "gamma: 0.99\n",
            "Number of games in episode 89: 45\n",
            "torch.Size([1628, 12])\n",
            "torch.Size([1628, 13])\n",
            "torch.Size([1628, 13])\n",
            "torch.Size([1628, 13])\n",
            "Avg Action Loss, 6 batches: 0.03549662232398987\n",
            "Avg Block Loss, 5 batches: 0.03134368732571602\n",
            "Avg Challenge Loss, 6 batches: 0.0034789866767823696\n",
            "Avg Card Loss, 1 batches: 0.014584844931960106\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 90 of 1000\n",
            "epsilon: 0.6369088258938781\n",
            "gamma: 0.99\n",
            "Number of games in episode 90: 46\n",
            "torch.Size([1629, 12])\n",
            "torch.Size([1629, 13])\n",
            "torch.Size([1629, 13])\n",
            "torch.Size([1629, 13])\n",
            "Avg Action Loss, 7 batches: 0.024243151769042015\n",
            "Avg Block Loss, 6 batches: 0.016521841287612915\n",
            "Avg Challenge Loss, 7 batches: 0.022068308666348457\n",
            "Avg Card Loss, 2 batches: 0.03231099247932434\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "Most common action for Bot 0: 3 taken 611 times.\n",
            "total game lengths: 3198\n",
            "win rate: 0.35\n",
            "episode 91 of 1000\n",
            "epsilon: 0.6337242817644086\n",
            "gamma: 0.99\n",
            "Number of games in episode 91: 46\n",
            "torch.Size([1611, 12])\n",
            "torch.Size([1611, 13])\n",
            "torch.Size([1611, 13])\n",
            "torch.Size([1611, 13])\n",
            "Avg Action Loss, 6 batches: 0.02357301488518715\n",
            "Avg Block Loss, 5 batches: 0.024170298129320145\n",
            "Avg Challenge Loss, 5 batches: 0.034052763134241104\n",
            "Avg Card Loss, 2 batches: 0.016507381573319435\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 92 of 1000\n",
            "epsilon: 0.6305556603555866\n",
            "gamma: 0.99\n",
            "Number of games in episode 92: 43\n",
            "torch.Size([1631, 12])\n",
            "torch.Size([1631, 13])\n",
            "torch.Size([1631, 13])\n",
            "torch.Size([1631, 13])\n",
            "Avg Action Loss, 6 batches: 0.022683102637529373\n",
            "Avg Block Loss, 5 batches: 0.035926952958106995\n",
            "Avg Challenge Loss, 6 batches: 0.028341053053736687\n",
            "Avg Card Loss, 2 batches: 0.026202397421002388\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 93 of 1000\n",
            "epsilon: 0.6274028820538087\n",
            "gamma: 0.99\n",
            "Number of games in episode 93: 44\n",
            "torch.Size([1634, 12])\n",
            "torch.Size([1634, 13])\n",
            "torch.Size([1634, 13])\n",
            "torch.Size([1634, 13])\n",
            "Avg Action Loss, 6 batches: 0.018871624022722244\n",
            "Avg Block Loss, 6 batches: 0.017982877790927887\n",
            "Avg Challenge Loss, 6 batches: 0.01664992980659008\n",
            "Avg Card Loss, 2 batches: 0.038044460117816925\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 94 of 1000\n",
            "epsilon: 0.6242658676435396\n",
            "gamma: 0.99\n",
            "Number of games in episode 94: 41\n",
            "torch.Size([1620, 12])\n",
            "torch.Size([1620, 13])\n",
            "torch.Size([1620, 13])\n",
            "torch.Size([1620, 13])\n",
            "Avg Action Loss, 6 batches: 0.0294242762029171\n",
            "Avg Block Loss, 6 batches: 0.01780109666287899\n",
            "Avg Challenge Loss, 6 batches: 0.016663208603858948\n",
            "Avg Card Loss, 2 batches: 0.0119509631767869\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 95 of 1000\n",
            "epsilon: 0.6211445383053219\n",
            "gamma: 0.99\n",
            "Number of games in episode 95: 41\n",
            "torch.Size([1610, 12])\n",
            "torch.Size([1610, 13])\n",
            "torch.Size([1610, 13])\n",
            "torch.Size([1610, 13])\n",
            "Avg Action Loss, 6 batches: 0.021513590589165688\n",
            "Avg Block Loss, 5 batches: 0.007438198663294315\n",
            "Avg Challenge Loss, 6 batches: 0.014374460093677044\n",
            "Avg Card Loss, 2 batches: 0.016677742823958397\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 96 of 1000\n",
            "epsilon: 0.6180388156137953\n",
            "gamma: 0.99\n",
            "Number of games in episode 96: 41\n",
            "torch.Size([1640, 12])\n",
            "torch.Size([1640, 13])\n",
            "torch.Size([1640, 13])\n",
            "torch.Size([1640, 13])\n",
            "Avg Action Loss, 6 batches: 0.019099336117506027\n",
            "Avg Block Loss, 5 batches: 0.021133603528141975\n",
            "Avg Challenge Loss, 6 batches: 0.016808712854981422\n",
            "Avg Card Loss, 2 batches: 0.014196970500051975\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 97 of 1000\n",
            "epsilon: 0.6149486215357263\n",
            "gamma: 0.99\n",
            "Number of games in episode 97: 42\n",
            "torch.Size([1622, 12])\n",
            "torch.Size([1622, 13])\n",
            "torch.Size([1622, 13])\n",
            "torch.Size([1622, 13])\n",
            "Avg Action Loss, 6 batches: 0.010673247277736664\n",
            "Avg Block Loss, 5 batches: 0.009173581376671791\n",
            "Avg Challenge Loss, 6 batches: 0.0139657873660326\n",
            "Avg Card Loss, 2 batches: 0.02051733247935772\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 98 of 1000\n",
            "epsilon: 0.6118738784280476\n",
            "gamma: 0.99\n",
            "Number of games in episode 98: 40\n",
            "torch.Size([1628, 12])\n",
            "torch.Size([1628, 13])\n",
            "torch.Size([1628, 13])\n",
            "torch.Size([1628, 13])\n",
            "Avg Action Loss, 6 batches: 0.018944822251796722\n",
            "Avg Block Loss, 5 batches: 0.00823820848017931\n",
            "Avg Challenge Loss, 6 batches: 0.021137220785021782\n",
            "Avg Card Loss, 2 batches: 0.014140338636934757\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 99 of 1000\n",
            "epsilon: 0.6088145090359074\n",
            "gamma: 0.99\n",
            "Number of games in episode 99: 39\n",
            "torch.Size([1610, 12])\n",
            "torch.Size([1610, 13])\n",
            "torch.Size([1610, 13])\n",
            "torch.Size([1610, 13])\n",
            "Avg Action Loss, 6 batches: 0.016628587618470192\n",
            "Avg Block Loss, 5 batches: 0.011532748118042946\n",
            "Avg Challenge Loss, 6 batches: 0.007589991670101881\n",
            "Avg Card Loss, 2 batches: 0.01794213056564331\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 100 of 1000\n",
            "epsilon: 0.6057704364907278\n",
            "gamma: 0.99\n",
            "Number of games in episode 100: 39\n",
            "torch.Size([1607, 12])\n",
            "torch.Size([1607, 13])\n",
            "torch.Size([1607, 13])\n",
            "torch.Size([1607, 13])\n",
            "Avg Action Loss, 5 batches: 0.016399739310145378\n",
            "Avg Block Loss, 5 batches: 0.016832657158374786\n",
            "Avg Challenge Loss, 6 batches: 0.013162246905267239\n",
            "Avg Card Loss, 2 batches: 0.02871885523200035\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "Most common action for Bot 0: 6 taken 973 times.\n",
            "total game lengths: 4373\n",
            "win rate: 0.08\n",
            "episode 101 of 1000\n",
            "epsilon: 0.6027415843082742\n",
            "gamma: 0.99\n",
            "Number of games in episode 101: 39\n",
            "torch.Size([1608, 12])\n",
            "torch.Size([1608, 13])\n",
            "torch.Size([1608, 13])\n",
            "torch.Size([1608, 13])\n",
            "Avg Action Loss, 6 batches: 0.02147682011127472\n",
            "Avg Block Loss, 5 batches: 0.012876242399215698\n",
            "Avg Challenge Loss, 6 batches: 0.02127886191010475\n",
            "Avg Card Loss, 2 batches: 0.023741213604807854\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 102 of 1000\n",
            "epsilon: 0.5997278763867329\n",
            "gamma: 0.99\n",
            "Number of games in episode 102: 39\n",
            "torch.Size([1623, 12])\n",
            "torch.Size([1623, 13])\n",
            "torch.Size([1623, 13])\n",
            "torch.Size([1623, 13])\n",
            "Avg Action Loss, 6 batches: 0.0042357733473181725\n",
            "Avg Block Loss, 5 batches: 0.01132820826023817\n",
            "Avg Challenge Loss, 6 batches: 0.007414414547383785\n",
            "Avg Card Loss, 2 batches: 0.009478379972279072\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 103 of 1000\n",
            "epsilon: 0.5967292370047992\n",
            "gamma: 0.99\n",
            "Number of games in episode 103: 40\n",
            "torch.Size([1632, 12])\n",
            "torch.Size([1632, 13])\n",
            "torch.Size([1632, 13])\n",
            "torch.Size([1632, 13])\n",
            "Avg Action Loss, 6 batches: 0.0027487382758408785\n",
            "Avg Block Loss, 5 batches: 0.020246582105755806\n",
            "Avg Challenge Loss, 6 batches: 0.028279852122068405\n",
            "Avg Card Loss, 2 batches: 0.014303281903266907\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 104 of 1000\n",
            "epsilon: 0.5937455908197752\n",
            "gamma: 0.99\n",
            "Number of games in episode 104: 39\n",
            "torch.Size([1605, 12])\n",
            "torch.Size([1605, 13])\n",
            "torch.Size([1605, 13])\n",
            "torch.Size([1605, 13])\n",
            "Avg Action Loss, 6 batches: 0.012387152761220932\n",
            "Avg Block Loss, 5 batches: 0.018698474392294884\n",
            "Avg Challenge Loss, 6 batches: 0.01669931411743164\n",
            "Avg Card Loss, 2 batches: 0.035304173827171326\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 105 of 1000\n",
            "epsilon: 0.5907768628656763\n",
            "gamma: 0.99\n",
            "Number of games in episode 105: 40\n",
            "torch.Size([1609, 12])\n",
            "torch.Size([1609, 13])\n",
            "torch.Size([1609, 13])\n",
            "torch.Size([1609, 13])\n",
            "Avg Action Loss, 6 batches: 0.011806434020400047\n",
            "Avg Block Loss, 5 batches: 0.0042272210121154785\n",
            "Avg Challenge Loss, 6 batches: 0.009827581234276295\n",
            "Avg Card Loss, 2 batches: 0.00537883210927248\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 106 of 1000\n",
            "epsilon: 0.5878229785513479\n",
            "gamma: 0.99\n",
            "Number of games in episode 106: 41\n",
            "torch.Size([1621, 12])\n",
            "torch.Size([1621, 13])\n",
            "torch.Size([1621, 13])\n",
            "torch.Size([1621, 13])\n",
            "Avg Action Loss, 6 batches: 0.018589120358228683\n",
            "Avg Block Loss, 4 batches: 0.015606994740664959\n",
            "Avg Challenge Loss, 5 batches: 0.003565071150660515\n",
            "Avg Card Loss, 2 batches: 0.01805800013244152\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 107 of 1000\n",
            "epsilon: 0.5848838636585911\n",
            "gamma: 0.99\n",
            "Number of games in episode 107: 43\n",
            "torch.Size([1607, 12])\n",
            "torch.Size([1607, 13])\n",
            "torch.Size([1607, 13])\n",
            "torch.Size([1607, 13])\n",
            "Avg Action Loss, 5 batches: 0.023781202733516693\n",
            "Avg Block Loss, 5 batches: 0.0070693157613277435\n",
            "Avg Challenge Loss, 5 batches: 0.0046256412751972675\n",
            "Avg Card Loss, 1 batches: 0.01605243794620037\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 108 of 1000\n",
            "epsilon: 0.5819594443402982\n",
            "gamma: 0.99\n",
            "Number of games in episode 108: 45\n",
            "torch.Size([1620, 12])\n",
            "torch.Size([1620, 13])\n",
            "torch.Size([1620, 13])\n",
            "torch.Size([1620, 13])\n",
            "Avg Action Loss, 6 batches: 0.03804493695497513\n",
            "Avg Block Loss, 5 batches: 0.017648393288254738\n",
            "Avg Challenge Loss, 6 batches: 0.01817946694791317\n",
            "Avg Card Loss, 1 batches: 0.02089516445994377\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 109 of 1000\n",
            "epsilon: 0.5790496471185967\n",
            "gamma: 0.99\n",
            "Number of games in episode 109: 45\n",
            "torch.Size([1611, 12])\n",
            "torch.Size([1611, 13])\n",
            "torch.Size([1611, 13])\n",
            "torch.Size([1611, 13])\n",
            "Avg Action Loss, 6 batches: 0.023964419960975647\n",
            "Avg Block Loss, 5 batches: 0.008233923465013504\n",
            "Avg Challenge Loss, 5 batches: 0.013169431127607822\n",
            "Avg Card Loss, 2 batches: 0.020480874925851822\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 110 of 1000\n",
            "epsilon: 0.5761543988830038\n",
            "gamma: 0.99\n",
            "Number of games in episode 110: 45\n",
            "torch.Size([1611, 12])\n",
            "torch.Size([1611, 13])\n",
            "torch.Size([1611, 13])\n",
            "torch.Size([1611, 13])\n",
            "Avg Action Loss, 6 batches: 0.01255754753947258\n",
            "Avg Block Loss, 5 batches: 0.01251545175909996\n",
            "Avg Challenge Loss, 6 batches: 0.00821294542402029\n",
            "Avg Card Loss, 1 batches: 0.019551951438188553\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "Most common action for Bot 0: 3 taken 588 times.\n",
            "total game lengths: 3305\n",
            "win rate: 0.43\n",
            "episode 111 of 1000\n",
            "epsilon: 0.5732736268885887\n",
            "gamma: 0.99\n",
            "Number of games in episode 111: 46\n",
            "torch.Size([1617, 12])\n",
            "torch.Size([1617, 13])\n",
            "torch.Size([1617, 13])\n",
            "torch.Size([1617, 13])\n",
            "Avg Action Loss, 6 batches: 0.021591665223240852\n",
            "Avg Block Loss, 5 batches: 0.019283844158053398\n",
            "Avg Challenge Loss, 6 batches: 0.021156124770641327\n",
            "Avg Card Loss, 2 batches: 0.024339089170098305\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 112 of 1000\n",
            "epsilon: 0.5704072587541458\n",
            "gamma: 0.99\n",
            "Number of games in episode 112: 46\n",
            "torch.Size([1637, 12])\n",
            "torch.Size([1637, 13])\n",
            "torch.Size([1637, 13])\n",
            "torch.Size([1637, 13])\n",
            "Avg Action Loss, 6 batches: 0.02160818688571453\n",
            "Avg Block Loss, 5 batches: 0.023780658841133118\n",
            "Avg Challenge Loss, 6 batches: 0.020014697685837746\n",
            "Avg Card Loss, 2 batches: 0.01119065098464489\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 113 of 1000\n",
            "epsilon: 0.567555222460375\n",
            "gamma: 0.99\n",
            "Number of games in episode 113: 45\n",
            "torch.Size([1604, 12])\n",
            "torch.Size([1604, 13])\n",
            "torch.Size([1604, 13])\n",
            "torch.Size([1604, 13])\n",
            "Avg Action Loss, 6 batches: 0.009063205681741238\n",
            "Avg Block Loss, 5 batches: 0.011383410543203354\n",
            "Avg Challenge Loss, 6 batches: 0.011585129424929619\n",
            "Avg Card Loss, 1 batches: 0.011403831653296947\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 114 of 1000\n",
            "epsilon: 0.5647174463480732\n",
            "gamma: 0.99\n",
            "Number of games in episode 114: 43\n",
            "torch.Size([1611, 12])\n",
            "torch.Size([1611, 13])\n",
            "torch.Size([1611, 13])\n",
            "torch.Size([1611, 13])\n",
            "Avg Action Loss, 6 batches: 0.011478958651423454\n",
            "Avg Block Loss, 5 batches: 0.024519935250282288\n",
            "Avg Challenge Loss, 6 batches: 0.0081480136141181\n",
            "Avg Card Loss, 1 batches: 0.04356050118803978\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 115 of 1000\n",
            "epsilon: 0.5618938591163328\n",
            "gamma: 0.99\n",
            "Number of games in episode 115: 40\n",
            "torch.Size([1609, 12])\n",
            "torch.Size([1609, 13])\n",
            "torch.Size([1609, 13])\n",
            "torch.Size([1609, 13])\n",
            "Avg Action Loss, 7 batches: 0.014175153337419033\n",
            "Avg Block Loss, 6 batches: 0.026458371430635452\n",
            "Avg Challenge Loss, 6 batches: 0.024273371323943138\n",
            "Avg Card Loss, 1 batches: 0.0139275211840868\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 116 of 1000\n",
            "epsilon: 0.5590843898207511\n",
            "gamma: 0.99\n",
            "Number of games in episode 116: 42\n",
            "torch.Size([1633, 12])\n",
            "torch.Size([1633, 13])\n",
            "torch.Size([1633, 13])\n",
            "torch.Size([1633, 13])\n",
            "Avg Action Loss, 7 batches: 0.03328913450241089\n",
            "Avg Block Loss, 6 batches: 0.013985156081616879\n",
            "Avg Challenge Loss, 6 batches: 0.033419013023376465\n",
            "Avg Card Loss, 1 batches: 0.01657840609550476\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 117 of 1000\n",
            "epsilon: 0.5562889678716474\n",
            "gamma: 0.99\n",
            "Number of games in episode 117: 40\n",
            "torch.Size([1619, 12])\n",
            "torch.Size([1619, 13])\n",
            "torch.Size([1619, 13])\n",
            "torch.Size([1619, 13])\n",
            "Avg Action Loss, 6 batches: 0.025601783767342567\n",
            "Avg Block Loss, 5 batches: 0.012869440950453281\n",
            "Avg Challenge Loss, 6 batches: 0.012856867164373398\n",
            "Avg Card Loss, 1 batches: 0.0129327904433012\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 118 of 1000\n",
            "epsilon: 0.5535075230322891\n",
            "gamma: 0.99\n",
            "Number of games in episode 118: 39\n",
            "torch.Size([1623, 12])\n",
            "torch.Size([1623, 13])\n",
            "torch.Size([1623, 13])\n",
            "torch.Size([1623, 13])\n",
            "Avg Action Loss, 6 batches: 0.014265946112573147\n",
            "Avg Block Loss, 5 batches: 0.009896043688058853\n",
            "Avg Challenge Loss, 6 batches: 0.023809367790818214\n",
            "Avg Card Loss, 1 batches: 0.0057626389898359776\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 119 of 1000\n",
            "epsilon: 0.5507399854171277\n",
            "gamma: 0.99\n",
            "Number of games in episode 119: 41\n",
            "torch.Size([1635, 12])\n",
            "torch.Size([1635, 13])\n",
            "torch.Size([1635, 13])\n",
            "torch.Size([1635, 13])\n",
            "Avg Action Loss, 6 batches: 0.019787155091762543\n",
            "Avg Block Loss, 5 batches: 0.013847170397639275\n",
            "Avg Challenge Loss, 6 batches: 0.005559023004025221\n",
            "Avg Card Loss, 1 batches: 0.026535581797361374\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 120 of 1000\n",
            "epsilon: 0.547986285490042\n",
            "gamma: 0.99\n",
            "Number of games in episode 120: 42\n",
            "torch.Size([1606, 12])\n",
            "torch.Size([1606, 13])\n",
            "torch.Size([1606, 13])\n",
            "torch.Size([1606, 13])\n",
            "Avg Action Loss, 6 batches: 0.016460832208395004\n",
            "Avg Block Loss, 5 batches: 0.009500587359070778\n",
            "Avg Challenge Loss, 6 batches: 0.011471116915345192\n",
            "Avg Card Loss, 2 batches: 0.03135843947529793\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "Most common action for Bot 0: 0 taken 350 times.\n",
            "total game lengths: 3990\n",
            "win rate: 0.36\n",
            "episode 121 of 1000\n",
            "epsilon: 0.5452463540625918\n",
            "gamma: 0.99\n",
            "Number of games in episode 121: 43\n",
            "torch.Size([1612, 12])\n",
            "torch.Size([1612, 13])\n",
            "torch.Size([1612, 13])\n",
            "torch.Size([1612, 13])\n",
            "Avg Action Loss, 5 batches: 0.00471882987767458\n",
            "Avg Block Loss, 5 batches: 0.018656840547919273\n",
            "Avg Challenge Loss, 6 batches: 0.016876330599188805\n",
            "Avg Card Loss, 2 batches: 0.01745351403951645\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 122 of 1000\n",
            "epsilon: 0.5425201222922789\n",
            "gamma: 0.99\n",
            "Number of games in episode 122: 45\n",
            "torch.Size([1613, 12])\n",
            "torch.Size([1613, 13])\n",
            "torch.Size([1613, 13])\n",
            "torch.Size([1613, 13])\n",
            "Avg Action Loss, 6 batches: 0.024599982425570488\n",
            "Avg Block Loss, 5 batches: 0.02113461121916771\n",
            "Avg Challenge Loss, 6 batches: 0.017989981919527054\n",
            "Avg Card Loss, 2 batches: 0.05018734559416771\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 123 of 1000\n",
            "epsilon: 0.5398075216808175\n",
            "gamma: 0.99\n",
            "Number of games in episode 123: 44\n",
            "torch.Size([1605, 12])\n",
            "torch.Size([1605, 13])\n",
            "torch.Size([1605, 13])\n",
            "torch.Size([1605, 13])\n",
            "Avg Action Loss, 6 batches: 0.02455843985080719\n",
            "Avg Block Loss, 5 batches: 0.024007923901081085\n",
            "Avg Challenge Loss, 6 batches: 0.014261890202760696\n",
            "Avg Card Loss, 2 batches: 0.020542513579130173\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 124 of 1000\n",
            "epsilon: 0.5371084840724134\n",
            "gamma: 0.99\n",
            "Number of games in episode 124: 43\n",
            "torch.Size([1641, 12])\n",
            "torch.Size([1641, 13])\n",
            "torch.Size([1641, 13])\n",
            "torch.Size([1641, 13])\n",
            "Avg Action Loss, 6 batches: 0.00717950938269496\n",
            "Avg Block Loss, 6 batches: 0.020001433789730072\n",
            "Avg Challenge Loss, 7 batches: 0.006441930308938026\n",
            "Avg Card Loss, 2 batches: 0.00952073372900486\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 125 of 1000\n",
            "epsilon: 0.5344229416520513\n",
            "gamma: 0.99\n",
            "Number of games in episode 125: 41\n",
            "torch.Size([1607, 12])\n",
            "torch.Size([1607, 13])\n",
            "torch.Size([1607, 13])\n",
            "torch.Size([1607, 13])\n",
            "Avg Action Loss, 6 batches: 0.008759014308452606\n",
            "Avg Block Loss, 5 batches: 0.010676616802811623\n",
            "Avg Challenge Loss, 6 batches: 0.017906686291098595\n",
            "Avg Card Loss, 2 batches: 0.03971974551677704\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 126 of 1000\n",
            "epsilon: 0.531750826943791\n",
            "gamma: 0.99\n",
            "Number of games in episode 126: 41\n",
            "torch.Size([1608, 12])\n",
            "torch.Size([1608, 13])\n",
            "torch.Size([1608, 13])\n",
            "torch.Size([1608, 13])\n",
            "Avg Action Loss, 6 batches: 0.012010094709694386\n",
            "Avg Block Loss, 5 batches: 0.009410356171429157\n",
            "Avg Challenge Loss, 6 batches: 0.029502687975764275\n",
            "Avg Card Loss, 1 batches: 0.014206800609827042\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 127 of 1000\n",
            "epsilon: 0.5290920728090721\n",
            "gamma: 0.99\n",
            "Number of games in episode 127: 42\n",
            "torch.Size([1630, 12])\n",
            "torch.Size([1630, 13])\n",
            "torch.Size([1630, 13])\n",
            "torch.Size([1630, 13])\n",
            "Avg Action Loss, 6 batches: 0.020442023873329163\n",
            "Avg Block Loss, 6 batches: 0.012020610272884369\n",
            "Avg Challenge Loss, 6 batches: 0.014324402436614037\n",
            "Avg Card Loss, 1 batches: 0.020420016720891\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 128 of 1000\n",
            "epsilon: 0.5264466124450268\n",
            "gamma: 0.99\n",
            "Number of games in episode 128: 40\n",
            "torch.Size([1613, 12])\n",
            "torch.Size([1613, 13])\n",
            "torch.Size([1613, 13])\n",
            "torch.Size([1613, 13])\n",
            "Avg Action Loss, 6 batches: 0.011909248307347298\n",
            "Avg Block Loss, 5 batches: 0.016611965373158455\n",
            "Avg Challenge Loss, 6 batches: 0.007418109569698572\n",
            "Avg Card Loss, 1 batches: 0.026664264500141144\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 129 of 1000\n",
            "epsilon: 0.5238143793828016\n",
            "gamma: 0.99\n",
            "Number of games in episode 129: 41\n",
            "torch.Size([1625, 12])\n",
            "torch.Size([1625, 13])\n",
            "torch.Size([1625, 13])\n",
            "torch.Size([1625, 13])\n",
            "Avg Action Loss, 6 batches: 0.009016817435622215\n",
            "Avg Block Loss, 5 batches: 0.021541347727179527\n",
            "Avg Challenge Loss, 6 batches: 0.007289066910743713\n",
            "Avg Card Loss, 1 batches: 0.016606250777840614\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 130 of 1000\n",
            "epsilon: 0.5211953074858876\n",
            "gamma: 0.99\n",
            "Number of games in episode 130: 40\n",
            "torch.Size([1636, 12])\n",
            "torch.Size([1636, 13])\n",
            "torch.Size([1636, 13])\n",
            "torch.Size([1636, 13])\n",
            "Avg Action Loss, 6 batches: 0.01554797776043415\n",
            "Avg Block Loss, 5 batches: 0.024535231292247772\n",
            "Avg Challenge Loss, 6 batches: 0.01428573951125145\n",
            "Avg Card Loss, 1 batches: 0.027202103286981583\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "Most common action for Bot 0: 0 taken 953 times.\n",
            "total game lengths: 4411\n",
            "win rate: 0.21\n",
            "episode 131 of 1000\n",
            "epsilon: 0.5185893309484582\n",
            "gamma: 0.99\n",
            "Number of games in episode 131: 40\n",
            "torch.Size([1603, 12])\n",
            "torch.Size([1603, 13])\n",
            "torch.Size([1603, 13])\n",
            "torch.Size([1603, 13])\n",
            "Avg Action Loss, 6 batches: 0.006239746231585741\n",
            "Avg Block Loss, 5 batches: 0.009138086810708046\n",
            "Avg Challenge Loss, 5 batches: 0.005435348488390446\n",
            "Avg Card Loss, 1 batches: 0.02230660617351532\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 132 of 1000\n",
            "epsilon: 0.5159963842937159\n",
            "gamma: 0.99\n",
            "Number of games in episode 132: 41\n",
            "torch.Size([1611, 12])\n",
            "torch.Size([1611, 13])\n",
            "torch.Size([1611, 13])\n",
            "torch.Size([1611, 13])\n",
            "Avg Action Loss, 6 batches: 0.0027499604038894176\n",
            "Avg Block Loss, 5 batches: 0.004107345826923847\n",
            "Avg Challenge Loss, 6 batches: 0.032983142882585526\n",
            "Avg Card Loss, 1 batches: 0.01690605655312538\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 133 of 1000\n",
            "epsilon: 0.5134164023722473\n",
            "gamma: 0.99\n",
            "Number of games in episode 133: 40\n",
            "torch.Size([1617, 12])\n",
            "torch.Size([1617, 13])\n",
            "torch.Size([1617, 13])\n",
            "torch.Size([1617, 13])\n",
            "Avg Action Loss, 6 batches: 0.015395015478134155\n",
            "Avg Block Loss, 5 batches: 0.014856422320008278\n",
            "Avg Challenge Loss, 6 batches: 0.009810508228838444\n",
            "Avg Card Loss, 1 batches: 0.03504854068160057\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 134 of 1000\n",
            "epsilon: 0.510849320360386\n",
            "gamma: 0.99\n",
            "Number of games in episode 134: 39\n",
            "torch.Size([1606, 12])\n",
            "torch.Size([1606, 13])\n",
            "torch.Size([1606, 13])\n",
            "torch.Size([1606, 13])\n",
            "Avg Action Loss, 6 batches: 0.02415452152490616\n",
            "Avg Block Loss, 5 batches: 0.033021993935108185\n",
            "Avg Challenge Loss, 6 batches: 0.01616721600294113\n",
            "Avg Card Loss, 1 batches: 0.007100853603333235\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 135 of 1000\n",
            "epsilon: 0.5082950737585841\n",
            "gamma: 0.99\n",
            "Number of games in episode 135: 39\n",
            "torch.Size([1637, 12])\n",
            "torch.Size([1637, 13])\n",
            "torch.Size([1637, 13])\n",
            "torch.Size([1637, 13])\n",
            "Avg Action Loss, 6 batches: 0.006843403913080692\n",
            "Avg Block Loss, 6 batches: 0.010663246735930443\n",
            "Avg Challenge Loss, 6 batches: 0.01729828305542469\n",
            "Avg Card Loss, 1 batches: 0.012345850467681885\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 136 of 1000\n",
            "epsilon: 0.5057535983897912\n",
            "gamma: 0.99\n",
            "Number of games in episode 136: 39\n",
            "torch.Size([1628, 12])\n",
            "torch.Size([1628, 13])\n",
            "torch.Size([1628, 13])\n",
            "torch.Size([1628, 13])\n",
            "Avg Action Loss, 6 batches: 0.010784263722598553\n",
            "Avg Block Loss, 5 batches: 0.007798942271620035\n",
            "Avg Challenge Loss, 6 batches: 0.025635244324803352\n",
            "Avg Card Loss, 1 batches: 0.017609545961022377\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 137 of 1000\n",
            "epsilon: 0.5032248303978422\n",
            "gamma: 0.99\n",
            "Number of games in episode 137: 38\n",
            "torch.Size([1608, 12])\n",
            "torch.Size([1608, 13])\n",
            "torch.Size([1608, 13])\n",
            "torch.Size([1608, 13])\n",
            "Avg Action Loss, 7 batches: 0.03254329785704613\n",
            "Avg Block Loss, 6 batches: 0.011662127450108528\n",
            "Avg Challenge Loss, 7 batches: 0.010441366583108902\n",
            "Avg Card Loss, 1 batches: 0.015500701032578945\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 138 of 1000\n",
            "epsilon: 0.500708706245853\n",
            "gamma: 0.99\n",
            "Number of games in episode 138: 38\n",
            "torch.Size([1606, 12])\n",
            "torch.Size([1606, 13])\n",
            "torch.Size([1606, 13])\n",
            "torch.Size([1606, 13])\n",
            "Avg Action Loss, 6 batches: 0.039953287690877914\n",
            "Avg Block Loss, 5 batches: 0.010661184787750244\n",
            "Avg Challenge Loss, 5 batches: 0.013811981305480003\n",
            "Avg Card Loss, 1 batches: 0.01677211932837963\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 139 of 1000\n",
            "epsilon: 0.4982051627146237\n",
            "gamma: 0.99\n",
            "Number of games in episode 139: 39\n",
            "torch.Size([1636, 12])\n",
            "torch.Size([1636, 13])\n",
            "torch.Size([1636, 13])\n",
            "torch.Size([1636, 13])\n",
            "Avg Action Loss, 6 batches: 0.013982448726892471\n",
            "Avg Block Loss, 5 batches: 0.006213714834302664\n",
            "Avg Challenge Loss, 6 batches: 0.01365971565246582\n",
            "Avg Card Loss, 1 batches: 0.008837563917040825\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 140 of 1000\n",
            "epsilon: 0.49571413690105054\n",
            "gamma: 0.99\n",
            "Number of games in episode 140: 39\n",
            "torch.Size([1608, 12])\n",
            "torch.Size([1608, 13])\n",
            "torch.Size([1608, 13])\n",
            "torch.Size([1608, 13])\n",
            "Avg Action Loss, 6 batches: 0.01883433759212494\n",
            "Avg Block Loss, 5 batches: 0.005407665856182575\n",
            "Avg Challenge Loss, 5 batches: 0.01640988141298294\n",
            "Avg Card Loss, 1 batches: 0.03638990968465805\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "Most common action for Bot 0: 0 taken 1084 times.\n",
            "total game lengths: 4729\n",
            "win rate: 0.22\n",
            "episode 141 of 1000\n",
            "epsilon: 0.4932355662165453\n",
            "gamma: 0.99\n",
            "Number of games in episode 141: 37\n",
            "torch.Size([1611, 12])\n",
            "torch.Size([1611, 13])\n",
            "torch.Size([1611, 13])\n",
            "torch.Size([1611, 13])\n",
            "Avg Action Loss, 6 batches: 0.019812941551208496\n",
            "Avg Block Loss, 6 batches: 0.021169602870941162\n",
            "Avg Challenge Loss, 6 batches: 0.012895782478153706\n",
            "Avg Card Loss, 1 batches: 0.019542984664440155\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 142 of 1000\n",
            "epsilon: 0.4907693883854626\n",
            "gamma: 0.99\n",
            "Number of games in episode 142: 37\n",
            "torch.Size([1628, 12])\n",
            "torch.Size([1628, 13])\n",
            "torch.Size([1628, 13])\n",
            "torch.Size([1628, 13])\n",
            "Avg Action Loss, 6 batches: 0.01230788417160511\n",
            "Avg Block Loss, 5 batches: 0.019587695598602295\n",
            "Avg Challenge Loss, 6 batches: 0.009922252967953682\n",
            "Avg Card Loss, 1 batches: 0.010909786447882652\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 143 of 1000\n",
            "epsilon: 0.4883155414435353\n",
            "gamma: 0.99\n",
            "Number of games in episode 143: 37\n",
            "torch.Size([1604, 12])\n",
            "torch.Size([1604, 13])\n",
            "torch.Size([1604, 13])\n",
            "torch.Size([1604, 13])\n",
            "Avg Action Loss, 6 batches: 0.00468603428453207\n",
            "Avg Block Loss, 5 batches: 0.01775205135345459\n",
            "Avg Challenge Loss, 6 batches: 0.023969607427716255\n",
            "Avg Card Loss, 1 batches: 0.02107219398021698\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 144 of 1000\n",
            "epsilon: 0.4858739637363176\n",
            "gamma: 0.99\n",
            "Number of games in episode 144: 39\n",
            "torch.Size([1616, 12])\n",
            "torch.Size([1616, 13])\n",
            "torch.Size([1616, 13])\n",
            "torch.Size([1616, 13])\n",
            "Avg Action Loss, 6 batches: 0.007592285983264446\n",
            "Avg Block Loss, 5 batches: 0.0060663423500955105\n",
            "Avg Challenge Loss, 6 batches: 0.01376389805227518\n",
            "Avg Card Loss, 1 batches: 0.01634189300239086\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 145 of 1000\n",
            "epsilon: 0.483444593917636\n",
            "gamma: 0.99\n",
            "Number of games in episode 145: 41\n",
            "torch.Size([1632, 12])\n",
            "torch.Size([1632, 13])\n",
            "torch.Size([1632, 13])\n",
            "torch.Size([1632, 13])\n",
            "Avg Action Loss, 5 batches: 0.0226436797529459\n",
            "Avg Block Loss, 5 batches: 0.004503435920923948\n",
            "Avg Challenge Loss, 5 batches: 0.011372714303433895\n",
            "Avg Card Loss, 2 batches: 0.012466153129935265\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 146 of 1000\n",
            "epsilon: 0.4810273709480478\n",
            "gamma: 0.99\n",
            "Number of games in episode 146: 42\n",
            "torch.Size([1602, 12])\n",
            "torch.Size([1602, 13])\n",
            "torch.Size([1602, 13])\n",
            "torch.Size([1602, 13])\n",
            "Avg Action Loss, 6 batches: 0.015392611734569073\n",
            "Avg Block Loss, 5 batches: 0.0026636235415935516\n",
            "Avg Challenge Loss, 6 batches: 0.025514839217066765\n",
            "Avg Card Loss, 2 batches: 0.012517482973635197\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 147 of 1000\n",
            "epsilon: 0.47862223409330756\n",
            "gamma: 0.99\n",
            "Number of games in episode 147: 45\n",
            "torch.Size([1612, 12])\n",
            "torch.Size([1612, 13])\n",
            "torch.Size([1612, 13])\n",
            "torch.Size([1612, 13])\n",
            "Avg Action Loss, 6 batches: 0.019902177155017853\n",
            "Avg Block Loss, 5 batches: 0.01484631933271885\n",
            "Avg Challenge Loss, 6 batches: 0.007720303256064653\n",
            "Avg Card Loss, 2 batches: 0.015062009915709496\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 148 of 1000\n",
            "epsilon: 0.47622912292284103\n",
            "gamma: 0.99\n",
            "Number of games in episode 148: 45\n",
            "torch.Size([1632, 12])\n",
            "torch.Size([1632, 13])\n",
            "torch.Size([1632, 13])\n",
            "torch.Size([1632, 13])\n",
            "Avg Action Loss, 6 batches: 0.002121344907209277\n",
            "Avg Block Loss, 6 batches: 0.009553780779242516\n",
            "Avg Challenge Loss, 6 batches: 0.011628233827650547\n",
            "Avg Card Loss, 2 batches: 0.01444510743021965\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 149 of 1000\n",
            "epsilon: 0.4738479773082268\n",
            "gamma: 0.99\n",
            "Number of games in episode 149: 46\n",
            "torch.Size([1630, 12])\n",
            "torch.Size([1630, 13])\n",
            "torch.Size([1630, 13])\n",
            "torch.Size([1630, 13])\n",
            "Avg Action Loss, 6 batches: 0.02131909690797329\n",
            "Avg Block Loss, 5 batches: 0.026527758687734604\n",
            "Avg Challenge Loss, 6 batches: 0.019696734845638275\n",
            "Avg Card Loss, 1 batches: 0.035534828901290894\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 150 of 1000\n",
            "epsilon: 0.47147873742168567\n",
            "gamma: 0.99\n",
            "Number of games in episode 150: 45\n",
            "torch.Size([1602, 12])\n",
            "torch.Size([1602, 13])\n",
            "torch.Size([1602, 13])\n",
            "torch.Size([1602, 13])\n",
            "Avg Action Loss, 6 batches: 0.012561412528157234\n",
            "Avg Block Loss, 5 batches: 0.0269453227519989\n",
            "Avg Challenge Loss, 5 batches: 0.008010338991880417\n",
            "Avg Card Loss, 2 batches: 0.017916399985551834\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "Most common action for Bot 0: 3 taken 529 times.\n",
            "total game lengths: 3456\n",
            "win rate: 0.35\n",
            "episode 151 of 1000\n",
            "epsilon: 0.46912134373457726\n",
            "gamma: 0.99\n",
            "Number of games in episode 151: 45\n",
            "torch.Size([1609, 12])\n",
            "torch.Size([1609, 13])\n",
            "torch.Size([1609, 13])\n",
            "torch.Size([1609, 13])\n",
            "Avg Action Loss, 5 batches: 0.0211655106395483\n",
            "Avg Block Loss, 5 batches: 0.029548579826951027\n",
            "Avg Challenge Loss, 5 batches: 0.02277606911957264\n",
            "Avg Card Loss, 2 batches: 0.013353172689676285\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 152 of 1000\n",
            "epsilon: 0.46677573701590436\n",
            "gamma: 0.99\n",
            "Number of games in episode 152: 43\n",
            "torch.Size([1619, 12])\n",
            "torch.Size([1619, 13])\n",
            "torch.Size([1619, 13])\n",
            "torch.Size([1619, 13])\n",
            "Avg Action Loss, 6 batches: 0.020391184836626053\n",
            "Avg Block Loss, 5 batches: 0.03251798450946808\n",
            "Avg Challenge Loss, 5 batches: 0.010793587192893028\n",
            "Avg Card Loss, 2 batches: 0.023429427295923233\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 153 of 1000\n",
            "epsilon: 0.46444185833082485\n",
            "gamma: 0.99\n",
            "Number of games in episode 153: 44\n",
            "torch.Size([1620, 12])\n",
            "torch.Size([1620, 13])\n",
            "torch.Size([1620, 13])\n",
            "torch.Size([1620, 13])\n",
            "Avg Action Loss, 6 batches: 0.026121241971850395\n",
            "Avg Block Loss, 5 batches: 0.0186813585460186\n",
            "Avg Challenge Loss, 5 batches: 0.022918613627552986\n",
            "Avg Card Loss, 2 batches: 0.011099076829850674\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 154 of 1000\n",
            "epsilon: 0.46211964903917074\n",
            "gamma: 0.99\n",
            "Number of games in episode 154: 43\n",
            "torch.Size([1628, 12])\n",
            "torch.Size([1628, 13])\n",
            "torch.Size([1628, 13])\n",
            "torch.Size([1628, 13])\n",
            "Avg Action Loss, 6 batches: 0.019694380462169647\n",
            "Avg Block Loss, 5 batches: 0.031674280762672424\n",
            "Avg Challenge Loss, 5 batches: 0.008123097009956837\n",
            "Avg Card Loss, 2 batches: 0.023831406608223915\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 155 of 1000\n",
            "epsilon: 0.4598090507939749\n",
            "gamma: 0.99\n",
            "Number of games in episode 155: 44\n",
            "torch.Size([1616, 12])\n",
            "torch.Size([1616, 13])\n",
            "torch.Size([1616, 13])\n",
            "torch.Size([1616, 13])\n",
            "Avg Action Loss, 6 batches: 0.0034095835871994495\n",
            "Avg Block Loss, 5 batches: 0.005824368447065353\n",
            "Avg Challenge Loss, 6 batches: 0.012039033696055412\n",
            "Avg Card Loss, 2 batches: 0.02177252434194088\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 156 of 1000\n",
            "epsilon: 0.457510005540005\n",
            "gamma: 0.99\n",
            "Number of games in episode 156: 43\n",
            "torch.Size([1614, 12])\n",
            "torch.Size([1614, 13])\n",
            "torch.Size([1614, 13])\n",
            "torch.Size([1614, 13])\n",
            "Avg Action Loss, 6 batches: 0.020339235663414\n",
            "Avg Block Loss, 5 batches: 0.023772522807121277\n",
            "Avg Challenge Loss, 6 batches: 0.013918787240982056\n",
            "Avg Card Loss, 2 batches: 0.017728447914123535\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 157 of 1000\n",
            "epsilon: 0.45522245551230495\n",
            "gamma: 0.99\n",
            "Number of games in episode 157: 43\n",
            "torch.Size([1610, 12])\n",
            "torch.Size([1610, 13])\n",
            "torch.Size([1610, 13])\n",
            "torch.Size([1610, 13])\n",
            "Avg Action Loss, 6 batches: 0.008696148172020912\n",
            "Avg Block Loss, 5 batches: 0.020227015018463135\n",
            "Avg Challenge Loss, 6 batches: 0.007710366044193506\n",
            "Avg Card Loss, 2 batches: 0.001150475349277258\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 158 of 1000\n",
            "epsilon: 0.4529463432347434\n",
            "gamma: 0.99\n",
            "Number of games in episode 158: 42\n",
            "torch.Size([1639, 12])\n",
            "torch.Size([1639, 13])\n",
            "torch.Size([1639, 13])\n",
            "torch.Size([1639, 13])\n",
            "Avg Action Loss, 5 batches: 0.015394976362586021\n",
            "Avg Block Loss, 5 batches: 0.0029848809354007244\n",
            "Avg Challenge Loss, 5 batches: 0.012511802837252617\n",
            "Avg Card Loss, 2 batches: 0.007853307761251926\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 159 of 1000\n",
            "epsilon: 0.4506816115185697\n",
            "gamma: 0.99\n",
            "Number of games in episode 159: 40\n",
            "torch.Size([1634, 12])\n",
            "torch.Size([1634, 13])\n",
            "torch.Size([1634, 13])\n",
            "torch.Size([1634, 13])\n",
            "Avg Action Loss, 6 batches: 0.020693141967058182\n",
            "Avg Block Loss, 5 batches: 0.005761147476732731\n",
            "Avg Challenge Loss, 6 batches: 0.01306505873799324\n",
            "Avg Card Loss, 2 batches: 0.019748443737626076\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 160 of 1000\n",
            "epsilon: 0.4484282034609769\n",
            "gamma: 0.99\n",
            "Number of games in episode 160: 40\n",
            "torch.Size([1619, 12])\n",
            "torch.Size([1619, 13])\n",
            "torch.Size([1619, 13])\n",
            "torch.Size([1619, 13])\n",
            "Avg Action Loss, 5 batches: 0.01611662656068802\n",
            "Avg Block Loss, 5 batches: 0.004429345019161701\n",
            "Avg Challenge Loss, 5 batches: 0.01888757199048996\n",
            "Avg Card Loss, 2 batches: 0.02619890868663788\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "Most common action for Bot 0: 6 taken 1081 times.\n",
            "total game lengths: 4686\n",
            "win rate: 0.08\n",
            "episode 161 of 1000\n",
            "epsilon: 0.446186062443672\n",
            "gamma: 0.99\n",
            "Number of games in episode 161: 38\n",
            "torch.Size([1638, 12])\n",
            "torch.Size([1638, 13])\n",
            "torch.Size([1638, 13])\n",
            "torch.Size([1638, 13])\n",
            "Avg Action Loss, 6 batches: 0.011866660788655281\n",
            "Avg Block Loss, 5 batches: 0.004203954711556435\n",
            "Avg Challenge Loss, 6 batches: 0.006101392209529877\n",
            "Avg Card Loss, 2 batches: 0.0045547629706561565\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 162 of 1000\n",
            "epsilon: 0.4439551321314536\n",
            "gamma: 0.99\n",
            "Number of games in episode 162: 36\n",
            "torch.Size([1637, 12])\n",
            "torch.Size([1637, 13])\n",
            "torch.Size([1637, 13])\n",
            "torch.Size([1637, 13])\n",
            "Avg Action Loss, 6 batches: 0.00202269502915442\n",
            "Avg Block Loss, 5 batches: 0.01785029098391533\n",
            "Avg Challenge Loss, 6 batches: 0.012760994024574757\n",
            "Avg Card Loss, 2 batches: 0.007503852713853121\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 163 of 1000\n",
            "epsilon: 0.4417353564707963\n",
            "gamma: 0.99\n",
            "Number of games in episode 163: 37\n",
            "torch.Size([1628, 12])\n",
            "torch.Size([1628, 13])\n",
            "torch.Size([1628, 13])\n",
            "torch.Size([1628, 13])\n",
            "Avg Action Loss, 6 batches: 0.010612844489514828\n",
            "Avg Block Loss, 5 batches: 0.03240183740854263\n",
            "Avg Challenge Loss, 6 batches: 0.02713080309331417\n",
            "Avg Card Loss, 3 batches: 0.03251740336418152\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 164 of 1000\n",
            "epsilon: 0.43952667968844233\n",
            "gamma: 0.99\n",
            "Number of games in episode 164: 36\n",
            "torch.Size([1629, 12])\n",
            "torch.Size([1629, 13])\n",
            "torch.Size([1629, 13])\n",
            "torch.Size([1629, 13])\n",
            "Avg Action Loss, 6 batches: 0.003954898566007614\n",
            "Avg Block Loss, 5 batches: 0.007386915385723114\n",
            "Avg Challenge Loss, 5 batches: 0.018528979271650314\n",
            "Avg Card Loss, 3 batches: 0.004391660913825035\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 165 of 1000\n",
            "epsilon: 0.43732904629000013\n",
            "gamma: 0.99\n",
            "Number of games in episode 165: 34\n",
            "torch.Size([1615, 12])\n",
            "torch.Size([1615, 13])\n",
            "torch.Size([1615, 13])\n",
            "torch.Size([1615, 13])\n",
            "Avg Action Loss, 6 batches: 0.019840532913804054\n",
            "Avg Block Loss, 5 batches: 0.009058634750545025\n",
            "Avg Challenge Loss, 6 batches: 0.008544500917196274\n",
            "Avg Card Loss, 3 batches: 0.00541288498789072\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 166 of 1000\n",
            "epsilon: 0.4351424010585501\n",
            "gamma: 0.99\n",
            "Number of games in episode 166: 37\n",
            "torch.Size([1634, 12])\n",
            "torch.Size([1634, 13])\n",
            "torch.Size([1634, 13])\n",
            "torch.Size([1634, 13])\n",
            "Avg Action Loss, 5 batches: 0.004031775519251823\n",
            "Avg Block Loss, 5 batches: 0.005746407900005579\n",
            "Avg Challenge Loss, 5 batches: 0.016378823667764664\n",
            "Avg Card Loss, 3 batches: 0.001403902773745358\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 167 of 1000\n",
            "epsilon: 0.43296668905325736\n",
            "gamma: 0.99\n",
            "Number of games in episode 167: 39\n",
            "torch.Size([1626, 12])\n",
            "torch.Size([1626, 13])\n",
            "torch.Size([1626, 13])\n",
            "torch.Size([1626, 13])\n",
            "Avg Action Loss, 6 batches: 0.01663237065076828\n",
            "Avg Block Loss, 5 batches: 0.02210996486246586\n",
            "Avg Challenge Loss, 5 batches: 0.03456128016114235\n",
            "Avg Card Loss, 2 batches: 0.00797068327665329\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 168 of 1000\n",
            "epsilon: 0.43080185560799106\n",
            "gamma: 0.99\n",
            "Number of games in episode 168: 39\n",
            "torch.Size([1629, 12])\n",
            "torch.Size([1629, 13])\n",
            "torch.Size([1629, 13])\n",
            "torch.Size([1629, 13])\n",
            "Avg Action Loss, 6 batches: 0.011503217741847038\n",
            "Avg Block Loss, 5 batches: 0.0053970771841704845\n",
            "Avg Challenge Loss, 6 batches: 0.01260707713663578\n",
            "Avg Card Loss, 2 batches: 0.004159215837717056\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 169 of 1000\n",
            "epsilon: 0.4286478463299511\n",
            "gamma: 0.99\n",
            "Number of games in episode 169: 40\n",
            "torch.Size([1608, 12])\n",
            "torch.Size([1608, 13])\n",
            "torch.Size([1608, 13])\n",
            "torch.Size([1608, 13])\n",
            "Avg Action Loss, 5 batches: 0.016849622130393982\n",
            "Avg Block Loss, 5 batches: 0.010845597833395004\n",
            "Avg Challenge Loss, 5 batches: 0.01527953241020441\n",
            "Avg Card Loss, 2 batches: 0.021940527483820915\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 170 of 1000\n",
            "epsilon: 0.42650460709830135\n",
            "gamma: 0.99\n",
            "Number of games in episode 170: 38\n",
            "torch.Size([1634, 12])\n",
            "torch.Size([1634, 13])\n",
            "torch.Size([1634, 13])\n",
            "torch.Size([1634, 13])\n",
            "Avg Action Loss, 6 batches: 0.01737034134566784\n",
            "Avg Block Loss, 5 batches: 0.01208363939076662\n",
            "Avg Challenge Loss, 6 batches: 0.01545853354036808\n",
            "Avg Card Loss, 2 batches: 0.006191037595272064\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "Most common action for Bot 0: 6 taken 625 times.\n",
            "total game lengths: 4914\n",
            "win rate: 0.15\n",
            "episode 171 of 1000\n",
            "epsilon: 0.42437208406280985\n",
            "gamma: 0.99\n",
            "Number of games in episode 171: 37\n",
            "torch.Size([1631, 12])\n",
            "torch.Size([1631, 13])\n",
            "torch.Size([1631, 13])\n",
            "torch.Size([1631, 13])\n",
            "Avg Action Loss, 6 batches: 0.01360513549298048\n",
            "Avg Block Loss, 6 batches: 0.013347706757485867\n",
            "Avg Challenge Loss, 6 batches: 0.011104254983365536\n",
            "Avg Card Loss, 2 batches: 0.021044524386525154\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 172 of 1000\n",
            "epsilon: 0.4222502236424958\n",
            "gamma: 0.99\n",
            "Number of games in episode 172: 36\n",
            "torch.Size([1628, 12])\n",
            "torch.Size([1628, 13])\n",
            "torch.Size([1628, 13])\n",
            "torch.Size([1628, 13])\n",
            "Avg Action Loss, 6 batches: 0.005855192430317402\n",
            "Avg Block Loss, 5 batches: 0.004324490204453468\n",
            "Avg Challenge Loss, 6 batches: 0.018417250365018845\n",
            "Avg Card Loss, 2 batches: 0.015257132239639759\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 173 of 1000\n",
            "epsilon: 0.42013897252428334\n",
            "gamma: 0.99\n",
            "Number of games in episode 173: 37\n",
            "torch.Size([1634, 12])\n",
            "torch.Size([1634, 13])\n",
            "torch.Size([1634, 13])\n",
            "torch.Size([1634, 13])\n",
            "Avg Action Loss, 6 batches: 0.01929926872253418\n",
            "Avg Block Loss, 5 batches: 0.0051818531937897205\n",
            "Avg Challenge Loss, 5 batches: 0.010596973821520805\n",
            "Avg Card Loss, 2 batches: 0.01824512518942356\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 174 of 1000\n",
            "epsilon: 0.4180382776616619\n",
            "gamma: 0.99\n",
            "Number of games in episode 174: 40\n",
            "torch.Size([1620, 12])\n",
            "torch.Size([1620, 13])\n",
            "torch.Size([1620, 13])\n",
            "torch.Size([1620, 13])\n",
            "Avg Action Loss, 6 batches: 0.006380489561706781\n",
            "Avg Block Loss, 5 batches: 0.013250085525214672\n",
            "Avg Challenge Loss, 6 batches: 0.015336927957832813\n",
            "Avg Card Loss, 2 batches: 0.02426287531852722\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 175 of 1000\n",
            "epsilon: 0.4159480862733536\n",
            "gamma: 0.99\n",
            "Number of games in episode 175: 41\n",
            "torch.Size([1607, 12])\n",
            "torch.Size([1607, 13])\n",
            "torch.Size([1607, 13])\n",
            "torch.Size([1607, 13])\n",
            "Avg Action Loss, 6 batches: 0.015006352216005325\n",
            "Avg Block Loss, 5 batches: 0.01033423375338316\n",
            "Avg Challenge Loss, 6 batches: 0.01851522922515869\n",
            "Avg Card Loss, 2 batches: 0.010336438193917274\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 176 of 1000\n",
            "epsilon: 0.41386834584198684\n",
            "gamma: 0.99\n",
            "Number of games in episode 176: 43\n",
            "torch.Size([1601, 12])\n",
            "torch.Size([1601, 13])\n",
            "torch.Size([1601, 13])\n",
            "torch.Size([1601, 13])\n",
            "Avg Action Loss, 6 batches: 0.017668643966317177\n",
            "Avg Block Loss, 5 batches: 0.01833881251513958\n",
            "Avg Challenge Loss, 6 batches: 0.02445572055876255\n",
            "Avg Card Loss, 2 batches: 0.016352884471416473\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 177 of 1000\n",
            "epsilon: 0.4117990041127769\n",
            "gamma: 0.99\n",
            "Number of games in episode 177: 45\n",
            "torch.Size([1633, 12])\n",
            "torch.Size([1633, 13])\n",
            "torch.Size([1633, 13])\n",
            "torch.Size([1633, 13])\n",
            "Avg Action Loss, 6 batches: 0.024599796161055565\n",
            "Avg Block Loss, 5 batches: 0.013838868588209152\n",
            "Avg Challenge Loss, 6 batches: 0.014796575531363487\n",
            "Avg Card Loss, 2 batches: 0.016055885702371597\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 178 of 1000\n",
            "epsilon: 0.40974000909221303\n",
            "gamma: 0.99\n",
            "Number of games in episode 178: 40\n",
            "torch.Size([1618, 12])\n",
            "torch.Size([1618, 13])\n",
            "torch.Size([1618, 13])\n",
            "torch.Size([1618, 13])\n",
            "Avg Action Loss, 7 batches: 0.016276555135846138\n",
            "Avg Block Loss, 6 batches: 0.004794313572347164\n",
            "Avg Challenge Loss, 7 batches: 0.013455536216497421\n",
            "Avg Card Loss, 1 batches: 0.03111383505165577\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 179 of 1000\n",
            "epsilon: 0.40769130904675194\n",
            "gamma: 0.99\n",
            "Number of games in episode 179: 39\n",
            "torch.Size([1623, 12])\n",
            "torch.Size([1623, 13])\n",
            "torch.Size([1623, 13])\n",
            "torch.Size([1623, 13])\n",
            "Avg Action Loss, 6 batches: 0.01684190146625042\n",
            "Avg Block Loss, 5 batches: 0.02165011502802372\n",
            "Avg Challenge Loss, 7 batches: 0.01561477966606617\n",
            "Avg Card Loss, 1 batches: 0.01263305265456438\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 180 of 1000\n",
            "epsilon: 0.40565285250151817\n",
            "gamma: 0.99\n",
            "Number of games in episode 180: 40\n",
            "torch.Size([1608, 12])\n",
            "torch.Size([1608, 13])\n",
            "torch.Size([1608, 13])\n",
            "torch.Size([1608, 13])\n",
            "Avg Action Loss, 6 batches: 0.0044088843278586864\n",
            "Avg Block Loss, 6 batches: 0.022707924246788025\n",
            "Avg Challenge Loss, 8 batches: 0.02027386799454689\n",
            "Avg Card Loss, 1 batches: 0.01795334555208683\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "Most common action for Bot 0: 2 taken 692 times.\n",
            "total game lengths: 4172\n",
            "win rate: 0.21\n",
            "episode 181 of 1000\n",
            "epsilon: 0.4036245882390106\n",
            "gamma: 0.99\n",
            "Number of games in episode 181: 41\n",
            "torch.Size([1622, 12])\n",
            "torch.Size([1622, 13])\n",
            "torch.Size([1622, 13])\n",
            "torch.Size([1622, 13])\n",
            "Avg Action Loss, 6 batches: 0.015648169443011284\n",
            "Avg Block Loss, 5 batches: 0.007223956286907196\n",
            "Avg Challenge Loss, 6 batches: 0.013095778413116932\n",
            "Avg Card Loss, 1 batches: 0.0250853281468153\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 182 of 1000\n",
            "epsilon: 0.4016064652978155\n",
            "gamma: 0.99\n",
            "Number of games in episode 182: 41\n",
            "torch.Size([1614, 12])\n",
            "torch.Size([1614, 13])\n",
            "torch.Size([1614, 13])\n",
            "torch.Size([1614, 13])\n",
            "Avg Action Loss, 5 batches: 0.01369145791977644\n",
            "Avg Block Loss, 5 batches: 0.004601403139531612\n",
            "Avg Challenge Loss, 6 batches: 0.021590884774923325\n",
            "Avg Card Loss, 1 batches: 0.01697562076151371\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 183 of 1000\n",
            "epsilon: 0.3995984329713264\n",
            "gamma: 0.99\n",
            "Number of games in episode 183: 42\n",
            "torch.Size([1619, 12])\n",
            "torch.Size([1619, 13])\n",
            "torch.Size([1619, 13])\n",
            "torch.Size([1619, 13])\n",
            "Avg Action Loss, 6 batches: 0.017169710248708725\n",
            "Avg Block Loss, 5 batches: 0.01957504078745842\n",
            "Avg Challenge Loss, 7 batches: 0.013105218298733234\n",
            "Avg Card Loss, 1 batches: 0.02057182788848877\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 184 of 1000\n",
            "epsilon: 0.3976004408064698\n",
            "gamma: 0.99\n",
            "Number of games in episode 184: 39\n",
            "torch.Size([1607, 12])\n",
            "torch.Size([1607, 13])\n",
            "torch.Size([1607, 13])\n",
            "torch.Size([1607, 13])\n",
            "Avg Action Loss, 6 batches: 0.006693330127745867\n",
            "Avg Block Loss, 5 batches: 0.017840133979916573\n",
            "Avg Challenge Loss, 7 batches: 0.0292157344520092\n",
            "Avg Card Loss, 1 batches: 0.019238803535699844\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 185 of 1000\n",
            "epsilon: 0.39561243860243744\n",
            "gamma: 0.99\n",
            "Number of games in episode 185: 40\n",
            "torch.Size([1624, 12])\n",
            "torch.Size([1624, 13])\n",
            "torch.Size([1624, 13])\n",
            "torch.Size([1624, 13])\n",
            "Avg Action Loss, 6 batches: 0.006416936405003071\n",
            "Avg Block Loss, 5 batches: 0.0120486319065094\n",
            "Avg Challenge Loss, 7 batches: 0.01947280950844288\n",
            "Avg Card Loss, 1 batches: 0.020943380892276764\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 186 of 1000\n",
            "epsilon: 0.3936343764094253\n",
            "gamma: 0.99\n",
            "Number of games in episode 186: 41\n",
            "torch.Size([1633, 12])\n",
            "torch.Size([1633, 13])\n",
            "torch.Size([1633, 13])\n",
            "torch.Size([1633, 13])\n",
            "Avg Action Loss, 6 batches: 0.006905427202582359\n",
            "Avg Block Loss, 5 batches: 0.010451946407556534\n",
            "Avg Challenge Loss, 7 batches: 0.01833614520728588\n",
            "Avg Card Loss, 1 batches: 0.009017103351652622\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 187 of 1000\n",
            "epsilon: 0.39166620452737816\n",
            "gamma: 0.99\n",
            "Number of games in episode 187: 40\n",
            "torch.Size([1621, 12])\n",
            "torch.Size([1621, 13])\n",
            "torch.Size([1621, 13])\n",
            "torch.Size([1621, 13])\n",
            "Avg Action Loss, 6 batches: 0.0043580736964941025\n",
            "Avg Block Loss, 5 batches: 0.00903095118701458\n",
            "Avg Challenge Loss, 6 batches: 0.003152301302179694\n",
            "Avg Card Loss, 1 batches: 0.0071529243141412735\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 188 of 1000\n",
            "epsilon: 0.3897078735047413\n",
            "gamma: 0.99\n",
            "Number of games in episode 188: 42\n",
            "torch.Size([1610, 12])\n",
            "torch.Size([1610, 13])\n",
            "torch.Size([1610, 13])\n",
            "torch.Size([1610, 13])\n",
            "Avg Action Loss, 6 batches: 0.015004792250692844\n",
            "Avg Block Loss, 5 batches: 0.02767379954457283\n",
            "Avg Challenge Loss, 7 batches: 0.01414402760565281\n",
            "Avg Card Loss, 1 batches: 0.010073641315102577\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 189 of 1000\n",
            "epsilon: 0.3877593341372176\n",
            "gamma: 0.99\n",
            "Number of games in episode 189: 40\n",
            "torch.Size([1622, 12])\n",
            "torch.Size([1622, 13])\n",
            "torch.Size([1622, 13])\n",
            "torch.Size([1622, 13])\n",
            "Avg Action Loss, 6 batches: 0.005450106225907803\n",
            "Avg Block Loss, 5 batches: 0.023174669593572617\n",
            "Avg Challenge Loss, 7 batches: 0.004951451439410448\n",
            "Avg Card Loss, 1 batches: 0.014610763639211655\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 190 of 1000\n",
            "epsilon: 0.3858205374665315\n",
            "gamma: 0.99\n",
            "Number of games in episode 190: 44\n",
            "torch.Size([1620, 12])\n",
            "torch.Size([1620, 13])\n",
            "torch.Size([1620, 13])\n",
            "torch.Size([1620, 13])\n",
            "Avg Action Loss, 6 batches: 0.010681184008717537\n",
            "Avg Block Loss, 5 batches: 0.010986345820128918\n",
            "Avg Challenge Loss, 6 batches: 0.024576887488365173\n",
            "Avg Card Loss, 2 batches: 0.038128916174173355\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "Most common action for Bot 0: 2 taken 469 times.\n",
            "total game lengths: 3644\n",
            "win rate: 0.39\n",
            "episode 191 of 1000\n",
            "epsilon: 0.38389143477919885\n",
            "gamma: 0.99\n",
            "Number of games in episode 191: 44\n",
            "torch.Size([1609, 12])\n",
            "torch.Size([1609, 13])\n",
            "torch.Size([1609, 13])\n",
            "torch.Size([1609, 13])\n",
            "Avg Action Loss, 7 batches: 0.01947157457470894\n",
            "Avg Block Loss, 5 batches: 0.01794319599866867\n",
            "Avg Challenge Loss, 6 batches: 0.021728515625\n",
            "Avg Card Loss, 1 batches: 0.02344716154038906\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 192 of 1000\n",
            "epsilon: 0.3819719776053028\n",
            "gamma: 0.99\n",
            "Number of games in episode 192: 47\n",
            "torch.Size([1613, 12])\n",
            "torch.Size([1613, 13])\n",
            "torch.Size([1613, 13])\n",
            "torch.Size([1613, 13])\n",
            "Avg Action Loss, 6 batches: 0.009677695110440254\n",
            "Avg Block Loss, 5 batches: 0.022081749513745308\n",
            "Avg Challenge Loss, 6 batches: 0.01546431053429842\n",
            "Avg Card Loss, 2 batches: 0.0370178185403347\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 193 of 1000\n",
            "epsilon: 0.3800621177172763\n",
            "gamma: 0.99\n",
            "Number of games in episode 193: 46\n",
            "torch.Size([1617, 12])\n",
            "torch.Size([1617, 13])\n",
            "torch.Size([1617, 13])\n",
            "torch.Size([1617, 13])\n",
            "Avg Action Loss, 6 batches: 0.011006065644323826\n",
            "Avg Block Loss, 5 batches: 0.013204964809119701\n",
            "Avg Challenge Loss, 6 batches: 0.023872382938861847\n",
            "Avg Card Loss, 2 batches: 0.015999361872673035\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 194 of 1000\n",
            "epsilon: 0.37816180712868996\n",
            "gamma: 0.99\n",
            "Number of games in episode 194: 45\n",
            "torch.Size([1632, 12])\n",
            "torch.Size([1632, 13])\n",
            "torch.Size([1632, 13])\n",
            "torch.Size([1632, 13])\n",
            "Avg Action Loss, 6 batches: 0.008477712981402874\n",
            "Avg Block Loss, 5 batches: 0.02198377624154091\n",
            "Avg Challenge Loss, 6 batches: 0.012643656693398952\n",
            "Avg Card Loss, 2 batches: 0.029214343056082726\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 195 of 1000\n",
            "epsilon: 0.37627099809304654\n",
            "gamma: 0.99\n",
            "Number of games in episode 195: 47\n",
            "torch.Size([1627, 12])\n",
            "torch.Size([1627, 13])\n",
            "torch.Size([1627, 13])\n",
            "torch.Size([1627, 13])\n",
            "Avg Action Loss, 7 batches: 0.01523725874722004\n",
            "Avg Block Loss, 6 batches: 0.03257886320352554\n",
            "Avg Challenge Loss, 6 batches: 0.02120218612253666\n",
            "Avg Card Loss, 2 batches: 0.02319486439228058\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 196 of 1000\n",
            "epsilon: 0.3743896431025813\n",
            "gamma: 0.99\n",
            "Number of games in episode 196: 45\n",
            "torch.Size([1633, 12])\n",
            "torch.Size([1633, 13])\n",
            "torch.Size([1633, 13])\n",
            "torch.Size([1633, 13])\n",
            "Avg Action Loss, 6 batches: 0.013860792852938175\n",
            "Avg Block Loss, 5 batches: 0.04700389504432678\n",
            "Avg Challenge Loss, 6 batches: 0.020724279806017876\n",
            "Avg Card Loss, 2 batches: 0.02928157150745392\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 197 of 1000\n",
            "epsilon: 0.37251769488706843\n",
            "gamma: 0.99\n",
            "Number of games in episode 197: 45\n",
            "torch.Size([1617, 12])\n",
            "torch.Size([1617, 13])\n",
            "torch.Size([1617, 13])\n",
            "torch.Size([1617, 13])\n",
            "Avg Action Loss, 5 batches: 0.016265753656625748\n",
            "Avg Block Loss, 5 batches: 0.01203577034175396\n",
            "Avg Challenge Loss, 5 batches: 0.011375055648386478\n",
            "Avg Card Loss, 2 batches: 0.03799162805080414\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 198 of 1000\n",
            "epsilon: 0.3706551064126331\n",
            "gamma: 0.99\n",
            "Number of games in episode 198: 45\n",
            "torch.Size([1603, 12])\n",
            "torch.Size([1603, 13])\n",
            "torch.Size([1603, 13])\n",
            "torch.Size([1603, 13])\n",
            "Avg Action Loss, 6 batches: 0.013311676681041718\n",
            "Avg Block Loss, 5 batches: 0.014197859913110733\n",
            "Avg Challenge Loss, 6 batches: 0.0208766870200634\n",
            "Avg Card Loss, 2 batches: 0.03830261528491974\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 199 of 1000\n",
            "epsilon: 0.36880183088056995\n",
            "gamma: 0.99\n",
            "Number of games in episode 199: 47\n",
            "torch.Size([1619, 12])\n",
            "torch.Size([1619, 13])\n",
            "torch.Size([1619, 13])\n",
            "torch.Size([1619, 13])\n",
            "Avg Action Loss, 6 batches: 0.01780072972178459\n",
            "Avg Block Loss, 5 batches: 0.012979800812900066\n",
            "Avg Challenge Loss, 5 batches: 0.043913960456848145\n",
            "Avg Card Loss, 2 batches: 0.013725833036005497\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 200 of 1000\n",
            "epsilon: 0.3669578217261671\n",
            "gamma: 0.99\n",
            "Number of games in episode 200: 44\n",
            "torch.Size([1622, 12])\n",
            "torch.Size([1622, 13])\n",
            "torch.Size([1622, 13])\n",
            "torch.Size([1622, 13])\n",
            "Avg Action Loss, 6 batches: 0.027130501344799995\n",
            "Avg Block Loss, 5 batches: 0.017580311745405197\n",
            "Avg Challenge Loss, 6 batches: 0.021714655682444572\n",
            "Avg Card Loss, 2 batches: 0.030119415372610092\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "Most common action for Bot 0: 2 taken 583 times.\n",
            "total game lengths: 3635\n",
            "win rate: 0.27\n",
            "episode 201 of 1000\n",
            "epsilon: 0.36512303261753626\n",
            "gamma: 0.99\n",
            "Number of games in episode 201: 42\n",
            "torch.Size([1617, 12])\n",
            "torch.Size([1617, 13])\n",
            "torch.Size([1617, 13])\n",
            "torch.Size([1617, 13])\n",
            "Avg Action Loss, 7 batches: 0.031102722510695457\n",
            "Avg Block Loss, 5 batches: 0.02221546322107315\n",
            "Avg Challenge Loss, 6 batches: 0.013407927006483078\n",
            "Avg Card Loss, 1 batches: 0.014164101332426071\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 202 of 1000\n",
            "epsilon: 0.3632974174544486\n",
            "gamma: 0.99\n",
            "Number of games in episode 202: 45\n",
            "torch.Size([1632, 12])\n",
            "torch.Size([1632, 13])\n",
            "torch.Size([1632, 13])\n",
            "torch.Size([1632, 13])\n",
            "Avg Action Loss, 6 batches: 0.010182702913880348\n",
            "Avg Block Loss, 5 batches: 0.018095502629876137\n",
            "Avg Challenge Loss, 7 batches: 0.008838515728712082\n",
            "Avg Card Loss, 2 batches: 0.05538666248321533\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 203 of 1000\n",
            "epsilon: 0.3614809303671764\n",
            "gamma: 0.99\n",
            "Number of games in episode 203: 44\n",
            "torch.Size([1613, 12])\n",
            "torch.Size([1613, 13])\n",
            "torch.Size([1613, 13])\n",
            "torch.Size([1613, 13])\n",
            "Avg Action Loss, 6 batches: 0.016566528007388115\n",
            "Avg Block Loss, 5 batches: 0.013392419554293156\n",
            "Avg Challenge Loss, 7 batches: 0.003840202232822776\n",
            "Avg Card Loss, 1 batches: 0.012670669704675674\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 204 of 1000\n",
            "epsilon: 0.3596735257153405\n",
            "gamma: 0.99\n",
            "Number of games in episode 204: 42\n",
            "torch.Size([1632, 12])\n",
            "torch.Size([1632, 13])\n",
            "torch.Size([1632, 13])\n",
            "torch.Size([1632, 13])\n",
            "Avg Action Loss, 6 batches: 0.010867546312510967\n",
            "Avg Block Loss, 5 batches: 0.00940940622240305\n",
            "Avg Challenge Loss, 7 batches: 0.022143464535474777\n",
            "Avg Card Loss, 1 batches: 0.013584730215370655\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 205 of 1000\n",
            "epsilon: 0.3578751580867638\n",
            "gamma: 0.99\n",
            "Number of games in episode 205: 41\n",
            "torch.Size([1632, 12])\n",
            "torch.Size([1632, 13])\n",
            "torch.Size([1632, 13])\n",
            "torch.Size([1632, 13])\n",
            "Avg Action Loss, 5 batches: 0.009634691290557384\n",
            "Avg Block Loss, 5 batches: 0.010217957198619843\n",
            "Avg Challenge Loss, 6 batches: 0.0019629995804280043\n",
            "Avg Card Loss, 2 batches: 0.032017238438129425\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 206 of 1000\n",
            "epsilon: 0.35608578229633\n",
            "gamma: 0.99\n",
            "Number of games in episode 206: 40\n",
            "torch.Size([1607, 12])\n",
            "torch.Size([1607, 13])\n",
            "torch.Size([1607, 13])\n",
            "torch.Size([1607, 13])\n",
            "Avg Action Loss, 6 batches: 0.0039807590655982494\n",
            "Avg Block Loss, 5 batches: 0.023122461512684822\n",
            "Avg Challenge Loss, 7 batches: 0.01102801039814949\n",
            "Avg Card Loss, 1 batches: 0.02264820784330368\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 207 of 1000\n",
            "epsilon: 0.3543053533848483\n",
            "gamma: 0.99\n",
            "Number of games in episode 207: 44\n",
            "torch.Size([1602, 12])\n",
            "torch.Size([1602, 13])\n",
            "torch.Size([1602, 13])\n",
            "torch.Size([1602, 13])\n",
            "Avg Action Loss, 6 batches: 0.003095438936725259\n",
            "Avg Block Loss, 5 batches: 0.023922540247440338\n",
            "Avg Challenge Loss, 6 batches: 0.028784537687897682\n",
            "Avg Card Loss, 2 batches: 0.05652255192399025\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 208 of 1000\n",
            "epsilon: 0.35253382661792404\n",
            "gamma: 0.99\n",
            "Number of games in episode 208: 45\n",
            "torch.Size([1610, 12])\n",
            "torch.Size([1610, 13])\n",
            "torch.Size([1610, 13])\n",
            "torch.Size([1610, 13])\n",
            "Avg Action Loss, 5 batches: 0.005997483618557453\n",
            "Avg Block Loss, 5 batches: 0.020866908133029938\n",
            "Avg Challenge Loss, 6 batches: 0.020317457616329193\n",
            "Avg Card Loss, 2 batches: 0.02399500459432602\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 209 of 1000\n",
            "epsilon: 0.3507711574848344\n",
            "gamma: 0.99\n",
            "Number of games in episode 209: 45\n",
            "torch.Size([1618, 12])\n",
            "torch.Size([1618, 13])\n",
            "torch.Size([1618, 13])\n",
            "torch.Size([1618, 13])\n",
            "Avg Action Loss, 6 batches: 0.019894789904356003\n",
            "Avg Block Loss, 5 batches: 0.014505196362733841\n",
            "Avg Challenge Loss, 6 batches: 0.007076775189489126\n",
            "Avg Card Loss, 2 batches: 0.0319729745388031\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 210 of 1000\n",
            "epsilon: 0.34901730169741024\n",
            "gamma: 0.99\n",
            "Number of games in episode 210: 46\n",
            "torch.Size([1627, 12])\n",
            "torch.Size([1627, 13])\n",
            "torch.Size([1627, 13])\n",
            "torch.Size([1627, 13])\n",
            "Avg Action Loss, 6 batches: 0.0028333663940429688\n",
            "Avg Block Loss, 5 batches: 0.015300166793167591\n",
            "Avg Challenge Loss, 6 batches: 0.012109789997339249\n",
            "Avg Card Loss, 2 batches: 0.04366414621472359\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "Most common action for Bot 0: 3 taken 332 times.\n",
            "total game lengths: 3258\n",
            "win rate: 0.23\n",
            "episode 211 of 1000\n",
            "epsilon: 0.3472722151889232\n",
            "gamma: 0.99\n",
            "Number of games in episode 211: 46\n",
            "torch.Size([1625, 12])\n",
            "torch.Size([1625, 13])\n",
            "torch.Size([1625, 13])\n",
            "torch.Size([1625, 13])\n",
            "Avg Action Loss, 6 batches: 0.00998762995004654\n",
            "Avg Block Loss, 5 batches: 0.024159399792551994\n",
            "Avg Challenge Loss, 6 batches: 0.025115590542554855\n",
            "Avg Card Loss, 2 batches: 0.030059710144996643\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 212 of 1000\n",
            "epsilon: 0.3455358541129786\n",
            "gamma: 0.99\n",
            "Number of games in episode 212: 49\n",
            "torch.Size([1630, 12])\n",
            "torch.Size([1630, 13])\n",
            "torch.Size([1630, 13])\n",
            "torch.Size([1630, 13])\n",
            "Avg Action Loss, 6 batches: 0.025013895705342293\n",
            "Avg Block Loss, 5 batches: 0.012776550836861134\n",
            "Avg Challenge Loss, 5 batches: 0.036221519112586975\n",
            "Avg Card Loss, 2 batches: 0.03090241178870201\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 213 of 1000\n",
            "epsilon: 0.3438081748424137\n",
            "gamma: 0.99\n",
            "Number of games in episode 213: 46\n",
            "torch.Size([1608, 12])\n",
            "torch.Size([1608, 13])\n",
            "torch.Size([1608, 13])\n",
            "torch.Size([1608, 13])\n",
            "Avg Action Loss, 6 batches: 0.008211757056415081\n",
            "Avg Block Loss, 5 batches: 0.014891620725393295\n",
            "Avg Challenge Loss, 6 batches: 0.012342089787125587\n",
            "Avg Card Loss, 2 batches: 0.032911818474531174\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 214 of 1000\n",
            "epsilon: 0.3420891339682016\n",
            "gamma: 0.99\n",
            "Number of games in episode 214: 47\n",
            "torch.Size([1631, 12])\n",
            "torch.Size([1631, 13])\n",
            "torch.Size([1631, 13])\n",
            "torch.Size([1631, 13])\n",
            "Avg Action Loss, 6 batches: 0.005638477858155966\n",
            "Avg Block Loss, 5 batches: 0.01716964691877365\n",
            "Avg Challenge Loss, 5 batches: 0.011797090992331505\n",
            "Avg Card Loss, 2 batches: 0.011195972561836243\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 215 of 1000\n",
            "epsilon: 0.3403786882983606\n",
            "gamma: 0.99\n",
            "Number of games in episode 215: 47\n",
            "torch.Size([1616, 12])\n",
            "torch.Size([1616, 13])\n",
            "torch.Size([1616, 13])\n",
            "torch.Size([1616, 13])\n",
            "Avg Action Loss, 6 batches: 0.010403471067547798\n",
            "Avg Block Loss, 5 batches: 0.011304834857583046\n",
            "Avg Challenge Loss, 5 batches: 0.020652780309319496\n",
            "Avg Card Loss, 1 batches: 0.01830897480249405\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 216 of 1000\n",
            "epsilon: 0.3386767948568688\n",
            "gamma: 0.99\n",
            "Number of games in episode 216: 47\n",
            "torch.Size([1627, 12])\n",
            "torch.Size([1627, 13])\n",
            "torch.Size([1627, 13])\n",
            "torch.Size([1627, 13])\n",
            "Avg Action Loss, 7 batches: 0.018462901934981346\n",
            "Avg Block Loss, 5 batches: 0.02065887674689293\n",
            "Avg Challenge Loss, 6 batches: 0.011815974488854408\n",
            "Avg Card Loss, 1 batches: 0.031443219631910324\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 217 of 1000\n",
            "epsilon: 0.33698341088258443\n",
            "gamma: 0.99\n",
            "Number of games in episode 217: 48\n",
            "torch.Size([1618, 12])\n",
            "torch.Size([1618, 13])\n",
            "torch.Size([1618, 13])\n",
            "torch.Size([1618, 13])\n",
            "Avg Action Loss, 7 batches: 0.01166040264070034\n",
            "Avg Block Loss, 5 batches: 0.0285212192684412\n",
            "Avg Challenge Loss, 6 batches: 0.01950022391974926\n",
            "Avg Card Loss, 2 batches: 0.031933240592479706\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 218 of 1000\n",
            "epsilon: 0.3352984938281715\n",
            "gamma: 0.99\n",
            "Number of games in episode 218: 45\n",
            "torch.Size([1630, 12])\n",
            "torch.Size([1630, 13])\n",
            "torch.Size([1630, 13])\n",
            "torch.Size([1630, 13])\n",
            "Avg Action Loss, 7 batches: 0.02802787534892559\n",
            "Avg Block Loss, 5 batches: 0.013152483850717545\n",
            "Avg Challenge Loss, 6 batches: 0.013699623756110668\n",
            "Avg Card Loss, 1 batches: 0.02177426964044571\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 219 of 1000\n",
            "epsilon: 0.33362200135903064\n",
            "gamma: 0.99\n",
            "Number of games in episode 219: 44\n",
            "torch.Size([1605, 12])\n",
            "torch.Size([1605, 13])\n",
            "torch.Size([1605, 13])\n",
            "torch.Size([1605, 13])\n",
            "Avg Action Loss, 6 batches: 0.014564566314220428\n",
            "Avg Block Loss, 5 batches: 0.00746537558734417\n",
            "Avg Challenge Loss, 6 batches: 0.009359301999211311\n",
            "Avg Card Loss, 1 batches: 0.004111142363399267\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 220 of 1000\n",
            "epsilon: 0.33195389135223546\n",
            "gamma: 0.99\n",
            "Number of games in episode 220: 44\n",
            "torch.Size([1628, 12])\n",
            "torch.Size([1628, 13])\n",
            "torch.Size([1628, 13])\n",
            "torch.Size([1628, 13])\n",
            "Avg Action Loss, 6 batches: 0.0057552591897547245\n",
            "Avg Block Loss, 4 batches: 0.0026516132056713104\n",
            "Avg Challenge Loss, 6 batches: 0.005140452645719051\n",
            "Avg Card Loss, 2 batches: 0.011860016733407974\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "Most common action for Bot 0: 2 taken 823 times.\n",
            "total game lengths: 3841\n",
            "win rate: 0.2\n",
            "episode 221 of 1000\n",
            "epsilon: 0.3302941218954743\n",
            "gamma: 0.99\n",
            "Number of games in episode 221: 43\n",
            "torch.Size([1602, 12])\n",
            "torch.Size([1602, 13])\n",
            "torch.Size([1602, 13])\n",
            "torch.Size([1602, 13])\n",
            "Avg Action Loss, 6 batches: 0.006431117653846741\n",
            "Avg Block Loss, 4 batches: 0.015502512454986572\n",
            "Avg Challenge Loss, 6 batches: 0.028096022084355354\n",
            "Avg Card Loss, 2 batches: 0.010821969248354435\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 222 of 1000\n",
            "epsilon: 0.32864265128599696\n",
            "gamma: 0.99\n",
            "Number of games in episode 222: 41\n",
            "torch.Size([1636, 12])\n",
            "torch.Size([1636, 13])\n",
            "torch.Size([1636, 13])\n",
            "torch.Size([1636, 13])\n",
            "Avg Action Loss, 6 batches: 0.004258680623024702\n",
            "Avg Block Loss, 5 batches: 0.02577824890613556\n",
            "Avg Challenge Loss, 7 batches: 0.02191661298274994\n",
            "Avg Card Loss, 2 batches: 0.014281577430665493\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 223 of 1000\n",
            "epsilon: 0.326999438029567\n",
            "gamma: 0.99\n",
            "Number of games in episode 223: 40\n",
            "torch.Size([1610, 12])\n",
            "torch.Size([1610, 13])\n",
            "torch.Size([1610, 13])\n",
            "torch.Size([1610, 13])\n",
            "Avg Action Loss, 6 batches: 0.015154068358242512\n",
            "Avg Block Loss, 5 batches: 0.02194167673587799\n",
            "Avg Challenge Loss, 7 batches: 0.008285955525934696\n",
            "Avg Card Loss, 1 batches: 0.022334624081850052\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 224 of 1000\n",
            "epsilon: 0.3253644408394192\n",
            "gamma: 0.99\n",
            "Number of games in episode 224: 41\n",
            "torch.Size([1639, 12])\n",
            "torch.Size([1639, 13])\n",
            "torch.Size([1639, 13])\n",
            "torch.Size([1639, 13])\n",
            "Avg Action Loss, 6 batches: 0.009916890412569046\n",
            "Avg Block Loss, 5 batches: 0.0066877566277980804\n",
            "Avg Challenge Loss, 6 batches: 0.020328834652900696\n",
            "Avg Card Loss, 1 batches: 0.022903945297002792\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 225 of 1000\n",
            "epsilon: 0.3237376186352221\n",
            "gamma: 0.99\n",
            "Number of games in episode 225: 41\n",
            "torch.Size([1636, 12])\n",
            "torch.Size([1636, 13])\n",
            "torch.Size([1636, 13])\n",
            "torch.Size([1636, 13])\n",
            "Avg Action Loss, 6 batches: 0.012004764750599861\n",
            "Avg Block Loss, 5 batches: 0.0031642653048038483\n",
            "Avg Challenge Loss, 6 batches: 0.011838734149932861\n",
            "Avg Card Loss, 1 batches: 0.006120916455984116\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 226 of 1000\n",
            "epsilon: 0.322118930542046\n",
            "gamma: 0.99\n",
            "Number of games in episode 226: 39\n",
            "torch.Size([1637, 12])\n",
            "torch.Size([1637, 13])\n",
            "torch.Size([1637, 13])\n",
            "torch.Size([1637, 13])\n",
            "Avg Action Loss, 6 batches: 0.01120084896683693\n",
            "Avg Block Loss, 5 batches: 0.02263629250228405\n",
            "Avg Challenge Loss, 6 batches: 0.011059163138270378\n",
            "Avg Card Loss, 1 batches: 0.02390357293188572\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 227 of 1000\n",
            "epsilon: 0.32050833588933575\n",
            "gamma: 0.99\n",
            "Number of games in episode 227: 39\n",
            "torch.Size([1613, 12])\n",
            "torch.Size([1613, 13])\n",
            "torch.Size([1613, 13])\n",
            "torch.Size([1613, 13])\n",
            "Avg Action Loss, 6 batches: 0.014571593143045902\n",
            "Avg Block Loss, 6 batches: 0.007755964994430542\n",
            "Avg Challenge Loss, 7 batches: 0.006579285953193903\n",
            "Avg Card Loss, 1 batches: 0.015284675173461437\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 228 of 1000\n",
            "epsilon: 0.31890579420988907\n",
            "gamma: 0.99\n",
            "Number of games in episode 228: 39\n",
            "torch.Size([1635, 12])\n",
            "torch.Size([1635, 13])\n",
            "torch.Size([1635, 13])\n",
            "torch.Size([1635, 13])\n",
            "Avg Action Loss, 6 batches: 0.006258837878704071\n",
            "Avg Block Loss, 5 batches: 0.02057388424873352\n",
            "Avg Challenge Loss, 6 batches: 0.005438851658254862\n",
            "Avg Card Loss, 1 batches: 0.007903794758021832\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 229 of 1000\n",
            "epsilon: 0.3173112652388396\n",
            "gamma: 0.99\n",
            "Number of games in episode 229: 40\n",
            "torch.Size([1638, 12])\n",
            "torch.Size([1638, 13])\n",
            "torch.Size([1638, 13])\n",
            "torch.Size([1638, 13])\n",
            "Avg Action Loss, 6 batches: 0.010566726326942444\n",
            "Avg Block Loss, 5 batches: 0.03136235475540161\n",
            "Avg Challenge Loss, 6 batches: 0.02103668823838234\n",
            "Avg Card Loss, 1 batches: 0.02138487994670868\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 230 of 1000\n",
            "epsilon: 0.3157247089126454\n",
            "gamma: 0.99\n",
            "Number of games in episode 230: 41\n",
            "torch.Size([1627, 12])\n",
            "torch.Size([1627, 13])\n",
            "torch.Size([1627, 13])\n",
            "torch.Size([1627, 13])\n",
            "Avg Action Loss, 6 batches: 0.021551519632339478\n",
            "Avg Block Loss, 5 batches: 0.014039034023880959\n",
            "Avg Challenge Loss, 6 batches: 0.015775064006447792\n",
            "Avg Card Loss, 1 batches: 0.012691597454249859\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "Most common action for Bot 0: 0 taken 475 times.\n",
            "total game lengths: 3856\n",
            "win rate: 0.22\n",
            "episode 231 of 1000\n",
            "epsilon: 0.3141460853680822\n",
            "gamma: 0.99\n",
            "Number of games in episode 231: 42\n",
            "torch.Size([1621, 12])\n",
            "torch.Size([1621, 13])\n",
            "torch.Size([1621, 13])\n",
            "torch.Size([1621, 13])\n",
            "Avg Action Loss, 6 batches: 0.012938662432134151\n",
            "Avg Block Loss, 5 batches: 0.017771245911717415\n",
            "Avg Challenge Loss, 6 batches: 0.006709386594593525\n",
            "Avg Card Loss, 1 batches: 0.03143058717250824\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 232 of 1000\n",
            "epsilon: 0.3125753549412418\n",
            "gamma: 0.99\n",
            "Number of games in episode 232: 41\n",
            "torch.Size([1607, 12])\n",
            "torch.Size([1607, 13])\n",
            "torch.Size([1607, 13])\n",
            "torch.Size([1607, 13])\n",
            "Avg Action Loss, 6 batches: 0.006786318961530924\n",
            "Avg Block Loss, 5 batches: 0.025148730725049973\n",
            "Avg Challenge Loss, 6 batches: 0.018278755247592926\n",
            "Avg Card Loss, 1 batches: 0.015453137457370758\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 233 of 1000\n",
            "epsilon: 0.31101247816653554\n",
            "gamma: 0.99\n",
            "Number of games in episode 233: 42\n",
            "torch.Size([1608, 12])\n",
            "torch.Size([1608, 13])\n",
            "torch.Size([1608, 13])\n",
            "torch.Size([1608, 13])\n",
            "Avg Action Loss, 5 batches: 0.011611420661211014\n",
            "Avg Block Loss, 4 batches: 0.022173279896378517\n",
            "Avg Challenge Loss, 5 batches: 0.01651569828391075\n",
            "Avg Card Loss, 1 batches: 0.026459187269210815\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 234 of 1000\n",
            "epsilon: 0.30945741577570285\n",
            "gamma: 0.99\n",
            "Number of games in episode 234: 42\n",
            "torch.Size([1632, 12])\n",
            "torch.Size([1632, 13])\n",
            "torch.Size([1632, 13])\n",
            "torch.Size([1632, 13])\n",
            "Avg Action Loss, 6 batches: 0.01225469820201397\n",
            "Avg Block Loss, 5 batches: 0.027673091739416122\n",
            "Avg Challenge Loss, 6 batches: 0.00929289124906063\n",
            "Avg Card Loss, 1 batches: 0.021520428359508514\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 235 of 1000\n",
            "epsilon: 0.3079101286968243\n",
            "gamma: 0.99\n",
            "Number of games in episode 235: 42\n",
            "torch.Size([1630, 12])\n",
            "torch.Size([1630, 13])\n",
            "torch.Size([1630, 13])\n",
            "torch.Size([1630, 13])\n",
            "Avg Action Loss, 6 batches: 0.02327052876353264\n",
            "Avg Block Loss, 5 batches: 0.02726488560438156\n",
            "Avg Challenge Loss, 6 batches: 0.018512364476919174\n",
            "Avg Card Loss, 1 batches: 0.01460161805152893\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 236 of 1000\n",
            "epsilon: 0.3063705780533402\n",
            "gamma: 0.99\n",
            "Number of games in episode 236: 42\n",
            "torch.Size([1620, 12])\n",
            "torch.Size([1620, 13])\n",
            "torch.Size([1620, 13])\n",
            "torch.Size([1620, 13])\n",
            "Avg Action Loss, 6 batches: 0.00993506982922554\n",
            "Avg Block Loss, 5 batches: 0.008101811632514\n",
            "Avg Challenge Loss, 6 batches: 0.017350947484374046\n",
            "Avg Card Loss, 1 batches: 0.031017595902085304\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 237 of 1000\n",
            "epsilon: 0.30483872516307353\n",
            "gamma: 0.99\n",
            "Number of games in episode 237: 43\n",
            "torch.Size([1607, 12])\n",
            "torch.Size([1607, 13])\n",
            "torch.Size([1607, 13])\n",
            "torch.Size([1607, 13])\n",
            "Avg Action Loss, 6 batches: 0.014861484989523888\n",
            "Avg Block Loss, 5 batches: 0.017142562195658684\n",
            "Avg Challenge Loss, 6 batches: 0.02506081387400627\n",
            "Avg Card Loss, 1 batches: 0.025037385523319244\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 238 of 1000\n",
            "epsilon: 0.3033145315372582\n",
            "gamma: 0.99\n",
            "Number of games in episode 238: 43\n",
            "torch.Size([1609, 12])\n",
            "torch.Size([1609, 13])\n",
            "torch.Size([1609, 13])\n",
            "torch.Size([1609, 13])\n",
            "Avg Action Loss, 6 batches: 0.015919575467705727\n",
            "Avg Block Loss, 5 batches: 0.03122110292315483\n",
            "Avg Challenge Loss, 6 batches: 0.013869203627109528\n",
            "Avg Card Loss, 1 batches: 0.027979731559753418\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 239 of 1000\n",
            "epsilon: 0.3017979588795719\n",
            "gamma: 0.99\n",
            "Number of games in episode 239: 44\n",
            "torch.Size([1625, 12])\n",
            "torch.Size([1625, 13])\n",
            "torch.Size([1625, 13])\n",
            "torch.Size([1625, 13])\n",
            "Avg Action Loss, 6 batches: 0.020531514659523964\n",
            "Avg Block Loss, 5 batches: 0.013114726170897484\n",
            "Avg Challenge Loss, 6 batches: 0.0053650690242648125\n",
            "Avg Card Loss, 1 batches: 0.019408807158470154\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 240 of 1000\n",
            "epsilon: 0.30028896908517405\n",
            "gamma: 0.99\n",
            "Number of games in episode 240: 44\n",
            "torch.Size([1611, 12])\n",
            "torch.Size([1611, 13])\n",
            "torch.Size([1611, 13])\n",
            "torch.Size([1611, 13])\n",
            "Avg Action Loss, 6 batches: 0.011605300940573215\n",
            "Avg Block Loss, 5 batches: 0.011656437069177628\n",
            "Avg Challenge Loss, 6 batches: 0.03619888052344322\n",
            "Avg Card Loss, 1 batches: 0.034708306193351746\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "Most common action for Bot 0: 2 taken 507 times.\n",
            "total game lengths: 3594\n",
            "win rate: 0.12\n",
            "episode 241 of 1000\n",
            "epsilon: 0.2987875242397482\n",
            "gamma: 0.99\n",
            "Number of games in episode 241: 46\n",
            "torch.Size([1615, 12])\n",
            "torch.Size([1615, 13])\n",
            "torch.Size([1615, 13])\n",
            "torch.Size([1615, 13])\n",
            "Avg Action Loss, 6 batches: 0.009054356254637241\n",
            "Avg Block Loss, 5 batches: 0.03782595321536064\n",
            "Avg Challenge Loss, 6 batches: 0.014943568035960197\n",
            "Avg Card Loss, 1 batches: 0.028088770806789398\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 242 of 1000\n",
            "epsilon: 0.29729358661854943\n",
            "gamma: 0.99\n",
            "Number of games in episode 242: 45\n",
            "torch.Size([1625, 12])\n",
            "torch.Size([1625, 13])\n",
            "torch.Size([1625, 13])\n",
            "torch.Size([1625, 13])\n",
            "Avg Action Loss, 6 batches: 0.010275652632117271\n",
            "Avg Block Loss, 5 batches: 0.0031901253387331963\n",
            "Avg Challenge Loss, 6 batches: 0.01234972383826971\n",
            "Avg Card Loss, 2 batches: 0.018727142363786697\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 243 of 1000\n",
            "epsilon: 0.29580711868545667\n",
            "gamma: 0.99\n",
            "Number of games in episode 243: 44\n",
            "torch.Size([1603, 12])\n",
            "torch.Size([1603, 13])\n",
            "torch.Size([1603, 13])\n",
            "torch.Size([1603, 13])\n",
            "Avg Action Loss, 6 batches: 0.007811667397618294\n",
            "Avg Block Loss, 5 batches: 0.008166860789060593\n",
            "Avg Challenge Loss, 6 batches: 0.0293798316270113\n",
            "Avg Card Loss, 1 batches: 0.012130390852689743\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 244 of 1000\n",
            "epsilon: 0.2943280830920294\n",
            "gamma: 0.99\n",
            "Number of games in episode 244: 44\n",
            "torch.Size([1606, 12])\n",
            "torch.Size([1606, 13])\n",
            "torch.Size([1606, 13])\n",
            "torch.Size([1606, 13])\n",
            "Avg Action Loss, 6 batches: 0.005750695709139109\n",
            "Avg Block Loss, 5 batches: 0.016549667343497276\n",
            "Avg Challenge Loss, 6 batches: 0.016563499346375465\n",
            "Avg Card Loss, 2 batches: 0.019120335578918457\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 245 of 1000\n",
            "epsilon: 0.29285644267656924\n",
            "gamma: 0.99\n",
            "Number of games in episode 245: 44\n",
            "torch.Size([1619, 12])\n",
            "torch.Size([1619, 13])\n",
            "torch.Size([1619, 13])\n",
            "torch.Size([1619, 13])\n",
            "Avg Action Loss, 6 batches: 0.006480059120804071\n",
            "Avg Block Loss, 5 batches: 0.013709806837141514\n",
            "Avg Challenge Loss, 6 batches: 0.004898357205092907\n",
            "Avg Card Loss, 2 batches: 0.015344169922173023\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 246 of 1000\n",
            "epsilon: 0.2913921604631864\n",
            "gamma: 0.99\n",
            "Number of games in episode 246: 43\n",
            "torch.Size([1632, 12])\n",
            "torch.Size([1632, 13])\n",
            "torch.Size([1632, 13])\n",
            "torch.Size([1632, 13])\n",
            "Avg Action Loss, 6 batches: 0.010402817279100418\n",
            "Avg Block Loss, 5 batches: 0.01395692490041256\n",
            "Avg Challenge Loss, 5 batches: 0.02158932201564312\n",
            "Avg Card Loss, 2 batches: 0.023403069004416466\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 247 of 1000\n",
            "epsilon: 0.28993519966087045\n",
            "gamma: 0.99\n",
            "Number of games in episode 247: 42\n",
            "torch.Size([1620, 12])\n",
            "torch.Size([1620, 13])\n",
            "torch.Size([1620, 13])\n",
            "torch.Size([1620, 13])\n",
            "Avg Action Loss, 7 batches: 0.012650842778384686\n",
            "Avg Block Loss, 6 batches: 0.020676549524068832\n",
            "Avg Challenge Loss, 6 batches: 0.01479146908968687\n",
            "Avg Card Loss, 2 batches: 0.013851791620254517\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 248 of 1000\n",
            "epsilon: 0.2884855236625661\n",
            "gamma: 0.99\n",
            "Number of games in episode 248: 42\n",
            "torch.Size([1638, 12])\n",
            "torch.Size([1638, 13])\n",
            "torch.Size([1638, 13])\n",
            "torch.Size([1638, 13])\n",
            "Avg Action Loss, 6 batches: 0.012283441610634327\n",
            "Avg Block Loss, 5 batches: 0.031293511390686035\n",
            "Avg Challenge Loss, 5 batches: 0.017076680436730385\n",
            "Avg Card Loss, 2 batches: 0.01285579614341259\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 249 of 1000\n",
            "epsilon: 0.28704309604425327\n",
            "gamma: 0.99\n",
            "Number of games in episode 249: 42\n",
            "torch.Size([1610, 12])\n",
            "torch.Size([1610, 13])\n",
            "torch.Size([1610, 13])\n",
            "torch.Size([1610, 13])\n",
            "Avg Action Loss, 6 batches: 0.016450321301817894\n",
            "Avg Block Loss, 4 batches: 0.026484882459044456\n",
            "Avg Challenge Loss, 5 batches: 0.010584753938019276\n",
            "Avg Card Loss, 2 batches: 0.012353818863630295\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "episode 250 of 1000\n",
            "epsilon: 0.285607880564032\n",
            "gamma: 0.99\n",
            "Number of games in episode 250: 41\n",
            "torch.Size([1615, 12])\n",
            "torch.Size([1615, 13])\n",
            "torch.Size([1615, 13])\n",
            "torch.Size([1615, 13])\n",
            "Avg Action Loss, 6 batches: 0.015481855720281601\n",
            "Avg Block Loss, 5 batches: 0.014454517513513565\n",
            "Avg Challenge Loss, 5 batches: 0.014905235730111599\n",
            "Avg Card Loss, 2 batches: 0.011756975203752518\n",
            "Action Q matches? True\n",
            "Block Q matches? True\n",
            "Challenge Q matches? True\n",
            "Card Q matches? True\n",
            "0\n",
            "Most common action for Bot 0: 6 taken 524 times.\n",
            "total game lengths: 3940\n",
            "win rate: 0.23\n",
            "episode 251 of 1000\n",
            "epsilon: 0.28417984116121187\n",
            "gamma: 0.99\n",
            "Number of games in episode 251: 41\n",
            "torch.Size([1614, 12])\n",
            "torch.Size([1614, 13])\n",
            "torch.Size([1614, 13])\n",
            "torch.Size([1614, 13])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-465-4fff822dc34b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    794\u001b[0m   \u001b[0;31m# ----------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m   \u001b[0mbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_challenge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m   \u001b[0mloss_challenge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    797\u001b[0m   \u001b[0mbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_challenge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from os import stat\n",
        "from collections import defaultdict\n",
        "from collections import Counter\n",
        "num_episodes = 1000\n",
        "max_steps_per_episode = 200\n",
        "epsilon = 1.0\n",
        "list_division = 4\n",
        "gamma = 0.99\n",
        "\n",
        "bot = copy.deepcopy(bots[0])\n",
        "\n",
        "avg_losses_action = []\n",
        "avg_losses_block = []\n",
        "avg_losses_challenge = []\n",
        "avg_losses_card = []\n",
        "\n",
        "# bots.remove(bots[-1])\n",
        "# bots.remove(bots[-1])\n",
        "\n",
        "win_rates = []\n",
        "avg_game_lengths = []\n",
        "\n",
        "data_fraction = 1/5\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "\n",
        "  replay_buffer_actions = []\n",
        "  replay_buffer_blocks = []\n",
        "  replay_buffer_challenges = []\n",
        "  replay_buffer_cards = []\n",
        "\n",
        "  print(f'episode {episode} of 1000')\n",
        "  print(f'epsilon: {epsilon}')\n",
        "  print(f'gamma: {gamma}')\n",
        "\n",
        "  # discard_piles, acting_players, reacting_players, current_players, actions_game, reactions_game, challenges_game, cards_game, coins_game, challenges_direction, done, rewards, cards_chosen, bots_copy = game_loop_random(bots, actions, influences_reverse, epsilon)\n",
        "  # bots = bots_copy\n",
        "  state = torch.empty((0, 12), dtype=torch.float32)  # Assume state_size = 25 for action network\n",
        "  state_block = torch.empty((0, 13), dtype=torch.float32)\n",
        "  state_challenge = torch.empty((0, 13), dtype=torch.float32)\n",
        "  state_card = torch.empty((0, 13), dtype=torch.float32)\n",
        "  # states_action = torch.empty((0, 24), dtype=torch.float32)  # Assume state_size = 25 for action network\n",
        "  # next_states_action = torch.empty((0, 24), dtype=torch.float32)\n",
        "  actions_main = torch.empty((0,), dtype=torch.int64)\n",
        "  # states_block = torch.empty((0, 23), dtype=torch.float32)  # Assume state_size = 24 for block network\n",
        "  # next_states_block = torch.empty((0, 23), dtype=torch.float32)\n",
        "  actions_block = torch.empty((0,), dtype=torch.int64)\n",
        "  # states_challenge = torch.empty((0, 24), dtype=torch.float32)  # Assume state_size = 25 for challenge network\n",
        "  # next_states_challenge = torch.empty((0, 24), dtype=torch.float32)\n",
        "  actions_challenge = torch.empty((0,), dtype=torch.int64)\n",
        "  # states_card = torch.empty((0, 19), dtype=torch.float32)  # Assume state_size = 20 for card network\n",
        "  # next_states_card = torch.empty((0, 19), dtype=torch.float32)\n",
        "  actions_card = torch.empty((0,), dtype=torch.int64)\n",
        "  rewards = torch.empty((0,), dtype=torch.float32)\n",
        "  done = torch.empty((0,), dtype=torch.float32)\n",
        "  game_length_sum = 0\n",
        "  all_discard_piles = []\n",
        "  acting_players = []\n",
        "  reacting_players = []\n",
        "  reactions_game = []\n",
        "\n",
        "  num_games = 0\n",
        "\n",
        "\n",
        "  while len(state) <= 25 * batch_size:\n",
        "    # print(f'game {num_games}')\n",
        "\n",
        "    discard_pile, acting_player, reacting_player, current_player, action_game, reaction_game, challenge_game, card_game, coin_game, challenge_direction, done_0, reward, card_chosen, bots_copy = game_loop_random(bots, actions, influences_reverse, epsilon)\n",
        "\n",
        "    num_games += 1\n",
        "    # print(f'Game Number {num_games}')\n",
        "\n",
        "    # if random.random():\n",
        "    #   print(reacting_player)\n",
        "\n",
        "    # start_index = int(3 * len(acting_player) / list_division)\n",
        "\n",
        "    bots = bots_copy\n",
        "\n",
        "    game_length_sum += len(acting_player)\n",
        "\n",
        "    split_point = int((1 - data_fraction) * len(acting_player))\n",
        "    acting_players += acting_player\n",
        "    reacting_players += reacting_player\n",
        "    current_players = current_player\n",
        "    actions_game = action_game\n",
        "    reactions_game += reaction_game\n",
        "    challenges_game = challenge_game\n",
        "    cards_game = card_game\n",
        "    coins_game = coin_game\n",
        "    challenges_direction = challenge_direction\n",
        "    cards_chosen = card_chosen\n",
        "    discard_piles = discard_pile\n",
        "    all_discard_piles += discard_pile\n",
        "\n",
        "    avg_game_lengths.append(game_length_sum / 100)\n",
        "\n",
        "    # 1. Cards in play (embedded):\n",
        "    all_cards_in_play_embedded = []\n",
        "\n",
        "    for current_discard_pile in discard_pile:\n",
        "        cards_in_play_embedded = []\n",
        "        for card_name in influences.keys():\n",
        "            num_in_discard = current_discard_pile.count(inf_map[card_name])\n",
        "            num_in_play = 3 - num_in_discard\n",
        "            cards_in_play_embedded.append(torch.tensor(num_in_play)) # Remove .tolist() here\n",
        "\n",
        "        all_cards_in_play_embedded.append(torch.stack(cards_in_play_embedded)) # Stack the embedded tensors here\n",
        "\n",
        "    # Convert to a single tensor outside the loop\n",
        "    all_cards_in_play_embedded = torch.stack(all_cards_in_play_embedded)\n",
        "\n",
        "    # 4. Bot 0's normalized coins:\n",
        "    bot0_coins_normalized = torch.tensor(coin_game)[:, 0] / 12  # Get Bot 0's coins and normalize\n",
        "\n",
        "    # 5. Average cards of other players (normalized and embedded):\n",
        "    avg_other_cards_normalized = []\n",
        "    for step_cards in cards_game:\n",
        "        other_bots_cards = [len([card for card in bot_cards if card != 0])\n",
        "                          for bot_cards in step_cards[1:]]  # Exclude Bot 0\n",
        "        avg_other_cards = sum(other_bots_cards) / len(other_bots_cards) if other_bots_cards else 0\n",
        "        avg_other_cards_normalized.append(avg_other_cards / 2)\n",
        "\n",
        "    avg_other_cards_normalized = torch.tensor(avg_other_cards_normalized)\n",
        "    # avg_other_cards_embedded = embedding_cards(torch.tensor(int(avg_other_cards_normalized))).tolist()  # Assuming embedding_cards is your embedding layer\n",
        "\n",
        "    # 6. Bot 0's current cards (embedded):\n",
        "    all_bot0_cards_embedded = []  # Store embedded cards for all steps\n",
        "\n",
        "    for step_cards in cards_game:\n",
        "        bot0_cards_embedded = []\n",
        "        for card in step_cards[0]:  # Get Bot 0's cards for this step\n",
        "            if card != 0:  # Assuming 0 represents the absence of a card\n",
        "                bot0_cards_embedded.extend(embedding_cards(torch.tensor(card)).tolist())\n",
        "\n",
        "        # If Bot 0 has no cards, add zero embeddings for consistency\n",
        "        while len(bot0_cards_embedded) < embedding_cards.embedding_dim * 2:  # Assuming 2 cards max\n",
        "            bot0_cards_embedded.extend([0] * embedding_cards.embedding_dim)\n",
        "\n",
        "        all_bot0_cards_embedded.append(torch.tensor(bot0_cards_embedded))  # Convert to tensor and store\n",
        "\n",
        "    all_bot0_cards_embedded = torch.stack(all_bot0_cards_embedded) # Stack to create a 2D tensor\n",
        "\n",
        "    # 7. The last action taken (embedded):\n",
        "    all_last_action_embedded = []  # Store embedded last actions for all steps\n",
        "\n",
        "    actions_game.insert(0, 7)  # Add a dummy action at the beginning\n",
        "    reaction_game.insert(0, 0)  # Add a dummy reaction at the beginning\n",
        "    challenges_game.insert(0, 0)  # Add a dummy challenge at the beginning\n",
        "\n",
        "    for i in range(len(actions_game[:-1])):\n",
        "        last_action = actions_game[i]  # Get the action for the current step\n",
        "        last_action_embedded = embedding_actions(torch.tensor(last_action)).tolist()\n",
        "        all_last_action_embedded.append(last_action_embedded)\n",
        "\n",
        "    all_last_action_embedded = torch.tensor(all_last_action_embedded)  # Convert to a tensor\n",
        "\n",
        "    new_state = torch.cat(([all_cards_in_play_embedded.unsqueeze(-1),\n",
        "                        bot0_coins_normalized.unsqueeze(-1).unsqueeze(-1),\n",
        "                        avg_other_cards_normalized.unsqueeze(-1).unsqueeze(-1),\n",
        "                        all_bot0_cards_embedded.unsqueeze(-1),\n",
        "                        all_last_action_embedded.unsqueeze(-1)]),\n",
        "                      1).squeeze(2)\n",
        "    state = torch.cat([state, new_state], 0)\n",
        "\n",
        "    # print(new_state.shape)\n",
        "\n",
        "    # print(torch.tensor(reactions_game[:-1]).shape)\n",
        "\n",
        "    new_state_block = torch.cat([new_state, torch.tensor(reaction_game[:-1]).unsqueeze(1)], 1)\n",
        "    state_block = torch.cat([state_block, new_state_block], 0)\n",
        "\n",
        "    new_state_challenge = torch.cat([new_state, torch.tensor(challenges_game[1:]).unsqueeze(1)], 1)\n",
        "    state_challenge = torch.cat([state_challenge, new_state_challenge], 0)\n",
        "\n",
        "    new_state_card = torch.cat([new_state, torch.tensor(cards_chosen).unsqueeze(1)], 1)\n",
        "    state_card = torch.cat([state_card, new_state_card], 0)\n",
        "\n",
        "    new_actions_main = torch.tensor(actions_game[1:]).type(torch.int64)\n",
        "    actions_main = torch.cat([actions_main, new_actions_main], 0)\n",
        "\n",
        "\n",
        "    new_actions_block = torch.tensor(reaction_game).type(torch.int64)\n",
        "    actions_block = torch.cat([actions_block, new_actions_block], 0)\n",
        "\n",
        "\n",
        "    new_actions_challenge = torch.tensor(challenges_game).type(torch.int64)\n",
        "    actions_challenge = torch.cat([actions_challenge, new_actions_challenge], 0)\n",
        "\n",
        "\n",
        "    new_actions_card = torch.tensor(cards_chosen).type(torch.int64)\n",
        "    actions_card = torch.cat([actions_card, new_actions_card], 0)\n",
        "\n",
        "    new_rewards = torch.tensor(reward).type(torch.float32)\n",
        "    rewards = torch.cat([rewards, new_rewards], 0)\n",
        "\n",
        "    new_done = torch.tensor(done_0).type(torch.float32)\n",
        "    done = torch.cat([done, new_done], 0)\n",
        "\n",
        "\n",
        "\n",
        "  print(f'Number of games in episode {episode}: {num_games}')\n",
        "\n",
        "  print(state.shape)\n",
        "  print(state_block.shape)\n",
        "  print(state_challenge.shape)\n",
        "  print(state_card.shape)\n",
        "\n",
        "  states_action = torch.empty((0, 12), dtype=torch.float32)\n",
        "  states_block = torch.empty((0, 13), dtype=torch.float32)\n",
        "  states_challenge = torch.empty((0, 13), dtype=torch.float32)\n",
        "  states_card = torch.empty((0, 13), dtype=torch.float32)\n",
        "\n",
        "  # Assuming you have a list called 'all_states' that contains all the states\n",
        "  # generated using the 'new_state' calculation you provided\n",
        "  # and 'acting_players' list that has acting players per state,\n",
        "  # and 'reacting_players' list for reacting players\n",
        "\n",
        "  # all_states = []  # Initialize with your existing state generation logic\n",
        "\n",
        "  # print(state.shape)\n",
        "  # print(all_bot0_cards_embedded.shape)\n",
        "\n",
        "  # Create a dictionary to store current states and their corresponding next states\n",
        "  state_transitions = defaultdict(list)\n",
        "\n",
        "  for i in range(len(state) - 1):  # Iterate through all states (except the last one)\n",
        "      # print(next_state[7:9])\n",
        "      current_state = state[i]\n",
        "      next_state = state[i + 1]\n",
        "\n",
        "      # Add the next state to the list of next states for the current state\n",
        "      state_transitions[tuple(current_state.tolist())].append(next_state)  # Convert to tuple for dictionary key\n",
        "\n",
        "  state_indices = {}\n",
        "\n",
        "  for i, state_tensor in enumerate(state):\n",
        "      state_indices[tuple(state_tensor.tolist())] = i\n",
        "\n",
        "  for i in range(len(state) - 1):  # Iterate through all states (except the last one)\n",
        "\n",
        "      current_state_action = state[i]\n",
        "      current_state_block = state_block[i]\n",
        "      current_state_challenge = state_challenge[i]\n",
        "      current_state_card = state_card[i]\n",
        "      # next_state = state[i + 1]\n",
        "\n",
        "      # --- Action Network ---\n",
        "      if acting_players[i] == 0:  # Check acting player for current state\n",
        "          states_action = torch.cat([states_action, current_state_action.unsqueeze(0)], 0)\n",
        "      # else:\n",
        "      #     next_states_action = torch.cat([next_states_action, next_state.unsqueeze(0)], 0)\n",
        "\n",
        "      # --- Reaction Network & Challenge Network ---\n",
        "      if reacting_players[i] == 0:  # Bot 0 is the reacting player\n",
        "          states_block = torch.cat([states_block, current_state_block.unsqueeze(0)], 0)\n",
        "      # else:\n",
        "      #     next_states_block = torch.cat([next_states_block, next_state.unsqueeze(0)], 0)\n",
        "\n",
        "      # --- Challenge Network ---\n",
        "      if reacting_players[i] == 0:\n",
        "          states_challenge = torch.cat([states_challenge, current_state_challenge.unsqueeze(0)], 0)\n",
        "      elif acting_players[i] == 0 and reactions_game[i+1] == 1: # Check acting player for current state\n",
        "          states_challenge = torch.cat([states_challenge, current_state_challenge.unsqueeze(0)], 0)\n",
        "      # else:\n",
        "      #     next_states_challenge = torch.cat([next_states_challenge, next_state.unsqueeze(0)], 0)\n",
        "\n",
        "      # --- Card Network ---\n",
        "      # Assuming 'all_discard_piles' contains discard piles for each state\n",
        "      # and 'all_bot0_cards_embedded' contains Bot 0's cards for each state\n",
        "\n",
        "      # current_discard_pile_size = len(all_discard_piles[i])\n",
        "      # next_discard_pile_size = len(all_discard_piles[i + 1])\n",
        "\n",
        "      # Check if Bot 0 lost a card in the transition\n",
        "      bot0_current_cards = state_card[i][7:9]\n",
        "      bot0_next_cards    = state_card[i+1][7:9]\n",
        "      if not torch.equal(bot0_current_cards, bot0_next_cards):\n",
        "\n",
        "          states_card = torch.cat([states_card, current_state_card.unsqueeze(0)], 0)\n",
        "      # else:\n",
        "      #     next_states_card = torch.cat([next_states_card, next_state.unsqueeze(0)], 0)\n",
        "\n",
        "\n",
        "  # print(state.shape)\n",
        "\n",
        "  # print(states_action.shape)\n",
        "  # print(next_states_action.shape)\n",
        "  # print(len(states_action) + len(next_states_action))\n",
        "\n",
        "  # print(states_block.shape)\n",
        "  # print(next_states_block.shape)\n",
        "  # print(len(states_block) + len(next_states_block))\n",
        "\n",
        "  # print(states_challenge.shape)\n",
        "  # print(next_states_challenge.shape)\n",
        "  # print(len(states_challenge) + len(next_states_challenge))\n",
        "\n",
        "  # print(states_card.shape)\n",
        "  # print(next_states_card.shape)\n",
        "  # print(len(states_card) + len(next_states_card))\n",
        "\n",
        "\n",
        "  losses_action = []\n",
        "  losses_block = []\n",
        "  losses_challenge = []\n",
        "  losses_card = []\n",
        "\n",
        "  ############################################\n",
        "# Example: On-the-fly building raw sequences\n",
        "#          and passing them into the RNN Q\n",
        "############################################\n",
        "  num_batches_action = len(states_action) // batch_size\n",
        "\n",
        "  # for i in range(num_batches_action):\n",
        "  # -------------------------------------------------\n",
        "  # 1) Sample a batch of indices\n",
        "  # -------------------------------------------------\n",
        "  batch_indices_action = random.sample(\n",
        "      range(len(states_action)),\n",
        "      min(batch_size, len(states_action))\n",
        "  )\n",
        "\n",
        "  batch_states_action = torch.stack([states_action[j] for j in batch_indices_action])\n",
        "  batch_actions_main  = actions_main[batch_indices_action]  # shape (batch_size,)\n",
        "  # (Optional) Make sure batch_actions_main is torch.LongTensor\n",
        "  batch_actions_main  = torch.tensor(batch_actions_main, dtype=torch.long)\n",
        "\n",
        "  # Lists to store Q-values we compute for each item in this mini-batch\n",
        "  q_values_current_list = []\n",
        "  q_values_nextmax_list = []\n",
        "  rewards_list          = []\n",
        "  done_list             = []\n",
        "\n",
        "  # -------------------------------------------------\n",
        "  # 2) Process each single-frame \"state\" in the batch\n",
        "  #    to build the raw sequences for current & next\n",
        "  # -------------------------------------------------\n",
        "  for idx, single_frame_state in enumerate(batch_states_action):\n",
        "      # ------------------\n",
        "      # Find global index\n",
        "      # ------------------\n",
        "      indices = torch.where((state == single_frame_state).all(dim=1))[0]\n",
        "      if len(indices) == 0:\n",
        "          # We did not find it => treat as terminal\n",
        "          # => Q(s) is “whatever”, we can do 0\n",
        "          # => Or we skip, but let's just do zero and done=1\n",
        "          q_values_current_list.append(torch.tensor(0.0))  # single float\n",
        "          q_values_nextmax_list.append(torch.tensor(0.0))\n",
        "          rewards_list.append(torch.tensor(0.0))\n",
        "          done_list.append(torch.tensor(1.0))\n",
        "          continue\n",
        "\n",
        "      state_index = indices[0].item()\n",
        "\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      # (a) Build the \"current\" sequence: from last done to now\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      cur_seq_frames = []\n",
        "      back_index = state_index\n",
        "      while back_index >= 0 and done[back_index] != 1:\n",
        "          cur_seq_frames.insert(0, state[back_index])  # front => chronological\n",
        "          back_index -= 1\n",
        "      # shape => (seq_len, 12)\n",
        "      # try:\n",
        "      cur_seq_tensor = torch.stack(cur_seq_frames, dim=0)\n",
        "      # except:\n",
        "      #   cur_seq_tensor = torch.stack(single_frame_state, dim=0)\n",
        "      # shape => (1, seq_len, 12)\n",
        "      cur_seq_tensor = cur_seq_tensor.unsqueeze(0)\n",
        "\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      # (b) Build the \"next\" sequence\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      next_seq_frames = []\n",
        "      rewards_for_seq = []\n",
        "      current_index = state_index + 1\n",
        "      while (current_index < len(state)):\n",
        "          next_seq_frames.append(state[current_index])\n",
        "          rewards_for_seq.append(rewards[current_index])\n",
        "          if done[current_index] == 1:\n",
        "              break\n",
        "          current_index += 1\n",
        "\n",
        "      # We will decide how to handle \"no next frames\"\n",
        "      if len(next_seq_frames) == 0:\n",
        "          # Means terminal or no next chunk\n",
        "          next_seq_tensor = None  # we'll handle it below\n",
        "      else:\n",
        "          next_seq_tensor = torch.stack(next_seq_frames, dim=0)\n",
        "          next_seq_tensor = next_seq_tensor.unsqueeze(0)  # shape (1, seq_len2, 12)\n",
        "\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      # (c) Forward pass for the current sequence\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      # shape => (1, action_dim) if batch_size=1\n",
        "      out_current = bot.action_q(cur_seq_tensor)\n",
        "      # We gather the chosen action’s Q\n",
        "      chosen_action = batch_actions_main[idx]  # a single int\n",
        "      q_val_current = out_current[0, chosen_action]  # shape => scalar\n",
        "\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      # (d) Forward pass for the next sequence (if it exists)\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      if next_seq_tensor is None:\n",
        "          # Terminal. Let's define next_max_Q = 0\n",
        "          q_val_nextmax = torch.tensor(0.0)\n",
        "          # Reward from the first step after current_index if in bounds\n",
        "          if current_index < len(rewards):\n",
        "              r = rewards[current_index]\n",
        "          else:\n",
        "              r = 0.0\n",
        "          done_flag = 1.0\n",
        "      else:\n",
        "          out_next = bot.action_q(next_seq_tensor)        # shape => (1, action_dim)\n",
        "          q_val_nextmax = out_next.max(dim=1)[0].squeeze() # scalar\n",
        "          # Reward is average of rewards_for_seq\n",
        "          if len(rewards_for_seq) > 0:\n",
        "              r = sum(rewards_for_seq) / len(rewards_for_seq)\n",
        "          else:\n",
        "              r = 0.0\n",
        "          # Are we done? If in range, check done[current_index]\n",
        "          if current_index < len(done):\n",
        "              done_flag = float(done[current_index])\n",
        "          else:\n",
        "              done_flag = 1.0\n",
        "\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      # (e) Collect everything in lists\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      q_values_current_list.append(q_val_current)\n",
        "      q_values_nextmax_list.append(q_val_nextmax)\n",
        "      rewards_list.append(torch.tensor(r, dtype=torch.float32))\n",
        "      done_list.append(torch.tensor(done_flag, dtype=torch.float32))\n",
        "\n",
        "  # -------------------------------------------------\n",
        "  # 3) Convert results to Tensors\n",
        "  #    so we can do a standard DQN loss\n",
        "  # -------------------------------------------------\n",
        "  # shape => (batch_size,)\n",
        "  q_current_t     = torch.stack(q_values_current_list)  # current Q chosen action\n",
        "  q_nextmax_t     = torch.stack(q_values_nextmax_list)\n",
        "  rewards_t       = torch.stack(rewards_list)\n",
        "  done_t          = torch.stack(done_list)\n",
        "\n",
        "  # DQN target\n",
        "  target_q = rewards_t + gamma * q_nextmax_t * (1.0 - done_t)\n",
        "\n",
        "  # -------------------------------------------------\n",
        "  # 4) Compute loss & backprop\n",
        "  # -------------------------------------------------\n",
        "  loss_action = criterion(q_current_t, target_q)\n",
        "  bot.optimizer_action.zero_grad()\n",
        "  loss_action.backward()\n",
        "  bot.optimizer_action.step()\n",
        "\n",
        "  # -------------------------------------------------\n",
        "  # 5) Remove used transitions from replay\n",
        "  #    (Optional, as in your original code)\n",
        "  # -------------------------------------------------\n",
        "  # states_action_np = states_action.cpu().numpy()\n",
        "  # actions_main_np  = actions_main.cpu().numpy()\n",
        "\n",
        "  # states_action_np = np.delete(states_action_np, batch_indices_action, axis=0)\n",
        "  # actions_main_np  = np.delete(actions_main_np,  batch_indices_action, axis=0)\n",
        "\n",
        "  # states_action = torch.tensor(states_action_np, dtype=torch.float32)\n",
        "  # actions_main  = torch.tensor(actions_main_np,  dtype=torch.int64)\n",
        "\n",
        "  losses_action.append(loss_action.item())\n",
        "\n",
        "\n",
        "    # print('action success')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  ###############################################################################\n",
        "# RNN-based DQN training loop for your \"block\" decision, without a summarizer\n",
        "###############################################################################\n",
        "  num_batches_block = len(states_block) // batch_size\n",
        "\n",
        "  # for i in range(num_batches_block):\n",
        "  # 1) Sample a random batch of indices\n",
        "  batch_indices_block = random.sample(\n",
        "      range(len(states_block)),\n",
        "      min(batch_size, len(states_block))\n",
        "  )\n",
        "\n",
        "  # 2) Gather Tensors for this mini-batch\n",
        "  batch_states_block  = torch.stack([states_block[j] for j in batch_indices_block])\n",
        "  batch_actions_block = actions_block[batch_indices_block]  # shape (batch_size,)\n",
        "  # Make sure actions are long-int for gather\n",
        "  batch_actions_block = torch.tensor(batch_actions_block, dtype=torch.long)\n",
        "\n",
        "  # Lists to store results for each item in the batch\n",
        "  q_values_current_list = []\n",
        "  q_values_nextmax_list = []\n",
        "  rewards_list          = []\n",
        "  done_list             = []\n",
        "\n",
        "  # ------------------------------------------------------------------\n",
        "  # 3) For each single-frame in batch_states_block:\n",
        "  #    (a) Build \"current\" sequence (backwards)\n",
        "  #    (b) Build \"next\" sequence (forwards)\n",
        "  #    (c) Forward pass each through RNN\n",
        "  #    (d) Collect chosen-action Q, next-max Q, reward, and done\n",
        "  # ------------------------------------------------------------------\n",
        "  for idx, single_frame_state in enumerate(batch_states_block):\n",
        "      # 3.1) Find the global index in your big `state` array\n",
        "      indices = torch.where((state_block == single_frame_state).all(dim=1))[0]\n",
        "\n",
        "      if len(indices) == 0:\n",
        "          # If we didn't find it, treat as terminal\n",
        "          q_values_current_list.append(torch.tensor(0.0))\n",
        "          q_values_nextmax_list.append(torch.tensor(0.0))\n",
        "          rewards_list.append(torch.tensor(0.0))\n",
        "          done_list.append(torch.tensor(1.0))\n",
        "          continue\n",
        "\n",
        "      state_index = indices[0].item()\n",
        "\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      # (a) Build \"current\" sequence by going backward until done\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      cur_seq_frames = []\n",
        "      back_index = state_index\n",
        "      while back_index >= 0 and done[back_index] != 1:\n",
        "          cur_seq_frames.insert(0, state_block[back_index])  # front => chronological\n",
        "          back_index -= 1\n",
        "      # If we ended up with an empty chunk, fallback to single frame\n",
        "      if len(cur_seq_frames) == 0:\n",
        "          cur_seq_frames.append(state_block[state_index])\n",
        "\n",
        "      # shape: (seq_len, 12)\n",
        "      cur_seq_tensor = torch.stack(cur_seq_frames, dim=0)\n",
        "      # shape: (1, seq_len, 12) for the RNN\n",
        "      cur_seq_tensor = cur_seq_tensor.unsqueeze(0)\n",
        "\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      # (b) Build \"next\" sequence by going forward until done or not reacting\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      next_seq_frames = []\n",
        "      rewards_for_seq = []\n",
        "      current_idx = state_index + 1\n",
        "\n",
        "      while (current_idx < len(state)):\n",
        "          next_seq_frames.append(state_block[current_idx])\n",
        "          rewards_for_seq.append(rewards[current_idx])\n",
        "          if done[current_idx] == 1:\n",
        "              break\n",
        "          current_idx += 1\n",
        "\n",
        "      # We will define \"no next frames\" => terminal\n",
        "      if len(next_seq_frames) == 0:\n",
        "          next_seq_tensor = None\n",
        "      else:\n",
        "          next_seq_tensor = torch.stack(next_seq_frames, dim=0)\n",
        "          next_seq_tensor = next_seq_tensor.unsqueeze(0)  # (1, seq_len2, 12)\n",
        "\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      # (c) Forward pass: current sequence\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      out_current = bot.block_q(cur_seq_tensor)  # shape => (1, num_actions)\n",
        "      chosen_action = batch_actions_block[idx]\n",
        "      q_val_current = out_current[0, chosen_action]  # => scalar\n",
        "\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      # (d) Forward pass: next sequence (if exists)\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      if next_seq_tensor is None:\n",
        "          # Terminal\n",
        "          q_val_nextmax = torch.tensor(0.0)\n",
        "          # Reward\n",
        "          if current_idx < len(rewards):\n",
        "              r = rewards[current_idx]\n",
        "          else:\n",
        "              r = 0.0\n",
        "          done_flag = 1.0\n",
        "      else:\n",
        "          out_next = bot.block_q(next_seq_tensor)         # => (1, num_actions)\n",
        "          q_val_nextmax = out_next.max(dim=1)[0].squeeze()  # => scalar\n",
        "\n",
        "          # Reward = average (or sum) across the forward chunk\n",
        "          if len(rewards_for_seq) > 0:\n",
        "              r = sum(rewards_for_seq) / len(rewards_for_seq)\n",
        "          else:\n",
        "              r = 0.0\n",
        "\n",
        "          # Done or not\n",
        "          if current_idx < len(done):\n",
        "              done_flag = float(done[current_idx])\n",
        "          else:\n",
        "              done_flag = 1.0\n",
        "\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      # (e) Store in lists\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      q_values_current_list.append(q_val_current)\n",
        "      q_values_nextmax_list.append(q_val_nextmax)\n",
        "      rewards_list.append(torch.tensor(r, dtype=torch.float32))\n",
        "      done_list.append(torch.tensor(done_flag, dtype=torch.float32))\n",
        "\n",
        "  # ------------------------------------------------------------------\n",
        "  # 4) Convert the results to Tensors for a standard DQN update\n",
        "  # ------------------------------------------------------------------\n",
        "  q_current_t = torch.stack(q_values_current_list)   # shape (batch_size,)\n",
        "  q_nextmax_t = torch.stack(q_values_nextmax_list)   # shape (batch_size,)\n",
        "  rewards_t   = torch.stack(rewards_list)            # shape (batch_size,)\n",
        "  done_t      = torch.stack(done_list)               # shape (batch_size,)\n",
        "\n",
        "  target_q = rewards_t + gamma * q_nextmax_t * (1.0 - done_t)\n",
        "\n",
        "  # ------------------------------------------------------------------\n",
        "  # 5) Compute the loss & backprop\n",
        "  # ------------------------------------------------------------------\n",
        "  loss_block = criterion(q_current_t, target_q)\n",
        "  bot.optimizer_block.zero_grad()\n",
        "  loss_block.backward()\n",
        "  bot.optimizer_block.step()\n",
        "\n",
        "  # ------------------------------------------------------------------\n",
        "  # 6) Remove these samples from your replay buffer (optional)\n",
        "  # ------------------------------------------------------------------\n",
        "  # Convert Tensors to NumPy for deletion\n",
        "  # states_block_np  = states_block.cpu().numpy()\n",
        "  # actions_block_np = actions_block.cpu().numpy()\n",
        "\n",
        "  # states_block_np  = np.delete(states_block_np,  batch_indices_block, axis=0)\n",
        "  # actions_block_np = np.delete(actions_block_np, batch_indices_block, axis=0)\n",
        "\n",
        "  # states_block  = torch.tensor(states_block_np,  dtype=torch.float32)\n",
        "  # actions_block = torch.tensor(actions_block_np, dtype=torch.int64)\n",
        "\n",
        "  # If you have next_states_block or others, remove them similarly...\n",
        "  # next_states_block = np.delete(...)\n",
        "\n",
        "  losses_block.append(loss_block.item())\n",
        "\n",
        "\n",
        "    # print('block success')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  ##############################################################################\n",
        "# RNN-based DQN loop for your \"challenge\" decision, without a separate summarizer\n",
        "##############################################################################\n",
        "\n",
        "  num_batches_challenge = len(states_challenge) // batch_size\n",
        "\n",
        "  # for i in range(num_batches_challenge):\n",
        "  # 1) Sample a batch of indices\n",
        "  batch_indices_challenge = random.sample(\n",
        "      range(len(states_challenge)),\n",
        "      min(batch_size, len(states_challenge))\n",
        "  )\n",
        "\n",
        "  # 2) Gather the states & actions for this mini-batch\n",
        "  batch_states_challenge  = torch.stack([states_challenge[j] for j in batch_indices_challenge])\n",
        "  batch_actions_challenge = actions_challenge[batch_indices_challenge]\n",
        "  batch_actions_challenge = torch.tensor(batch_actions_challenge, dtype=torch.long)\n",
        "\n",
        "  # Lists to store results for the DQN update\n",
        "  q_values_current_list = []\n",
        "  q_values_nextmax_list = []\n",
        "  rewards_list          = []\n",
        "  done_list             = []\n",
        "\n",
        "  # ----------------------------------------------------------------\n",
        "  # 3) For each single-frame in batch_states_challenge:\n",
        "  #    - Build backward-chunk (current)\n",
        "  #    - Build forward-chunk (next) per your challenge logic\n",
        "  #    - RNN forward pass -> Q-values\n",
        "  # ----------------------------------------------------------------\n",
        "  for idx, single_frame_state in enumerate(batch_states_challenge):\n",
        "      # Try to locate this state in the global `state` array\n",
        "      indices = torch.where((state_challenge == single_frame_state).all(dim=1))[0]\n",
        "\n",
        "      if len(indices) == 0:\n",
        "          # If not found, treat as terminal\n",
        "          # Q(current) = 0, Q(next_max) = 0, reward=0, done=1\n",
        "          q_values_current_list.append(torch.tensor(0.0))\n",
        "          q_values_nextmax_list.append(torch.tensor(0.0))\n",
        "          rewards_list.append(torch.tensor(0.0))\n",
        "          done_list.append(torch.tensor(1.0))\n",
        "          continue\n",
        "\n",
        "      state_index = indices[0].item()\n",
        "\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      # 3A) Build the backward-chunk for the current state\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      cur_seq_frames = []\n",
        "      back_index = state_index\n",
        "      # Move backward until we hit done=1 or index < 0\n",
        "      # (You could also incorporate more challenge-specific conditions\n",
        "      # if you want symmetrical logic with forward-chunk.)\n",
        "      while back_index >= 0 and done[back_index] != 1:\n",
        "          cur_seq_frames.insert(0, state_challenge[back_index])  # front => chronological order\n",
        "          back_index -= 1\n",
        "\n",
        "      if len(cur_seq_frames) == 0:\n",
        "          # fallback to single frame if we got nothing\n",
        "          cur_seq_frames.append(state_challenge[state_index])\n",
        "\n",
        "      # Shape: (seq_len, 12)\n",
        "      cur_seq_tensor = torch.stack(cur_seq_frames, dim=0)\n",
        "      # Shape: (1, seq_len, 12) for the RNN\n",
        "      cur_seq_tensor = cur_seq_tensor.unsqueeze(0)\n",
        "\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      # 3B) Build the forward-chunk for the next state\n",
        "      #     Use your \"challenge\" condition:\n",
        "      #         reacting_players[idx] != 0\n",
        "      #         AND (acting_players[idx] != 0 OR reactions_game[idx] != 1)\n",
        "      #         AND done[idx] != 1\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      next_seq_frames = []\n",
        "      rewards_for_seq = []\n",
        "      current_idx     = state_index + 1\n",
        "\n",
        "      while (\n",
        "          current_idx < len(state)):\n",
        "          next_seq_frames.append(state_challenge[current_idx])\n",
        "          rewards_for_seq.append(rewards[current_idx])\n",
        "          if done[current_idx] == 1:\n",
        "              break\n",
        "          current_idx += 1\n",
        "\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      # RNN Forward pass for the \"current\" sequence\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      out_current = bot.challenge_q(cur_seq_tensor)  # shape (1, num_actions)\n",
        "      chosen_action = batch_actions_challenge[idx]\n",
        "      q_val_current = out_current[0, chosen_action]   # => scalar\n",
        "\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      # RNN Forward pass for the \"next\" sequence\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      q_val_nextmax = torch.tensor(0.0)\n",
        "      r = 0.0\n",
        "      done_flag = 1.0\n",
        "      if len(next_seq_frames) == 0:\n",
        "          # Terminal\n",
        "          q_val_nextmax = torch.tensor(0.0)\n",
        "          # Reward:\n",
        "          if current_idx < len(rewards):\n",
        "              r = rewards[current_idx]\n",
        "          else:\n",
        "              r = 0.0\n",
        "          done_flag = 1.0\n",
        "      else:\n",
        "          # Non-terminal\n",
        "          next_seq_tensor = torch.stack(next_seq_frames, dim=0).unsqueeze(0)\n",
        "          out_next        = bot.challenge_q(next_seq_tensor)   # shape (1, num_actions)\n",
        "          q_val_nextmax   = out_next.max(dim=1)[0].squeeze()   # => scalar\n",
        "\n",
        "          # Reward as average (or adjust logic as you see fit)\n",
        "          if len(rewards_for_seq) > 0:\n",
        "              r = sum(rewards_for_seq) / len(rewards_for_seq)\n",
        "          else:\n",
        "              r = 0.0\n",
        "\n",
        "          if current_idx < len(done):\n",
        "              done_flag = float(done[current_idx])\n",
        "          else:\n",
        "              done_flag = 1.0  # out of bounds => terminal\n",
        "\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      # Collect everything for the DQN update\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      q_values_current_list.append(q_val_current)\n",
        "      q_values_nextmax_list.append(q_val_nextmax)\n",
        "      rewards_list.append(torch.tensor(r, dtype=torch.float32))\n",
        "      done_list.append(torch.tensor(done_flag, dtype=torch.float32))\n",
        "\n",
        "  # ----------------------------------------------------------------\n",
        "  # 4) Convert to Tensors & compute DQN loss\n",
        "  # ----------------------------------------------------------------\n",
        "  q_current_t = torch.stack(q_values_current_list)  # shape (batch_size,)\n",
        "  q_nextmax_t = torch.stack(q_values_nextmax_list)  # shape (batch_size,)\n",
        "  rewards_t   = torch.stack(rewards_list)           # shape (batch_size,)\n",
        "  done_t      = torch.stack(done_list)              # shape (batch_size,)\n",
        "\n",
        "  target_q = rewards_t + gamma * q_nextmax_t * (1.0 - done_t)\n",
        "\n",
        "  loss_challenge = criterion(q_current_t, target_q)\n",
        "\n",
        "  # ----------------------------------------------------------------\n",
        "  # 5) Backprop & optimize\n",
        "  # ----------------------------------------------------------------\n",
        "  bot.optimizer_challenge.zero_grad()\n",
        "  loss_challenge.backward()\n",
        "  bot.optimizer_challenge.step()\n",
        "\n",
        "  # ----------------------------------------------------------------\n",
        "  # 6) Remove these samples from your replay buffer\n",
        "  # ----------------------------------------------------------------\n",
        "  # Convert Tensors -> NumPy for np.delete\n",
        "  # states_challenge_np  = states_challenge.cpu().numpy()\n",
        "  # actions_challenge_np = actions_challenge.cpu().numpy()\n",
        "  # # If you have \"next_states_challenge\" or others, similarly convert them\n",
        "\n",
        "  # states_challenge_np  = np.delete(states_challenge_np,  batch_indices_challenge, axis=0)\n",
        "  # actions_challenge_np = np.delete(actions_challenge_np, batch_indices_challenge, axis=0)\n",
        "\n",
        "  # # Rebuild your PyTorch Tensors\n",
        "  # states_challenge  = torch.tensor(states_challenge_np,  dtype=torch.float32)\n",
        "  # actions_challenge = torch.tensor(actions_challenge_np, dtype=torch.int64)\n",
        "\n",
        "  # If you store next_states_challenge, remove them as well:\n",
        "  # next_states_challenge_np = next_states_challenge.cpu().numpy()\n",
        "  # next_states_challenge_np = np.delete(next_states_challenge_np, batch_indices_challenge, axis=0)\n",
        "  # next_states_challenge = torch.tensor(next_states_challenge_np, dtype=torch.float32)\n",
        "\n",
        "  losses_challenge.append(loss_challenge.item())\n",
        "\n",
        "\n",
        "    # print('challenge success')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  ##############################################################################\n",
        "# RNN-based training loop for your \"card\" decision,\n",
        "# building raw backward and forward sequences on-the-fly\n",
        "##############################################################################\n",
        "  num_batches_card = len(states_card) // batch_size\n",
        "\n",
        "  if num_batches_card == 0:\n",
        "    raise Exception(\"Something went wrong\")\n",
        "\n",
        "  # for i in range(num_batches_card):\n",
        "\n",
        "  # 1) Sample batch indices\n",
        "  batch_indices_card = random.sample(\n",
        "      range(len(states_card)),\n",
        "      min(batch_size, len(states_card))\n",
        "  )\n",
        "\n",
        "  # 2) Gather states & actions for this batch\n",
        "  batch_states_card  = torch.stack([states_card[j] for j in batch_indices_card])  # (batch_size, 12)\n",
        "  batch_actions_card = actions_card[batch_indices_card]                           # (batch_size,)\n",
        "  batch_actions_card = torch.tensor(batch_actions_card, dtype=torch.long)\n",
        "\n",
        "  # We'll collect Q-values and targets for the entire batch\n",
        "  q_current_list = []\n",
        "  q_nextmax_list = []\n",
        "  reward_list    = []\n",
        "  done_list      = []\n",
        "\n",
        "  # ------------------------------------------------------------------------\n",
        "  # 3) For each single-frame in batch_states_card, build backward & forward\n",
        "  # ------------------------------------------------------------------------\n",
        "  for idx, single_frame_state in enumerate(batch_states_card):\n",
        "      # Locate this state in the global 'state' buffer\n",
        "      indices = torch.where((state_card == single_frame_state).all(dim=1))[0]\n",
        "      if len(indices) == 0:\n",
        "          # Not found => treat as terminal\n",
        "          # Q(current) = 0, Q(next) = 0, reward=0, done=1\n",
        "          q_current_list.append(torch.tensor(0.0))\n",
        "          q_nextmax_list.append(torch.tensor(0.0))\n",
        "          reward_list.append(torch.tensor(0.0))\n",
        "          done_list.append(torch.tensor(1.0))\n",
        "          continue\n",
        "\n",
        "      state_index = indices[0].item()\n",
        "\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      # 3A) BACKWARD chunk for \"current\" state\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      cur_seq_frames = []\n",
        "      back_idx = state_index\n",
        "\n",
        "      # Move backward until done=1 or out of array\n",
        "      while back_idx >= 0 and done[back_idx] != 1:\n",
        "          cur_seq_frames.insert(0, state_card[back_idx])  # insert at front => chronological order\n",
        "          back_idx -= 1\n",
        "\n",
        "      if len(cur_seq_frames) == 0:\n",
        "          # fallback to just the single frame\n",
        "          cur_seq_frames.append(state_card[state_index])\n",
        "\n",
        "      # Turn into shape (1, seq_len, 12) for the RNN\n",
        "      cur_seq_tensor = torch.stack(cur_seq_frames, dim=0).unsqueeze(0)\n",
        "\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      # 3B) FORWARD chunk for \"next\" state, until:\n",
        "      #     - done=1\n",
        "      #     - Bot 0's cards change (state[..., 7:9] differs from next step)\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      fwd_seq_frames = []\n",
        "      rewards_for_seq = []\n",
        "      current_idx = state_index + 1\n",
        "\n",
        "      while current_idx < len(state):\n",
        "\n",
        "          # Append the current frame\n",
        "          fwd_seq_frames.append(state_card[current_idx])\n",
        "          rewards_for_seq.append(rewards[current_idx])\n",
        "\n",
        "          # Check if the *next* step changes Bot 0's cards\n",
        "          # so we break *after* including the current frame if it\n",
        "          # leads to a change in [7:9].\n",
        "          # if current_idx + 1 < len(state):\n",
        "          #     # Compare the slice [7:9] of the current vs. the next\n",
        "          #     bot0_current_cards = state_card[current_idx][7:9]\n",
        "          #     bot0_next_cards    = state_card[current_idx+1][7:9]\n",
        "          #     if not torch.equal(bot0_current_cards, bot0_next_cards):\n",
        "          #         # Bot 0's cards changed => break\n",
        "          #         current_idx += 1  # increment so we include the reward\n",
        "          #         break\n",
        "\n",
        "          # Stop if done=1\n",
        "          if done[current_idx] == 1:\n",
        "              break\n",
        "\n",
        "          # If no change, keep going\n",
        "          current_idx += 1\n",
        "\n",
        "          # Also break if we’ve run out of array (the while condition checks that anyway)\n",
        "          # but we’ll rely on the loop condition.\n",
        "\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      # Forward pass in the RNN for the \"current\" sequence\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      out_current = bot.card_q(cur_seq_tensor)     # shape: (1, num_actions)\n",
        "      # pick the Q-value for the chosen action\n",
        "      chosen_action = batch_actions_card[idx]\n",
        "      q_val_current = out_current[0, chosen_action] # => scalar\n",
        "\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      # Forward pass for the \"next\" sequence\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      if len(fwd_seq_frames) == 0:\n",
        "          # Terminal if we got no next frames\n",
        "          q_val_nextmax = torch.tensor(0.0)\n",
        "          # Reward\n",
        "          if current_idx < len(state):\n",
        "              r = rewards[current_idx]\n",
        "          else:\n",
        "              r = 0.0\n",
        "          done_flag = 1.0\n",
        "      else:\n",
        "          # Non-terminal\n",
        "          fwd_seq_tensor = torch.stack(fwd_seq_frames, dim=0).unsqueeze(0)\n",
        "          out_next       = bot.card_q(fwd_seq_tensor)  # shape (1, num_actions)\n",
        "          q_val_nextmax  = out_next.max(dim=1)[0].squeeze()      # => scalar\n",
        "\n",
        "          # Could do average or sum of rewards\n",
        "          if len(rewards_for_seq) > 0:\n",
        "              r = sum(rewards_for_seq) / len(rewards_for_seq)\n",
        "          else:\n",
        "              r = 0.0\n",
        "\n",
        "          # done flag\n",
        "          if current_idx < len(done):\n",
        "              done_flag = float(done[current_idx])\n",
        "          else:\n",
        "              done_flag = 1.0\n",
        "\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      # Collect results for the DQN update\n",
        "      # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "      q_current_list.append(q_val_current)\n",
        "      q_nextmax_list.append(q_val_nextmax)\n",
        "      reward_list.append(torch.tensor(r, dtype=torch.float32))\n",
        "      done_list.append(torch.tensor(done_flag, dtype=torch.float32))\n",
        "\n",
        "  # ------------------------------------------------------------------\n",
        "  # 4) Convert to Tensors & compute the DQN target\n",
        "  # ------------------------------------------------------------------\n",
        "  q_current_t = torch.stack(q_current_list)   # (batch_size,)\n",
        "  q_nextmax_t = torch.stack(q_nextmax_list)   # (batch_size,)\n",
        "  reward_t    = torch.stack(reward_list)      # (batch_size,)\n",
        "  done_t      = torch.stack(done_list)        # (batch_size,)\n",
        "\n",
        "  # DQN target: r + gamma * max(Q(next)) * (1 - done)\n",
        "  target_q = reward_t + gamma * q_nextmax_t * (1.0 - done_t)\n",
        "\n",
        "  # ------------------------------------------------------------------\n",
        "  # 5) Compute loss & optimize\n",
        "  # ------------------------------------------------------------------\n",
        "  loss_card = criterion(q_current_t, target_q)\n",
        "\n",
        "  bot.optimizer_card.zero_grad()\n",
        "  loss_card.backward()\n",
        "  bot.optimizer_card.step()\n",
        "\n",
        "  # ------------------------------------------------------------------\n",
        "  # 6) Remove these samples from the replay buffer\n",
        "  # ------------------------------------------------------------------\n",
        "  # states_card_np   = states_card.cpu().numpy()\n",
        "  # actions_card_np  = actions_card.cpu().numpy()\n",
        "  # # If you have next_states_card, similarly convert & remove\n",
        "\n",
        "  # states_card_np   = np.delete(states_card_np,  batch_indices_card, axis=0)\n",
        "  # actions_card_np  = np.delete(actions_card_np, batch_indices_card, axis=0)\n",
        "\n",
        "  # states_card  = torch.tensor(states_card_np, dtype=torch.float32)\n",
        "  # actions_card = torch.tensor(actions_card_np, dtype=torch.int64)\n",
        "\n",
        "  losses_card.append(loss_card.item())\n",
        "\n",
        "\n",
        "\n",
        "  # bot.cards = bots[0].cards\n",
        "  # bot.num_coins = bots[0].num_coins\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # i += 1\n",
        "  epsilon *= 0.995\n",
        "\n",
        "  # if (episode + 1) % 100 == 0:\n",
        "  #   data_fraction = min(data_fraction + 1/5, 1)\n",
        "  #   epsilon = 1.0\n",
        "\n",
        "  # gamma = min(0.99, gamma + 0.001)\n",
        "\n",
        "  avg_losses_action.append(sum(losses_action) / len(losses_action))\n",
        "  avg_losses_block.append(sum(losses_block) / len(losses_block))\n",
        "  avg_losses_challenge.append(sum(losses_challenge) / len(losses_challenge))\n",
        "  avg_losses_card.append(sum(losses_card) / len(losses_card))\n",
        "\n",
        "  print(f'Avg Action Loss, {num_batches_action} batches: {avg_losses_action[-1]}')\n",
        "  print(f'Avg Block Loss, {num_batches_block} batches: {avg_losses_block[-1]}')\n",
        "  print(f'Avg Challenge Loss, {num_batches_challenge} batches: {avg_losses_challenge[-1]}')\n",
        "  print(f'Avg Card Loss, {num_batches_card} batches: {avg_losses_card[-1]}')\n",
        "\n",
        "  # Copy parameters of action_q network\n",
        "  bots[0].action_q.load_state_dict(bot.action_q.state_dict())\n",
        "\n",
        "  # Copy parameters of block_q network\n",
        "  bots[0].block_q.load_state_dict(bot.block_q.state_dict())\n",
        "\n",
        "  # Copy parameters of challenge_q network\n",
        "  bots[0].challenge_q.load_state_dict(bot.challenge_q.state_dict())\n",
        "\n",
        "  # Copy parameters of card_q network\n",
        "  bots[0].card_q.load_state_dict(bot.card_q.state_dict())\n",
        "\n",
        "  def verify_allclose(net_a, net_b, eps=1e-6):\n",
        "    for p_a, p_b in zip(net_a.parameters(), net_b.parameters()):\n",
        "        if not torch.allclose(p_a, p_b, atol=eps, rtol=1e-5):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "  print(\"Action Q matches?\",\n",
        "        verify_allclose(bots[0].action_q, bot.action_q))\n",
        "  print(\"Block Q matches?\",\n",
        "        verify_allclose(bots[0].block_q, bot.block_q))\n",
        "  print(\"Challenge Q matches?\",\n",
        "        verify_allclose(bots[0].challenge_q, bot.challenge_q))\n",
        "  print(\"Card Q matches?\",\n",
        "        verify_allclose(bots[0].card_q, bot.card_q))\n",
        "\n",
        "\n",
        "  print(bots[0].name)\n",
        "\n",
        "  if episode % 10 == 0:\n",
        "\n",
        "    win_rate = 0\n",
        "\n",
        "    bot0_actions = []\n",
        "\n",
        "    game_lengths = 0\n",
        "\n",
        "    for i in range(100):\n",
        "\n",
        "      discard_pile, acting_player, reacting_player, current_player, action_game, reaction_game, challenge_game, card_game, coin_game, challenge_direction, done_0, reward, card_chosen, bots_copy = game_loop_random(bots, actions, influences_reverse, 0.0)\n",
        "      bots = bots_copy\n",
        "\n",
        "      game_lengths += len(action_game)\n",
        "\n",
        "      for actor, action in zip(acting_player, action_game):\n",
        "            if actor == 0:\n",
        "                bot0_actions.append(action)\n",
        "\n",
        "      if reward[-1] == 1.0:\n",
        "        win_rate += 1\n",
        "\n",
        "    # win_rate = win_rate / 50\n",
        "    # print(f'Bot 0 Win Rate, Random Actions: {win_rate / 100}')\n",
        "\n",
        "    if bot0_actions:  # ensure the list is not empty\n",
        "        counter = Counter(bot0_actions)\n",
        "        most_common_action, count = counter.most_common(1)[0]\n",
        "        print(f'Most common action for Bot 0: {most_common_action} taken {count} times.')\n",
        "        print(f'total game lengths: {game_lengths}')\n",
        "\n",
        "    win_rate = win_rate / 100\n",
        "    win_rates.append(win_rate)\n",
        "\n",
        "    print(f'win rate: {win_rate}')\n",
        "\n",
        "\n",
        "  # df = pd.DataFrame(data = data)\n",
        "  # print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Dky8U78hbAps",
      "metadata": {
        "id": "Dky8U78hbAps"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(avg_losses_action, label='Avg Action Loss')\n",
        "plt.plot(avg_losses_block, label='Avg Block Loss')\n",
        "plt.plot(avg_losses_challenge, label='Avg Challenge Loss')\n",
        "plt.plot(avg_losses_card, label='Avg Card Loss')\n",
        "\n",
        "plt.xlabel('Episode/Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Losses')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vKOENygAg-jY",
      "metadata": {
        "id": "vKOENygAg-jY"
      },
      "outputs": [],
      "source": [
        "plt.plot(win_rates, label='Win Rate')\n",
        "\n",
        "plt.xlabel('Episode/Iteration')\n",
        "plt.ylabel('Win Rate')\n",
        "plt.title('Win Rate over Time')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mQYFQF_TiYhE",
      "metadata": {
        "id": "mQYFQF_TiYhE"
      },
      "outputs": [],
      "source": [
        "plt.plot(avg_game_lengths, label='Avg Game Lengths')\n",
        "\n",
        "plt.xlabel('Episode/Iteration')\n",
        "plt.ylabel('Avg Game Length (timesteps)')\n",
        "plt.title('Avg Game Length over Time')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RttgbXIgzFfB",
      "metadata": {
        "id": "RttgbXIgzFfB"
      },
      "outputs": [],
      "source": [
        "torch.save({\n",
        "        'action_q_state_dict': bot.action_q.state_dict(),\n",
        "        'block_q_state_dict': bot.block_q.state_dict(),\n",
        "        'challenge_q_state_dict': bot.challenge_q.state_dict(),\n",
        "        'card_q_state_dict': bot.card_q.state_dict(),\n",
        "        'optimizer_action_state_dict': bot.optimizer_action.state_dict(),\n",
        "        'optimizer_block_state_dict': bot.optimizer_block.state_dict(),\n",
        "        'optimizer_challenge_state_dict': bot.optimizer_challenge.state_dict(),\n",
        "        'optimizer_card_state_dict': bot.optimizer_card.state_dict()\n",
        "    }, 'bot_parameters.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wh_pxHPuzNGM",
      "metadata": {
        "id": "wh_pxHPuzNGM"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load('bot_parameters.pth')\n",
        "bot.action_q.load_state_dict(checkpoint['action_q_state_dict'])\n",
        "bot.block_q.load_state_dict(checkpoint['block_q_state_dict'])\n",
        "bot.challenge_q.load_state_dict(checkpoint['challenge_q_state_dict'])\n",
        "bot.card_q.load_state_dict(checkpoint['card_q_state_dict'])\n",
        "bot.optimizer_action.load_state_dict(checkpoint['optimizer_action_state_dict'])\n",
        "bot.optimizer_block.load_state_dict(checkpoint['optimizer_block_state_dict'])\n",
        "bot.optimizer_challenge.load_state_dict(checkpoint['optimizer_challenge_state_dict'])\n",
        "bot.optimizer_card.load_state_dict(checkpoint['optimizer_card_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q4YC59Kfkn0B",
      "metadata": {
        "id": "Q4YC59Kfkn0B"
      },
      "outputs": [],
      "source": [
        "bots[0].action_q.load_state_dict(bot.action_q.state_dict())\n",
        "\n",
        "# Copy parameters of block_q network\n",
        "bots[0].block_q.load_state_dict(bot.block_q.state_dict())\n",
        "\n",
        "# Copy parameters of challenge_q network\n",
        "bots[0].challenge_q.load_state_dict(bot.challenge_q.state_dict())\n",
        "\n",
        "# Copy parameters of card_q network\n",
        "bots[0].card_q.load_state_dict(bot.card_q.state_dict())\n",
        "\n",
        "\n",
        "win_rate = 0\n",
        "\n",
        "for i in range(100):\n",
        "\n",
        "  # print(i)\n",
        "  discard_pile, acting_player, reacting_player, current_player, action_game, reaction_game, challenge_game, card_game, coin_game, challenge_direction, done_0, reward, card_chosen, bots_copy = game_loop_random(bots, actions, influences_reverse, 0.0)\n",
        "  bots = bots_copy\n",
        "  if reward[-1] == 1:\n",
        "    win_rate += 1\n",
        "\n",
        "# win_rate = win_rate / 50\n",
        "print(f'Bot 0 Win Rate, Random Actions: {win_rate / 100}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}