{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "17ca9a8f",
      "metadata": {
        "id": "17ca9a8f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "6a2f9eb3",
      "metadata": {
        "id": "6a2f9eb3"
      },
      "outputs": [],
      "source": [
        "actions_map = {\n",
        "    0: 'take 1 coin',\n",
        "    1: 'coup',\n",
        "    2: 'take 2 coins',\n",
        "    3: 'take 3 coins',\n",
        "    4: 'steal 2 coins',\n",
        "    5: 'assassinate',\n",
        "    6: 'exchange',\n",
        "    7: 'challenge',\n",
        "    8: 'block foreign aid',\n",
        "    9: 'block stealing',\n",
        "    10: 'block assassination'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "4d15621e",
      "metadata": {
        "id": "4d15621e"
      },
      "outputs": [],
      "source": [
        "class Action:\n",
        "    def __init__(self, name, challengeable, response_card, response_action,\n",
        "                 p1_net_coins, p2_net_coins, p1_net_cards, p2_net_cards, vector):\n",
        "        self.name = name\n",
        "        self.challengeable = challengeable\n",
        "        self.response_card = response_card\n",
        "        self.response_action = response_action\n",
        "        self.p1_net_coins = p1_net_coins\n",
        "        self.p2_net_coins = p2_net_coins\n",
        "        self.p1_net_cards = p1_net_cards\n",
        "        self.p2_net_cards = p2_net_cards\n",
        "#         self.base_utility = base_utility\n",
        "#         self.p_bluff = p_bluff\n",
        "        self.vector = vector\n",
        "\n",
        "    def update_responses(self, response_card, response_action):\n",
        "        self.response_card = response_card\n",
        "        self.response_action = response_action\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "baef8cbb",
      "metadata": {
        "id": "baef8cbb"
      },
      "outputs": [],
      "source": [
        "take_1 = Action(actions_map[0], False, None, None, 1, 0, 0, 0, [0])\n",
        "\n",
        "coup = Action(actions_map[1], False, None, None, -7, 0, 0, -1, [1])\n",
        "\n",
        "take_2 = Action(actions_map[2], True, 'Duke', actions_map[8], 2, 0, 0, 0, [2])\n",
        "\n",
        "take_3 = Action(actions_map[3], True, None, actions_map[7], 3, 0, 0, 0, [3])\n",
        "\n",
        "steal_2 = Action(actions_map[4], True, ['Captain', 'Ambassador'], actions_map[9], 2, -2, 0, 0,[4])\n",
        "\n",
        "assassinate = Action(actions_map[5], True, 'Contessa', actions_map[10], -3, 0, 0, -1, [5])\n",
        "\n",
        "exchange = Action(actions_map[6], True, None, actions_map[7], 0, 0, 0, 0,[6])\n",
        "\n",
        "# challenge = Action(actions_map[7], False, None, None, 0, 0, -1, -1, 1, 0)\n",
        "\n",
        "block_take_2 = Action(actions_map[8], True, None, actions_map[7], 0, -2, 0, 0, [7])\n",
        "\n",
        "block_steal = Action(actions_map[9], True, None, actions_map[7], 2, -2, 0, 0, [8])\n",
        "\n",
        "block_assassination = Action(actions_map[10], True, None, actions_map[7], 0, 0, 1, 0, [9])\n",
        "\n",
        "# challenge =\n",
        "\n",
        "actions = {\n",
        "    0: take_1,\n",
        "    1: coup,\n",
        "    2: take_2,\n",
        "    3: take_3,\n",
        "    4: steal_2,\n",
        "    5: assassinate,\n",
        "    6: exchange,\n",
        "    7: block_take_2,\n",
        "    8: block_steal,\n",
        "    9: block_assassination\n",
        "}\n",
        "\n",
        "take_2.response_action = actions[7]\n",
        "steal_2.response_action = actions[8]\n",
        "assassinate.response_action = actions[9]\n",
        "\n",
        "influences = {\n",
        "    'Duke': [take_3, block_take_2, take_1, coup],\n",
        "    'Captain': [steal_2, block_steal, take_2, take_1, coup],\n",
        "    'Assassin': [assassinate, take_2, take_1, coup],\n",
        "    'Contessa': [take_2, block_assassination, take_1, coup],\n",
        "    'Ambassador': [exchange, block_steal, take_2, take_1, coup]\n",
        "    }\n",
        "\n",
        "inf_map = {\n",
        "    'Dead': 0,\n",
        "    'Duke': 1,\n",
        "    'Captain': 2,\n",
        "    'Assassin': 3,\n",
        "    'Contessa': 4,\n",
        "    'Ambassador': 5,\n",
        "    'Hidden': 6\n",
        "}\n",
        "\n",
        "\n",
        "influences_reverse = {\n",
        "    take_1: ['Duke', 'Captain', 'Assassin', 'Contessa', 'Ambassador'],\n",
        "    coup: ['Duke', 'Captain', 'Assassin', 'Contessa', 'Ambassador'],\n",
        "    take_2: ['Captain', 'Assassin', 'Contessa', 'Ambassador'],\n",
        "    take_3: ['Duke'],\n",
        "    steal_2: ['Captain'],\n",
        "    assassinate: ['Assassin'],\n",
        "    exchange: ['Ambassador'],\n",
        "    block_take_2: ['Duke'],\n",
        "    block_steal: ['Captain','Ambassador'],\n",
        "    block_assassination: ['Contessa']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "d617da4f",
      "metadata": {
        "id": "d617da4f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size, hidden_size=128):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first = True)\n",
        "        self.fc3 = nn.Linear(hidden_size, 64)\n",
        "        self.fc4 = nn.Linear(64, action_size)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self.fc1(state)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x.unsqueeze(1)\n",
        "        out, _ = self.rnn(x)\n",
        "        x = out.squeeze(1)\n",
        "        x = out[:, -1, :]\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu(x)\n",
        "        return self.fc4(x)\n",
        "\n",
        "embedding_cards = nn.Embedding(6, 1)\n",
        "cards_tens = torch.tensor([0,1,2,3,4,5])\n",
        "cards_emb = embedding_cards(cards_tens)\n",
        "\n",
        "embedding_actions = nn.Embedding(16, 3)\n",
        "actions_tens = torch.tensor([ [0,0,0,0], [1,1,1,1], [2,2,2,2], [3,3,3,3], [4,4,4,4], [5,5,5,5], [6,6,6,6], [7,7,7,7] ])\n",
        "actions_emb = embedding_actions(actions_tens)\n",
        "\n",
        "embedding_coins = nn.Embedding(13, 2)\n",
        "coins_tens = torch.tensor([0,1,2,3,4,5,6,7,8,9,10,11,12])\n",
        "coins_emb = embedding_coins(coins_tens)\n",
        "\n",
        "embedding_players = nn.Embedding(5, 3)\n",
        "players_tens = torch.tensor([0,1,2,3,4])\n",
        "players_emb = embedding_players(players_tens)\n",
        "\n",
        "state_size = 12\n",
        "state_size_card = 16\n",
        "action_size = 16\n",
        "block_size = 2\n",
        "challenge_size = 2\n",
        "card_size = 2\n",
        "\n",
        "criterion = nn.HuberLoss(delta=1.0)\n",
        "learning_rate = 0.001\n",
        "gamma = 0.99\n",
        "epsilon = 1.0\n",
        "epsilon_decay = 0.005\n",
        "min_epsilon = 0.01\n",
        "# batch_size = 64\n",
        "# replay_buffer_size = 10000"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class StateSummarizer(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, embedding_size):\n",
        "        super(StateSummarizer, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, embedding_size)\n",
        "\n",
        "    def forward(self, next_states):\n",
        "        # Initialize hidden state and cell state\n",
        "        h0 = torch.zeros(1, next_states.shape[0], self.hidden_size).to(next_states.device)\n",
        "        c0 = torch.zeros(1, next_states.shape[0], self.hidden_size).to(next_states.device)\n",
        "\n",
        "        # Pass the sequence of next states through the LSTM\n",
        "        out, _ = self.lstm(next_states, (h0, c0))\n",
        "\n",
        "        # print(f'out.shape: {out.shape}')\n",
        "\n",
        "        # Take the last hidden state as the summary\n",
        "        summary = torch.mean(out, dim=1)  # Average across the sequence dimension (dim=1)\n",
        "\n",
        "        # Project the summary to the desired embedding size\n",
        "        embedding = self.fc(summary)\n",
        "        return embedding\n",
        "\n",
        "summarizer = StateSummarizer(12, 64, 12)"
      ],
      "metadata": {
        "id": "EvwXxvtVf4_3"
      },
      "id": "EvwXxvtVf4_3",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "952658b8",
      "metadata": {
        "id": "952658b8"
      },
      "outputs": [],
      "source": [
        "class Bot:\n",
        "    def __init__(self, cards, num_coins, hostility, name, action_q, block_q, challenge_q, card_q,\n",
        "                 optimizer_action, optimizer_block, optimizer_challenge, optimizer_card):\n",
        "        self.cards = cards\n",
        "        self.num_coins = num_coins\n",
        "        self.hostility = hostility\n",
        "        self.name = name\n",
        "        self.action_q = action_q\n",
        "        self.block_q = block_q\n",
        "        self.challenge_q = challenge_q\n",
        "        self.card_q = card_q\n",
        "        self.optimizer_action = optimizer_action\n",
        "        self.optimizer_block = optimizer_block\n",
        "        self.optimizer_challenge = optimizer_challenge\n",
        "        self.optimizer_card = optimizer_card\n",
        "\n",
        "    def num_coins_adj(self, n):\n",
        "        self.num_coins += n\n",
        "\n",
        "    def cards_adj(self, card):\n",
        "        self.cards.remove(card)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "7826ddbf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7826ddbf",
        "outputId": "f315fdaf-64b6-4b40-cb52-7c9d1d3da36e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Captain', 'Assassin']\n",
            "['Ambassador', 'Assassin']\n",
            "['Duke', 'Contessa']\n",
            "['Captain', 'Contessa']\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "bag = ['Duke', 'Captain', 'Assassin', 'Contessa', 'Ambassador'] * 3\n",
        "random.shuffle(bag)\n",
        "\n",
        "bots = []\n",
        "bluff_degree = 0\n",
        "\n",
        "for i in range(4):\n",
        "    cards = random.sample(bag, 2)\n",
        "    for card in cards:\n",
        "        bag.remove(card)\n",
        "#     kb = []\n",
        "\n",
        "    action_q = QNetwork(state_size, action_size)\n",
        "    optimizer_action = optim.Adam(action_q.parameters(), lr=learning_rate)\n",
        "\n",
        "    block_q = QNetwork(state_size, block_size)\n",
        "    optimizer_block = optim.Adam(block_q.parameters(), lr=learning_rate)\n",
        "\n",
        "    challenge_q = QNetwork(state_size, challenge_size)\n",
        "    optimizer_challenge = optim.Adam(challenge_q.parameters(), lr=learning_rate)\n",
        "\n",
        "    card_q = QNetwork(state_size, card_size)\n",
        "    optimizer_card = optim.Adam(card_q.parameters(), lr=learning_rate)\n",
        "\n",
        "    bots.append(Bot(cards, 2, None, f'{i}', action_q, block_q, challenge_q, card_q,\n",
        "                    optimizer_action, optimizer_block, optimizer_challenge, optimizer_card))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for bot in bots:\n",
        "    print(bot.cards)\n",
        "print(bots[0].name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "av5X5iZs2UJQ",
      "metadata": {
        "id": "av5X5iZs2UJQ"
      },
      "outputs": [],
      "source": [
        "replay_buffer = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "TadCAcsPTrIz",
      "metadata": {
        "id": "TadCAcsPTrIz"
      },
      "outputs": [],
      "source": [
        "def get_legal_actions(bot, bots):\n",
        "  \"\"\"Returns a list of legal action indices for the given bot.\"\"\"\n",
        "  legal_actions = []\n",
        "\n",
        "  if bot.num_coins >= 10:\n",
        "    return [1]\n",
        "\n",
        "  # Always legal actions\n",
        "  legal_actions.extend([0, 1, 6])  # Income, Coup, Exchange are always legal\n",
        "\n",
        "  # Conditional actions\n",
        "  if bot.num_coins >= 3:\n",
        "    legal_actions.append(3)  # Foreign Aid\n",
        "  if bot.num_coins >= 7:\n",
        "    legal_actions.append(1)  # Coup\n",
        "  if bot.num_coins >= 3:\n",
        "    legal_actions.append(5)  # Assassinate (if enough coins)\n",
        "\n",
        "  # Actions that target other players\n",
        "  for other_bot in bots:\n",
        "    if other_bot != bot and other_bot.num_coins > 0 : #Can't steal from players with no coins\n",
        "      legal_actions.append(4) #Steal\n",
        "      break  # Only need to add steal once if there's a valid target\n",
        "\n",
        "  return legal_actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "0cc07371",
      "metadata": {
        "id": "0cc07371"
      },
      "outputs": [],
      "source": [
        "def action_selection(i, bots, actions_vector, actions, epsilon, state):\n",
        "\n",
        "    legal_actions = get_legal_actions(bots[i], bots)  # Get list of legal actions\n",
        "\n",
        "    if bots[i].name == 0:\n",
        "      if random.random() >= epsilon:\n",
        "        q_values = bots[i].action_q(state)\n",
        "        max_q_value = float('-inf')\n",
        "        best_action = None\n",
        "        col_index = None  # Initialize col_index\n",
        "\n",
        "        # Iterate through Q-values and find the maximum for legal actions\n",
        "        for action_idx, q_value in enumerate(q_values[0]):  # Iterate with index\n",
        "            if action_idx in legal_actions and q_value.item() > max_q_value:\n",
        "                max_q_value = q_value.item()\n",
        "                best_action = action_idx\n",
        "                # col_index is now the index of best_action in legal_actions\n",
        "                col_index = legal_actions.index(best_action)\n",
        "\n",
        "        if best_action is None:\n",
        "            # Handle the case where no legal actions have Q-values\n",
        "            # This could happen if all legal actions have -inf Q-values\n",
        "            # You might want to choose a random legal action here\n",
        "            best_action = random.choice(legal_actions)\n",
        "            col_index = legal_actions.index(best_action)\n",
        "\n",
        "        return [best_action, col_index]\n",
        "\n",
        "      else:\n",
        "        # Random action selection:\n",
        "        action = None\n",
        "        if bots[i].num_coins >= 10:\n",
        "            action = 1  # Coup\n",
        "        else:\n",
        "            # Choose a random action from the legal actions\n",
        "            action = random.choice(legal_actions)\n",
        "\n",
        "        target = None\n",
        "        if (actions[action].p2_net_coins != 0 or actions[action].p2_net_cards != 0) and actions[action].response_action != 'challenge':\n",
        "            targets = bots[:i] + bots[i+1:]\n",
        "            valid_targets = [bots.index(bot) for bot in targets if bot.num_coins >= -actions[action].p2_net_coins]\n",
        "            if valid_targets:\n",
        "                target = random.choice(valid_targets)\n",
        "\n",
        "        return [action, target]\n",
        "\n",
        "    else:\n",
        "\n",
        "      target = None\n",
        "      action = None\n",
        "      targets = bots[:i] + bots[i+1:]\n",
        "\n",
        "      # Play truthfully:\n",
        "      # bot = bots[i]\n",
        "      if bots[i].num_coins >= 10:  # Coup if possible\n",
        "          action = 1  # Coup action index\n",
        "          target = random.choice(bots[:i] + bots[i+1:])  # Choose a random target\n",
        "      else:\n",
        "          # Prioritize actions based on cards and coins:\n",
        "          if 'Duke' in bots[i].cards and bots[i].num_coins < 10:  # Take 3 coins if Duke\n",
        "              action = 3  # Take 3 coins action index\n",
        "          elif 'Captain' in bots[i].cards and 4 in legal_actions:\n",
        "              action = 4  # Steal action index\n",
        "              valid_targets = [bots.index(bot) for bot in targets if bot.num_coins >= -actions[action].p2_net_coins]\n",
        "              if valid_targets:\n",
        "                  target = random.choice(valid_targets)\n",
        "              # target = bots.index(random.choice([other_bot for other_bot in bots if other_bot != bots[i] and other_bot.num_coins > 0]))\n",
        "          elif 'Assassin' in bot.cards and 5 in legal_actions:\n",
        "              action = 5  # Assassinate action index\n",
        "              target = random.choice(bots[:i] + bots[i+1:])  # Choose a random target\n",
        "          elif 'Ambassador' in bots[i].cards:  # Exchange if Ambassador\n",
        "              action = 6  # Exchange action index\n",
        "          else:  # Otherwise, take income\n",
        "              action = 0  # Take 1 coin action index\n",
        "          if bots[i].num_coins >= 7:\n",
        "            action = 1 # Coup if previous action was not legal and can coup\n",
        "          elif bots[i].num_coins >= 3:\n",
        "            action = 3 # foreign aid if previous action was not legal and can foreign aid\n",
        "          # else:\n",
        "          #   action = 0 # income if previous action was not legal and can only income\n",
        "\n",
        "      # If action requires a target and none is selected yet:\n",
        "      if (actions[action].p2_net_coins != 0 or actions[action].p2_net_cards != 0) and actions[action].response_action != 'challenge' and target is None:\n",
        "          target = bots.index(random.choice(bots[:i] + bots[i+1:]))\n",
        "\n",
        "      return [action, target]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def reaction_selection(i, bots, target, response_action, epsilon, state):\n",
        "\n",
        "  if target is None:\n",
        "\n",
        "    target = random.choice(bots[:i] + bots[i+1:])\n",
        "\n",
        "  if random.random() >= epsilon and target.name == 0:\n",
        "\n",
        "    state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "    q_values = target.block_q(state_tensor)\n",
        "    return torch.argmax(q_values).item(), target\n",
        "\n",
        "  elif target.name != 0:\n",
        "\n",
        "    # Play truthfully - block only if has the card\n",
        "    if response_action.name == actions_map[8]:  # Block foreign aid\n",
        "        if 'Duke' in target.cards:\n",
        "            return 1, target  # Block\n",
        "        else:\n",
        "            return 0, target  # Pass\n",
        "    elif response_action.name == actions_map[9]:  # Block stealing\n",
        "        if 'Captain' in target.cards or 'Ambassador' in target.cards:\n",
        "            return 1, target  # Block\n",
        "        else:\n",
        "            return 0, target  # Pass\n",
        "    elif response_action.name == actions_map[10]:  # Block assassination\n",
        "        if 'Contessa' in target.cards:\n",
        "            return 1, target  # Block\n",
        "        else:\n",
        "            return 0, target  # Pass\n",
        "    else:  # Other actions (cannot be blocked truthfully)\n",
        "        return 0, target  # Pass\n",
        "\n",
        "  else:\n",
        "\n",
        "    return random.choice([0, 1]), target\n",
        "\n",
        "\n",
        "def challenge_selection(epsilon, state, bot):\n",
        "\n",
        "  if random.random() >= epsilon and bot.name == 0:\n",
        "\n",
        "    return 0\n",
        "\n",
        "    # state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "    # q_values = bot.challenge_q(state_tensor)\n",
        "    # return torch.argmax(q_values).item()\n",
        "\n",
        "  else:\n",
        "\n",
        "    return random.choice([0, 1])\n",
        "\n",
        "\n",
        "def card_selection(bot, cards, epsilon, state):\n",
        "\n",
        "  if random.random() >= epsilon and len(cards) > 1 and bot.name == 0:\n",
        "\n",
        "    state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "    q_values = bot.card_q(state_tensor)\n",
        "    card_index = torch.argmax(q_values).item()  # Get index (0 or 1)\n",
        "    return card_index  # Return the index directly\n",
        "\n",
        "  else:\n",
        "\n",
        "    c = random.choice(cards)\n",
        "\n",
        "    c = cards.index(c)\n",
        "\n",
        "    return c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "af20170f",
      "metadata": {
        "id": "af20170f"
      },
      "outputs": [],
      "source": [
        "def perform_action(bot, target, action, discard_pile, state, card_chosen, epsilon, bag):\n",
        "\n",
        "    if target is not None:\n",
        "\n",
        "        target.num_coins += action.p2_net_coins\n",
        "\n",
        "        if action.p2_net_cards < 0 and len(target.cards) > 0:\n",
        "\n",
        "            card = card_selection(target, target.cards, epsilon, state)\n",
        "            # print(card)\n",
        "\n",
        "            x = target.cards[card]\n",
        "\n",
        "            discard_pile.append(inf_map[x])\n",
        "\n",
        "            card_chosen = card\n",
        "\n",
        "            target.cards.remove(x)\n",
        "\n",
        "    bot.num_coins += action.p1_net_coins\n",
        "\n",
        "    if action == exchange:\n",
        "\n",
        "        card = card_selection(bot, bot.cards, epsilon, state)\n",
        "\n",
        "        x = bot.cards[card]\n",
        "\n",
        "        c = random.sample(bag, 2)\n",
        "\n",
        "        # arr = [x] + c\n",
        "\n",
        "        next_choice = card_selection(bot, c, epsilon, state)\n",
        "\n",
        "        next_choice = c[next_choice]\n",
        "\n",
        "        arr = [x] + [next_choice]\n",
        "\n",
        "        final_choice = card_selection(bot, arr, epsilon, state)\n",
        "\n",
        "        card_chosen = final_choice\n",
        "\n",
        "        final_choice = arr[final_choice]\n",
        "\n",
        "        arr.remove(final_choice)\n",
        "\n",
        "        for i in arr:\n",
        "            bag.insert(-1, i)\n",
        "\n",
        "        random.shuffle(bag)\n",
        "        bot.cards.insert(-1, final_choice)\n",
        "        bot.cards.remove(x)\n",
        "\n",
        "    return bot, target, discard_pile, card_chosen, bag\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "LCgzznC-DY3Y",
      "metadata": {
        "id": "LCgzznC-DY3Y"
      },
      "outputs": [],
      "source": [
        "def reset_game(bots_copy):\n",
        "  bag = ['Duke', 'Captain', 'Assassin', 'Contessa', 'Ambassador'] * 3\n",
        "  random.shuffle(bag)\n",
        "  new_bots = []  # Create a new list\n",
        "  for i, bot in enumerate(bots_copy):\n",
        "      cards = random.sample(bag, 2)\n",
        "      for card in cards:\n",
        "          bag.remove(card)\n",
        "      new_bots.append(Bot(cards, 2, None, f'{i}', bot.action_q, bot.block_q, bot.challenge_q, bot.card_q,\n",
        "                          bot.optimizer_action, bot.optimizer_block, bot.optimizer_challenge, bot.optimizer_card))\n",
        "  return new_bots  # Return the new list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "3c6e59e2",
      "metadata": {
        "id": "3c6e59e2"
      },
      "outputs": [],
      "source": [
        "# Base Game Loop\n",
        "\n",
        "def game_loop_random(bots, actions, influences_reverse, epsilon):\n",
        "    bag = ['Duke', 'Captain', 'Assassin', 'Contessa', 'Ambassador'] * 3\n",
        "    random.shuffle(bag)\n",
        "\n",
        "    bots_copy = copy.deepcopy(bots)\n",
        "\n",
        "    states = torch.empty((0, 26), dtype=torch.float32)\n",
        "\n",
        "    discard_piles = []\n",
        "    discard_piles.append([])\n",
        "    acting_players = []\n",
        "    reacting_players = []\n",
        "    current_players = [[1,1,1,1]]\n",
        "    actions_game = [7]\n",
        "    reactions_game = []\n",
        "    challenges_game = []\n",
        "    challenges_direction = []\n",
        "    cards_game = []\n",
        "    coins_game = []\n",
        "\n",
        "    rewards = [0]\n",
        "\n",
        "    action_history = [7]\n",
        "    reaction_history = [0]\n",
        "    challenge_history = [0]\n",
        "    card_history = [0]\n",
        "\n",
        "    cards_turn = [[inf_map[c] for c in bots[0].cards],\n",
        "                  [6,6],\n",
        "                  [6,6],\n",
        "                  [6,6]]\n",
        "    # for i in range(3):\n",
        "\n",
        "    #     cards_ind = [inf_map[c] for c in bot.cards]\n",
        "    #     cards_turn.append(cards_ind)\n",
        "\n",
        "    cards_game.append(cards_turn)\n",
        "\n",
        "    coins_turn = [2,2,2,2]\n",
        "    coins_game.append(coins_turn)\n",
        "    done = []\n",
        "    cards_chosen = [0]\n",
        "\n",
        "#     print(cards_game[-1])\n",
        "#     print(coins_game[-1])\n",
        "\n",
        "    while len(bots) > 1:\n",
        "\n",
        "\n",
        "#         for bot in bots:\n",
        "#             print(f'{bot.name}')\n",
        "        i = 0\n",
        "        while i < len(bots):\n",
        "\n",
        "            # for bot in bots:\n",
        "            #   print(bot.cards)\n",
        "\n",
        "            card_chosen = 0\n",
        "#             print(i)\n",
        "\n",
        "            if len(bots) == 1:\n",
        "                # done.append(1)\n",
        "                break\n",
        "\n",
        "\n",
        "            done.append(0)\n",
        "            rewards.append(0)\n",
        "\n",
        "            challenge_dir = 2\n",
        "\n",
        "            discard_pile = copy.deepcopy(discard_piles[-1])\n",
        "\n",
        "            curr = None\n",
        "            try:\n",
        "                curr = bots[i]\n",
        "            except:\n",
        "                i = 0\n",
        "                curr = bots[i]\n",
        "\n",
        "            acting_players.append(int(curr.name))\n",
        "\n",
        "            # cards_state = cards_game[-1]\n",
        "            # coins_state = coins_game[-1]\n",
        "            # current_players_state = current_players[-1]\n",
        "\n",
        "            # cards_game: N x 4 x 2 -> 8 tensors of size N\n",
        "            cards_game_tensors = [\n",
        "                torch.tensor([cards_game[i][j][k] for i in range(len(cards_game))] )\n",
        "                for j in range(4) for k in range(2)\n",
        "            ]\n",
        "\n",
        "            # current_players: N x 4 -> 1 tensor of size N\n",
        "            current_players_tensors = torch.tensor([sum(row) for row in current_players])\n",
        "\n",
        "            coins_game_tensors = [\n",
        "                      torch.tensor([coins_game[i][j] for i in range(len(coins_game))] )\n",
        "                      for j in range(4)\n",
        "            ]\n",
        "\n",
        "            # discard_piles: N x y (max y = 7) -> 7 tensors of size N\n",
        "            max_discard_len = 7  # Maximum possible length of discard_piles\n",
        "            discard_piles_tensors = [\n",
        "                torch.tensor([discard_piles[i][j] if j < len(discard_piles[i]) else 0\n",
        "                              for i in range(len(discard_piles))] )\n",
        "                for j in range(max_discard_len)\n",
        "            ]\n",
        "\n",
        "            # Concatenate tensors for states_action\n",
        "            # states_action = torch.cat(([\n",
        "            #     torch.tensor(acting_players[1:]).unsqueeze(1), # unsqueeze to add a dimension\n",
        "            #     torch.tensor(reacting_players).unsqueeze(1),\n",
        "            #     torch.tensor(reactions_game).unsqueeze(1),\n",
        "            #     torch.tensor(challenges_game).unsqueeze(1),\n",
        "            #     torch.tensor(current_players_tensors).unsqueeze(1),\n",
        "            #     *[t.unsqueeze(1) for t in cards_game_tensors],\n",
        "            #     *[t.unsqueeze(1) for t in discard_piles_tensors],\n",
        "            #     *[t.unsqueeze(1) for t in coins_game_tensors],\n",
        "            #     torch.tensor(done).unsqueeze(1)\n",
        "            # ]), 1)  # changed dim to 1\n",
        "\n",
        "            state = None\n",
        "\n",
        "            if bots[0].name == 0:\n",
        "\n",
        "              # 1. Cards in play (embedded):\n",
        "              cards_in_play_embedded = []\n",
        "              for card_name in influences.keys():\n",
        "                  num_in_discard = discard_piles[-1].count(inf_map[card_name])\n",
        "                  num_in_play = 3 - num_in_discard  # Assuming 3 of each card initially\n",
        "                  cards_in_play_embedded.append(embedding_cards(torch.tensor(num_in_play)))  # Keep as tensor\n",
        "\n",
        "              # Stack the embeddings to create a 2D tensor\n",
        "              cards_in_play_embedded = torch.stack(cards_in_play_embedded)\n",
        "\n",
        "              # 4. Bot 0's normalized coins:\n",
        "              bot0_coins_normalized = bots[0].num_coins / 12 # Normalize to 0-1 range (assuming max coins is 12)\n",
        "\n",
        "              # 5. Average cards of other players (normalized and embedded):\n",
        "              other_bots_cards = [len(bot.cards) for bot in bots if bot != bots[0]]\n",
        "              avg_other_cards = sum(other_bots_cards) / len(other_bots_cards) if other_bots_cards else 0  # Avoid division by zero\n",
        "              avg_other_cards_normalized = avg_other_cards / 2  # Normalize to 0-1 range (assuming max cards per bot is 2)\n",
        "              # avg_other_cards_embedded = embedding_cards(torch.tensor(int(avg_other_cards_normalized))).tolist()  # Assuming embedding_cards is your embedding layer\n",
        "\n",
        "              # 6. Bot 0's current cards (embedded):\n",
        "              bot0_cards_embedded = []\n",
        "              for card in bots[0].cards:\n",
        "                  bot0_cards_embedded.append(embedding_cards(torch.tensor(inf_map[card])))  # Keep as tensor\n",
        "\n",
        "              # If the bot has cards, concatenate the embeddings. Otherwise, create a zero tensor\n",
        "              if bot0_cards_embedded:\n",
        "                  bot0_cards_embedded = torch.cat(bot0_cards_embedded)\n",
        "              else:\n",
        "                  # Create a zero tensor with the expected shape if bot has no cards\n",
        "                  bot0_cards_embedded = torch.zeros(embedding_cards.embedding_dim * 2)  # Assuming 2 cards max\n",
        "\n",
        "              # 7. The last action taken (embedded):\n",
        "              last_action = actions_game[-1]\n",
        "              last_action_embedded = embedding_actions(torch.tensor(last_action))  # Keep as a tensor\n",
        "\n",
        "              state = torch.cat(([cards_in_play_embedded.unsqueeze(-1),\n",
        "                                  torch.tensor([bot0_coins_normalized]).unsqueeze(-1).unsqueeze(-1),\n",
        "                                  torch.tensor([avg_other_cards_normalized]).unsqueeze(-1).unsqueeze(-1),\n",
        "                                  bot0_cards_embedded.unsqueeze(-1),\n",
        "                                  last_action_embedded.unsqueeze(-1)]),\n",
        "                                1).type(torch.float32)\n",
        "\n",
        "              states = torch.cat((states, state.unsqueeze(0)), 0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            action_stack = []\n",
        "\n",
        "            action_vector = [0,1,2,3,4,5,6]\n",
        "            for j in action_vector:\n",
        "                if actions[j].p1_net_coins * (-1) > bots[i].num_coins:\n",
        "                    action_vector.remove(j)\n",
        "\n",
        "            # state = None\n",
        "            # state_tensor = None\n",
        "            # if bots[0].name == 0:\n",
        "            #   state = get_state(bots, discard_pile, action_history, reaction_history, challenge_history, card_history, bots[i], network_type=\"action\")\n",
        "            #   state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            #   print(len(state_tensor[0]))\n",
        "\n",
        "            action_selection_output = action_selection(i, bots, action_vector, actions, epsilon, state)\n",
        "\n",
        "            action = action_selection_output[0]\n",
        "\n",
        "            actions_game.append(action)\n",
        "\n",
        "            action_e = actions_emb[action]\n",
        "\n",
        "            action = actions[action]\n",
        "    #         print(action_selection_output[1])\n",
        "\n",
        "            # print(f'bot {bots[i].name} is performing action {action.name}')\n",
        "            # print(f'target is {action_selection_output[1]}')\n",
        "\n",
        "            target = None\n",
        "            reacting_player = 4\n",
        "            challenge = 0\n",
        "            reaction = 0\n",
        "            try:\n",
        "                target = next((bot for bot in bots if bot.name == action_selection_output[1]), None)\n",
        "                reacting_player = int(target.name)\n",
        "            except:\n",
        "                target = None\n",
        "#                 reacting_players.append(4)\n",
        "                # print(\"no target\")\n",
        "            # if target is not None:\n",
        "            #     print(f'target is {target.name}')\n",
        "\n",
        "            action_stack.append(action)\n",
        "\n",
        "            if action.response_action is not None and target is None:\n",
        "              target = random.choice(bots)\n",
        "              reacting_player = int(target.name)\n",
        "            reacting_players.append(reacting_player)\n",
        "\n",
        "            # state_reaction = copy.deepcopy(state_action)\n",
        "            # state_reaction.append(action)\n",
        "\n",
        "            # state_challenge = copy.deepcopy(state_action)\n",
        "            # state_challenge.append(action)\n",
        "\n",
        "            # state_card = copy.deepcopy(state_action)\n",
        "            # state_card.append(action)\n",
        "\n",
        "            if action.response_action is not None and action.response_action != 'challenge':  # is blockable?\n",
        "\n",
        "                response, target = reaction_selection(i, bots, target, action.response_action, epsilon, state)\n",
        "\n",
        "                # reacting_player = int(target.name)\n",
        "\n",
        "#                 try:\n",
        "#                     print(f'bot {target.name} is considering blocking')\n",
        "#                 except:\n",
        "#                     print(\"no target, check reaction selection\")\n",
        "\n",
        "                if response == 1:\n",
        "\n",
        "                    reaction = 1\n",
        "\n",
        "                    reactions_game.append(1)\n",
        "\n",
        "#                     reacting_players.append(int(target.name))\n",
        "\n",
        "                    action_stack.append(action.response_action)\n",
        "                    # state_challenge.append(reaction)\n",
        "                    # state_card.append(reaction)\n",
        "\n",
        "                    # print(f'bot {target.name} is performing action {action.response_action.name} against bot {bots[i].name}')\n",
        "\n",
        "                else:\n",
        "\n",
        "                    reactions_game.append(0)\n",
        "\n",
        "            else:\n",
        "\n",
        "                reactions_game.append(0)\n",
        "\n",
        "#                     print(f'target will not block')\n",
        "\n",
        "            if action_stack[-1].response_action == 'challenge':  # is challengeable?\n",
        "\n",
        "                response = challenge_selection(epsilon, state, target if len(action_stack) == 3 else bots[i])\n",
        "\n",
        "                if response == 1:\n",
        "\n",
        "                    challenge = 1\n",
        "\n",
        "                    challenges_game.append(1)\n",
        "\n",
        "                    action_stack.append('challenge')\n",
        "\n",
        "                    if len(action_stack) == 3:\n",
        "                        challenge_dir = 1\n",
        "\n",
        "                    else:\n",
        "                        challenge_dir = 0\n",
        "                    challenges_direction.append(challenge_dir)\n",
        "\n",
        "                else:\n",
        "\n",
        "                    challenges_game.append(0)\n",
        "                    challenges_direction.append(challenge_dir)\n",
        "\n",
        "#                     print('no challenge')\n",
        "\n",
        "            else:\n",
        "\n",
        "                challenges_game.append(0)\n",
        "                challenges_direction.append(2)\n",
        "\n",
        "            # challenges_game.append(0)\n",
        "            # challenges_direction.append(2)\n",
        "\n",
        "            while len(action_stack) != 0:\n",
        "\n",
        "                # state_card.append(challenge)\n",
        "                # state_card.append(challenge_dir)\n",
        "\n",
        "                a = action_stack.pop()\n",
        "                # if a != 'challenge':\n",
        "                #   print(a.name)\n",
        "\n",
        "                if bots[0].name != 0:\n",
        "                  rewards[-1] -= 1.0\n",
        "                # else:\n",
        "                #   if acting_player == 0:\n",
        "                #     if action.name in ['take 1 coin', 'take 2 coins', 'steal 2 coins', 'take 3 coins']:\n",
        "                #         if challenge == 0 and (action.name != 'steal 2 coins' or reaction == 0):  # Action successful\n",
        "                #             rewards[-1] += 0.1 * action.num_coins # Small reward for increasing coins\n",
        "                #     elif action.name == 'assassinate':\n",
        "                #         if challenge == 0 and reaction == 0:  # Action successful\n",
        "                #             rewards[-1] += 0.6  # Larger reward for eliminating an opponent\n",
        "                #     if challenge == 1:\n",
        "                #       if a == 'challenge':\n",
        "                #           if len(action_stack) > 1:\n",
        "                #               if influences_reverse[action_stack[-1]] not in target.cards:\n",
        "                #                   rewards[-1] += 0.5  # Penalty for losing a challenge\n",
        "\n",
        "                #               else:\n",
        "                #                   rewards[-1] -= 0.5  # Penalty for losing a challenge\n",
        "\n",
        "                #           else:\n",
        "                #               if influences_reverse[action_stack[-1]] not in bots[i].cards:\n",
        "                #                   rewards[-1] -= 0.5  # Penalty for losing a challenge\n",
        "\n",
        "                #               else:\n",
        "                #                   rewards[-1] += 0.25  # Penalty for losing a challenge\n",
        "                #   elif reacting_player == 0:\n",
        "                #     if action.name in ['take 1 coin', 'take 2 coins', 'steal 2 coins', 'take 3 coins']:\n",
        "                #         if challenge == 0 and (action.name != 'steal 2 coins' or reaction == 0):  # Action successful\n",
        "                #             rewards[-1] -= 0.1 * action.num_coins # Small reward for increasing coins\n",
        "                #     elif action.name == 'assassinate':\n",
        "                #         if challenge == 0 and reaction == 0:  # Action successful\n",
        "                #             rewards[-1] -= 0.5  # Larger reward for eliminating an opponent\n",
        "                #     if challenge == 1:\n",
        "                #       if a == 'challenge':\n",
        "                #           if len(action_stack) > 1:\n",
        "                #               if influences_reverse[action_stack[-1]] not in target.cards:\n",
        "                #                   rewards[-1] -= 0.5  # Penalty for losing a challenge\n",
        "\n",
        "                #               else:\n",
        "                #                   rewards[-1] += 0.5  # Penalty for losing a challenge\n",
        "\n",
        "                #           else:\n",
        "                #               if influences_reverse[action_stack[-1]] not in bots[i].cards:\n",
        "                #                   rewards[-1] += 0.5  # Penalty for losing a challenge\n",
        "\n",
        "                #               else:\n",
        "                #                   rewards[-1] += 0.5  # Penalty for losing a challenge\n",
        "\n",
        "                if a == 'challenge':\n",
        "\n",
        "                    if len(action_stack) > 1:\n",
        "\n",
        "                        if influences_reverse[action_stack[-1]] in target.cards:\n",
        "\n",
        "                            # print(f'bot {bots[i].name} has lost the challenge')\n",
        "\n",
        "                            card = 0\n",
        "                            if len(bots[i].cards) > 1:\n",
        "\n",
        "                              card = card_selection(bots[i], bots[i].cards, epsilon, state)\n",
        "\n",
        "                            # print(card)\n",
        "                            x = bots[i].cards[card]\n",
        "\n",
        "                            discard_pile.append(inf_map[x])\n",
        "\n",
        "                            card_chosen = card\n",
        "                            # card = inf_map.get(card)\n",
        "\n",
        "                            bots[i].cards.remove(x)\n",
        "\n",
        "                            if len(bots[i].cards) == 0:\n",
        "\n",
        "                                print (f'bot {bots[i].name} is out!')\n",
        "\n",
        "                                bots.remove(bots[i])\n",
        "\n",
        "                                i -= 1\n",
        "\n",
        "                            target.cards.remove(influences_reverse[action_stack[-1]])\n",
        "                            bag.insert(influences_reverse[action_stack[-1]])\n",
        "                            random.shuffle(bag)\n",
        "                            c = random.sample(bag, 1)\n",
        "                            bag.remove(c)\n",
        "                            target.cards.insert(c)\n",
        "\n",
        "                            action_stack.clear()\n",
        "\n",
        "                        else:\n",
        "\n",
        "                            # print(f'bot {target.name} has lost the challenge')\n",
        "\n",
        "                            card = 0\n",
        "                            if len(target.cards) > 1:\n",
        "                              card = card_selection(target, target.cards, epsilon, state)\n",
        "\n",
        "                            # print(card)\n",
        "                            x = target.cards[card]\n",
        "\n",
        "                            discard_pile.append(inf_map[x])\n",
        "\n",
        "                            card_chosen = card\n",
        "\n",
        "                            # card = inf_map.get(card)\n",
        "\n",
        "                            target.cards.remove(x)\n",
        "\n",
        "                            if len(target.cards) == 0:\n",
        "\n",
        "                                bots.remove(target)\n",
        "\n",
        "                                # print (f'bot {target.name} is out!')\n",
        "\n",
        "                            action_stack.pop()\n",
        "\n",
        "                    else:\n",
        "\n",
        "                        if influences_reverse[action_stack[-1]] in bots[i].cards:\n",
        "\n",
        "                            # print(f'bot {target.name} has lost the challenge')\n",
        "\n",
        "                            card = 0\n",
        "                            if len(target.cards) > 1:\n",
        "                              card = card_selection(target, target.cards, epsilon, state)\n",
        "\n",
        "                            # print(card)\n",
        "                            x = target.cards[card]\n",
        "\n",
        "                            discard_pile.append(inf_map[x])\n",
        "\n",
        "                            card_chosen = card\n",
        "                            # card = inf_map.get(card)\n",
        "\n",
        "                            target.cards.remove(x)\n",
        "\n",
        "                            if len(target.cards) == 0:\n",
        "\n",
        "                                # print (f'bot {target.name} is out!')\n",
        "\n",
        "                                bots.remove(target)\n",
        "\n",
        "                            bots[i].cards.remove(influences_reverse[action_stack[-1]])\n",
        "                            bag.insert(influences_reverse[action_stack[-1]])\n",
        "                            random.shuffle(bag)\n",
        "                            c = random.sample(bag, 1)\n",
        "                            bag.remove(c)\n",
        "                            bots[i].cards.insert(c)\n",
        "\n",
        "                        else:\n",
        "\n",
        "                            # print(f'bot {bots[i].name} has lost the challenge')\n",
        "\n",
        "                            card = 0\n",
        "                            if len(bots[i].cards) > 1:\n",
        "                              card = card_selection(bots[i], bots[i].cards, epsilon, state)\n",
        "                            # print(card)\n",
        "                            x = bots[i].cards[card]\n",
        "\n",
        "                            discard_pile.append(inf_map[x])\n",
        "\n",
        "                            card_chosen = card\n",
        "                            # card = inf_map.get(card)\n",
        "\n",
        "                            bots[i].cards.remove(x)\n",
        "\n",
        "                            if len(bots[i].cards) == 0:\n",
        "\n",
        "                                # print (f'bot {bots[i].name} is out!')\n",
        "\n",
        "                                bots.remove(bots[i])\n",
        "\n",
        "                                i -= 1\n",
        "\n",
        "                            action_stack.pop()\n",
        "\n",
        "                else:\n",
        "\n",
        "                    # print(f'current action: {a.name}')\n",
        "\n",
        "                    if len(action_stack) == 1:\n",
        "\n",
        "                        target, curr, discard_pile, card_chosen, bag = perform_action(target, curr, a, discard_pile, state, card_chosen, epsilon, bag)\n",
        "                        if curr is not None:\n",
        "                            if len(curr.cards) == 0:\n",
        "                                if curr in bots:\n",
        "                                    bots.remove(curr)\n",
        "                                    # print(f'{curr.name} is out!')\n",
        "                                    i -= 1\n",
        "\n",
        "                    else:\n",
        "\n",
        "                        curr, target, discard_pile, card_chosen, bag = perform_action(curr, target, a, discard_pile, state, card_chosen, epsilon, bag)\n",
        "                        if target is not None:\n",
        "                            if len(target.cards) == 0:\n",
        "                                if target in bots:\n",
        "                                    bots.remove(target)\n",
        "                                    # print(f'{target.name} is out!')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#             print(f'bot {curr.name} has {curr.num_coins} coins.')\n",
        "#             if target is not None:\n",
        "#                 print(f'bot {target.name} has {target.num_coins} coins.')\n",
        "\n",
        "#             print(f'bot {curr.name} has {len(curr.cards)} cards.')\n",
        "#             if target is not None:\n",
        "#                 print(f'bot {target.name} has {len(target.cards)} cards.')\n",
        "\n",
        "\n",
        "            curr_players = [0,0,0,0]\n",
        "            for bot in bots:\n",
        "#                 print(int(bot.name))\n",
        "                curr_players[int(bot.name)] = 1\n",
        "#             print(curr_players)\n",
        "            current_players.append(curr_players)\n",
        "\n",
        "            cards_turn = [[0,0],\n",
        "                          [0,0],\n",
        "                          [0,0],\n",
        "                          [0,0]]\n",
        "            coins_turn = [0,0,0,0]\n",
        "\n",
        "            # if len(bots[0].cards) == 0:\n",
        "            #     cards_turn[0] = [0,0]\n",
        "            # if len(bots[0].cards) == 1:\n",
        "            #     cards_turn[0].append(0)\n",
        "\n",
        "            for bot in bots:\n",
        "\n",
        "                bot_index = int(bot.name)\n",
        "\n",
        "                cards_ind = []\n",
        "\n",
        "                cards_turn[bot_index] = [inf_map[c] for c in bot.cards]\n",
        "\n",
        "                if len(bot.cards) == 0:\n",
        "                    cards_turn[bot_index] = [0, 0]\n",
        "                if len(bot.cards) == 1:\n",
        "                    cards_turn[bot_index].append(0)\n",
        "\n",
        "                coins_turn[bot_index] = bot.num_coins\n",
        "\n",
        "            cards_game.append(cards_turn)\n",
        "            coins_game.append(coins_turn)\n",
        "\n",
        "            discard_piles.append(discard_pile)\n",
        "\n",
        "            # reacting_players.append(reacting_player)\n",
        "            # reactions_game.append(reaction)\n",
        "            # challenges_game.append(challenge)\n",
        "            # challenges_direction.append(challenge_dir)\n",
        "            cards_chosen.append(card_chosen)\n",
        "\n",
        "            action_history.append(action)\n",
        "            reaction_history.append(reaction)\n",
        "            challenge_history.append(challenge)\n",
        "            card_history.append(card_chosen)\n",
        "\n",
        "            i += 1\n",
        "#             print(cards_turn)\n",
        "#             print(coins_turn)\n",
        "# #             print(curr_players)\n",
        "#             print(discard_pile)\n",
        "\n",
        "\n",
        "\n",
        "    # print(f'bot {bots[0].name} wins!')\n",
        "    acting_players.append(4)\n",
        "    reacting_players.append(4)\n",
        "    reactions_game.append(0)\n",
        "    challenges_game.append(0)\n",
        "    challenges_direction.append(2)\n",
        "    done.append(1)\n",
        "    # rewards = copy.deepcopy(done)\n",
        "    if bots[0].name != '0':\n",
        "      rewards[-1] = -1.0\n",
        "    else:\n",
        "      rewards[-1] = 1.0\n",
        "    # print(rewards[-1])\n",
        "\n",
        "    # rewards = [[0,0,0,0]]\n",
        "    # i = 1\n",
        "    # while i < len(acting_players):\n",
        "    #   rewards_new = copy.deepcopy(rewards[-1])\n",
        "\n",
        "    #   punishment_curr_player = (current_players[i][acting_players[i]] - current_players[i-1][acting_players[i]])\n",
        "\n",
        "    #   punishment_curr_cards = len(cards_game[i][acting_players[i]]) - len(cards_game[i-1][acting_players[i]])\n",
        "\n",
        "    #   reward_curr_player = 0\n",
        "    #   punishment_reacting_player = 0\n",
        "    #   reward_curr_cards = 0\n",
        "    #   if reacting_players[i] != 4:\n",
        "    #     reward_curr_cards = len(cards_game[i][reacting_players[i]]) - len(cards_game[i-1][reacting_players[i]])\n",
        "    #     reward_curr_player = current_players[i-1][reacting_players[i]] - current_players[i][reacting_players[i]]\n",
        "    #     punishment_reacting_player = -1 * (reward_curr_player + reward_curr_cards)\n",
        "\n",
        "\n",
        "    #   coins_reward_curr = (coins_game[i][acting_players[i]] - coins_game[i-1][acting_players[i]])\n",
        "    #   coins_reward_react = 0\n",
        "    #   if reacting_players[i] != 4:\n",
        "    #     coins_reward_react = -1 * coins_reward_curr\n",
        "\n",
        "    #   victory_reward = 50 * done[i]\n",
        "\n",
        "    #   new_reward_curr = punishment_curr_player + punishment_curr_cards + reward_curr_player + coins_reward_curr\n",
        "    #   new_reward_react = punishment_reacting_player + coins_reward_react\n",
        "\n",
        "    #   if victory_reward > 0:\n",
        "    #     if punishment_curr_player > 0:\n",
        "    #       new_reward_curr += victory_reward\n",
        "    #     else:\n",
        "    #       new_reward_react += victory_reward\n",
        "\n",
        "    #   rewards_new[acting_players[i]] += new_reward_curr\n",
        "    #   if reacting_players[i] != 4:\n",
        "    #     rewards_new[reacting_players[i]] += new_reward_react\n",
        "\n",
        "    #   rewards.append(rewards_new)\n",
        "\n",
        "    #   i += 1\n",
        "\n",
        "\n",
        "    # Reset Game\n",
        "\n",
        "    bag = ['Duke', 'Captain', 'Assassin', 'Contessa', 'Ambassador'] * 3\n",
        "    random.shuffle(bag)\n",
        "    # new_bots = []  # Create a new list\n",
        "\n",
        "    for bot in bots_copy:\n",
        "        bot.cards = random.sample(bag, 2)\n",
        "        for card in bot.cards:\n",
        "            bag.remove(card)\n",
        "        bot.num_coins = 2\n",
        "\n",
        "    bots = bots_copy\n",
        "\n",
        "\n",
        "    return discard_piles, acting_players, reacting_players, current_players, actions_game, reactions_game, challenges_game, cards_game, coins_game, challenges_direction, done, rewards, cards_chosen, bots_copy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "be4160d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "be4160d0",
        "outputId": "6025362a-8d81-4ac8-ae2f-2b25693d7cba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode 0 of 1000\n",
            "epsilon: 1.0\n",
            "gamma: 0.99\n",
            "Number of games in episode 0: 77\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-60796da87302>:499: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_rewards_action.append(torch.tensor(sum(rewards_for_state) / len(rewards_for_state) if rewards_for_state else rewards[current_index-1])) # Average reward, handle empty list\n",
            "<ipython-input-30-60796da87302>:500: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_done_action.append(torch.tensor(done[current_index - 1] if current_index >= len(state) else torch.tensor(done[current_index])))\n",
            "<ipython-input-30-60796da87302>:588: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_rewards_block.append(torch.tensor(sum(rewards_for_state) / len(rewards_for_state) if rewards_for_state else rewards[current_index-1])) # Average reward, handle empty list\n",
            "<ipython-input-30-60796da87302>:589: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_done_block.append(torch.tensor(done[current_index-1]) if current_index >= len(state) else torch.tensor(done[current_index]))\n",
            "<ipython-input-30-60796da87302>:669: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_rewards_challenge.append(torch.tensor(sum(rewards_for_state) / len(rewards_for_state) if rewards_for_state else rewards[current_index-1])) # Average reward, handle empty list\n",
            "<ipython-input-30-60796da87302>:670: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_done_challenge.append(torch.tensor(done[current_index-1]) if current_index >= len(state) else torch.tensor(done[current_index]))\n",
            "<ipython-input-30-60796da87302>:767: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_rewards_card.append(torch.tensor(sum(rewards_for_state) / len(rewards_for_state) if rewards_for_state else rewards[current_index-1])) # Average reward, handle empty list\n",
            "<ipython-input-30-60796da87302>:768: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  batch_done_card.append(torch.tensor(done[current_index-1]) if current_index >= len(state) else torch.tensor(done[current_index]))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "0\n",
            "episode 123 of 1000\n",
            "epsilon: 0.5398075216808175\n",
            "gamma: 0.99\n",
            "Number of games in episode 123: 77\n",
            "Avg Action Loss, 13 batches: 0.03919730206521658\n",
            "Avg Block Loss, 8 batches: 0.04139342618873343\n",
            "Avg Challenge Loss, 9 batches: 0.054010369504491486\n",
            "Avg Card Loss, 12 batches: 0.06127004612547656\n",
            "0\n",
            "episode 124 of 1000\n",
            "epsilon: 0.5371084840724134\n",
            "gamma: 0.99\n",
            "Number of games in episode 124: 72\n",
            "Avg Action Loss, 13 batches: 0.03374566524647749\n",
            "Avg Block Loss, 9 batches: 0.07734859445028835\n",
            "Avg Challenge Loss, 9 batches: 0.07610877147979206\n",
            "Avg Card Loss, 10 batches: 0.06598290763795375\n",
            "0\n",
            "episode 125 of 1000\n",
            "epsilon: 0.5344229416520513\n",
            "gamma: 0.99\n",
            "Number of games in episode 125: 75\n",
            "Avg Action Loss, 11 batches: 0.040466784753582695\n",
            "Avg Block Loss, 7 batches: 0.07823382903422628\n",
            "Avg Challenge Loss, 8 batches: 0.08731952053494751\n",
            "Avg Card Loss, 11 batches: 0.06130341351540251\n",
            "0\n",
            "episode 126 of 1000\n",
            "epsilon: 0.531750826943791\n",
            "gamma: 0.99\n",
            "Number of games in episode 126: 72\n",
            "Avg Action Loss, 12 batches: 0.03758662442366282\n",
            "Avg Block Loss, 8 batches: 0.08218501694500446\n",
            "Avg Challenge Loss, 9 batches: 0.07920376459757487\n",
            "Avg Card Loss, 11 batches: 0.06774572266096418\n",
            "0\n",
            "episode 127 of 1000\n",
            "epsilon: 0.5290920728090721\n",
            "gamma: 0.99\n",
            "Number of games in episode 127: 66\n",
            "Avg Action Loss, 14 batches: 0.036598097399941514\n",
            "Avg Block Loss, 8 batches: 0.05989720160141587\n",
            "Avg Challenge Loss, 9 batches: 0.07195055277811156\n",
            "Avg Card Loss, 9 batches: 0.06760433937112491\n",
            "0\n",
            "episode 128 of 1000\n",
            "epsilon: 0.5264466124450268\n",
            "gamma: 0.99\n",
            "Number of games in episode 128: 71\n",
            "Avg Action Loss, 12 batches: 0.02454474944776545\n",
            "Avg Block Loss, 8 batches: 0.05696283315774053\n",
            "Avg Challenge Loss, 8 batches: 0.05613847612403333\n",
            "Avg Card Loss, 11 batches: 0.07015824351798404\n",
            "0\n",
            "episode 129 of 1000\n",
            "epsilon: 0.5238143793828016\n",
            "gamma: 0.99\n",
            "Number of games in episode 129: 75\n",
            "Avg Action Loss, 11 batches: 0.026496193287047474\n",
            "Avg Block Loss, 7 batches: 0.06664586918694633\n",
            "Avg Challenge Loss, 8 batches: 0.0825649332255125\n",
            "Avg Card Loss, 11 batches: 0.06389825499023903\n",
            "0\n",
            "episode 130 of 1000\n",
            "epsilon: 0.5211953074858876\n",
            "gamma: 0.99\n",
            "Number of games in episode 130: 73\n",
            "Avg Action Loss, 12 batches: 0.030372111049170297\n",
            "Avg Block Loss, 9 batches: 0.07849694747063848\n",
            "Avg Challenge Loss, 9 batches: 0.08706031284398502\n",
            "Avg Card Loss, 10 batches: 0.06675514848902822\n",
            "0\n",
            "win rate: 0.28\n",
            "episode 131 of 1000\n",
            "epsilon: 0.5185893309484582\n",
            "gamma: 0.99\n",
            "Number of games in episode 131: 68\n",
            "Avg Action Loss, 12 batches: 0.02944741180787484\n",
            "Avg Block Loss, 9 batches: 0.08658734295103285\n",
            "Avg Challenge Loss, 9 batches: 0.09211419564154413\n",
            "Avg Card Loss, 9 batches: 0.058372537708944745\n",
            "0\n",
            "episode 132 of 1000\n",
            "epsilon: 0.5159963842937159\n",
            "gamma: 0.99\n",
            "Number of games in episode 132: 71\n",
            "Avg Action Loss, 12 batches: 0.03895855710531274\n",
            "Avg Block Loss, 7 batches: 0.06849929504096508\n",
            "Avg Challenge Loss, 8 batches: 0.06657660170458257\n",
            "Avg Card Loss, 10 batches: 0.06929161306470633\n",
            "0\n",
            "episode 133 of 1000\n",
            "epsilon: 0.5134164023722473\n",
            "gamma: 0.99\n",
            "Number of games in episode 133: 78\n",
            "Avg Action Loss, 12 batches: 0.03431999318612119\n",
            "Avg Block Loss, 8 batches: 0.054071076097898185\n",
            "Avg Challenge Loss, 8 batches: 0.05311611411161721\n",
            "Avg Card Loss, 12 batches: 0.05768134421668947\n",
            "0\n",
            "episode 134 of 1000\n",
            "epsilon: 0.510849320360386\n",
            "gamma: 0.99\n",
            "Number of games in episode 134: 72\n",
            "Avg Action Loss, 12 batches: 0.03253839444369078\n",
            "Avg Block Loss, 8 batches: 0.05418474576435983\n",
            "Avg Challenge Loss, 8 batches: 0.05195045960135758\n",
            "Avg Card Loss, 11 batches: 0.05254618213935332\n",
            "0\n",
            "episode 135 of 1000\n",
            "epsilon: 0.5082950737585841\n",
            "gamma: 0.99\n",
            "Number of games in episode 135: 76\n",
            "Avg Action Loss, 13 batches: 0.03465909955020134\n",
            "Avg Block Loss, 8 batches: 0.041727040661498904\n",
            "Avg Challenge Loss, 8 batches: 0.05475840228609741\n",
            "Avg Card Loss, 11 batches: 0.058045747012577274\n",
            "0\n",
            "episode 136 of 1000\n",
            "epsilon: 0.5057535983897912\n",
            "gamma: 0.99\n",
            "Number of games in episode 136: 76\n",
            "Avg Action Loss, 12 batches: 0.034625045489519835\n",
            "Avg Block Loss, 7 batches: 0.05604973994195461\n",
            "Avg Challenge Loss, 8 batches: 0.06664936465676874\n",
            "Avg Card Loss, 11 batches: 0.030599444681270557\n",
            "0\n",
            "episode 137 of 1000\n",
            "epsilon: 0.5032248303978422\n",
            "gamma: 0.99\n",
            "Number of games in episode 137: 74\n",
            "Avg Action Loss, 12 batches: 0.043977480847388506\n",
            "Avg Block Loss, 8 batches: 0.04117662739008665\n",
            "Avg Challenge Loss, 9 batches: 0.04257511626929045\n",
            "Avg Card Loss, 11 batches: 0.03680757822638208\n",
            "0\n",
            "episode 138 of 1000\n",
            "epsilon: 0.500708706245853\n",
            "gamma: 0.99\n",
            "Number of games in episode 138: 74\n",
            "Avg Action Loss, 12 batches: 0.053747749887406826\n",
            "Avg Block Loss, 8 batches: 0.07404531538486481\n",
            "Avg Challenge Loss, 10 batches: 0.08816649541258811\n",
            "Avg Card Loss, 10 batches: 0.06424811873584986\n",
            "0\n",
            "episode 139 of 1000\n",
            "epsilon: 0.4982051627146237\n",
            "gamma: 0.99\n",
            "Number of games in episode 139: 72\n",
            "Avg Action Loss, 12 batches: 0.03289488609880209\n",
            "Avg Block Loss, 7 batches: 0.07120440527796745\n",
            "Avg Challenge Loss, 8 batches: 0.0672449772246182\n",
            "Avg Card Loss, 11 batches: 0.05972625586119565\n",
            "0\n",
            "episode 140 of 1000\n",
            "epsilon: 0.49571413690105054\n",
            "gamma: 0.99\n",
            "Number of games in episode 140: 75\n",
            "Avg Action Loss, 12 batches: 0.04761785765488943\n",
            "Avg Block Loss, 8 batches: 0.05068013211712241\n",
            "Avg Challenge Loss, 8 batches: 0.052493557799607515\n",
            "Avg Card Loss, 12 batches: 0.04105933949661752\n",
            "0\n",
            "win rate: 0.29\n",
            "episode 141 of 1000\n",
            "epsilon: 0.4932355662165453\n",
            "gamma: 0.99\n",
            "Number of games in episode 141: 74\n",
            "Avg Action Loss, 11 batches: 0.04414987555620345\n",
            "Avg Block Loss, 8 batches: 0.052574463188648224\n",
            "Avg Challenge Loss, 9 batches: 0.06177721772756842\n",
            "Avg Card Loss, 11 batches: 0.059624604203484276\n",
            "0\n",
            "episode 142 of 1000\n",
            "epsilon: 0.4907693883854626\n",
            "gamma: 0.99\n",
            "Number of games in episode 142: 83\n",
            "Avg Action Loss, 12 batches: 0.0375225692987442\n",
            "Avg Block Loss, 8 batches: 0.07996633509173989\n",
            "Avg Challenge Loss, 8 batches: 0.07255518028978258\n",
            "Avg Card Loss, 12 batches: 0.058462540696685515\n",
            "0\n",
            "episode 143 of 1000\n",
            "epsilon: 0.4883155414435353\n",
            "gamma: 0.99\n",
            "Number of games in episode 143: 75\n",
            "Avg Action Loss, 12 batches: 0.03981646538401643\n",
            "Avg Block Loss, 8 batches: 0.0723738381639123\n",
            "Avg Challenge Loss, 8 batches: 0.06387187098152936\n",
            "Avg Card Loss, 11 batches: 0.0472482301464135\n",
            "0\n",
            "episode 144 of 1000\n",
            "epsilon: 0.4858739637363176\n",
            "gamma: 0.99\n",
            "Number of games in episode 144: 71\n",
            "Avg Action Loss, 12 batches: 0.04380973366399606\n",
            "Avg Block Loss, 8 batches: 0.0718073130119592\n",
            "Avg Challenge Loss, 8 batches: 0.07274854625575244\n",
            "Avg Card Loss, 11 batches: 0.0339126520451497\n",
            "0\n",
            "episode 145 of 1000\n",
            "epsilon: 0.483444593917636\n",
            "gamma: 0.99\n",
            "Number of games in episode 145: 74\n",
            "Avg Action Loss, 11 batches: 0.040893082252957604\n",
            "Avg Block Loss, 8 batches: 0.06443318189121783\n",
            "Avg Challenge Loss, 8 batches: 0.07278973195934668\n",
            "Avg Card Loss, 12 batches: 0.04127879661973566\n",
            "0\n",
            "episode 146 of 1000\n",
            "epsilon: 0.4810273709480478\n",
            "gamma: 0.99\n",
            "Number of games in episode 146: 76\n",
            "Avg Action Loss, 12 batches: 0.04358942915375034\n",
            "Avg Block Loss, 8 batches: 0.05393422325141728\n",
            "Avg Challenge Loss, 9 batches: 0.060593843977484435\n",
            "Avg Card Loss, 11 batches: 0.048561763780360874\n",
            "0\n",
            "episode 147 of 1000\n",
            "epsilon: 0.47862223409330756\n",
            "gamma: 0.99\n",
            "Number of games in episode 147: 76\n",
            "Avg Action Loss, 13 batches: 0.03970524539741186\n",
            "Avg Block Loss, 8 batches: 0.06651089165825397\n",
            "Avg Challenge Loss, 9 batches: 0.0655893027368519\n",
            "Avg Card Loss, 11 batches: 0.05179753950373693\n",
            "0\n",
            "episode 148 of 1000\n",
            "epsilon: 0.47622912292284103\n",
            "gamma: 0.99\n",
            "Number of games in episode 148: 74\n",
            "Avg Action Loss, 12 batches: 0.03954556258395314\n",
            "Avg Block Loss, 7 batches: 0.043233748259288926\n",
            "Avg Challenge Loss, 8 batches: 0.06190835335291922\n",
            "Avg Card Loss, 11 batches: 0.03289327364076267\n",
            "0\n",
            "episode 149 of 1000\n",
            "epsilon: 0.4738479773082268\n",
            "gamma: 0.99\n",
            "Number of games in episode 149: 83\n",
            "Avg Action Loss, 12 batches: 0.04262038110755384\n",
            "Avg Block Loss, 8 batches: 0.04423786548431963\n",
            "Avg Challenge Loss, 9 batches: 0.056658137693173356\n",
            "Avg Card Loss, 13 batches: 0.04495491406235557\n",
            "0\n",
            "episode 150 of 1000\n",
            "epsilon: 0.47147873742168567\n",
            "gamma: 0.99\n",
            "Number of games in episode 150: 76\n",
            "Avg Action Loss, 11 batches: 0.05479574254290624\n",
            "Avg Block Loss, 8 batches: 0.07382675958797336\n",
            "Avg Challenge Loss, 9 batches: 0.07803235409988298\n",
            "Avg Card Loss, 12 batches: 0.0643534236587584\n",
            "0\n",
            "win rate: 0.19\n",
            "episode 151 of 1000\n",
            "epsilon: 0.46912134373457726\n",
            "gamma: 0.99\n",
            "Number of games in episode 151: 83\n",
            "Avg Action Loss, 12 batches: 0.03230523566404978\n",
            "Avg Block Loss, 8 batches: 0.09160734806209803\n",
            "Avg Challenge Loss, 9 batches: 0.093909060375558\n",
            "Avg Card Loss, 12 batches: 0.05583929092002412\n",
            "0\n",
            "episode 152 of 1000\n",
            "epsilon: 0.46677573701590436\n",
            "gamma: 0.99\n",
            "Number of games in episode 152: 74\n",
            "Avg Action Loss, 13 batches: 0.0503248692705081\n",
            "Avg Block Loss, 7 batches: 0.06184547394514084\n",
            "Avg Challenge Loss, 8 batches: 0.06188202800694853\n",
            "Avg Card Loss, 11 batches: 0.04412404176863757\n",
            "0\n",
            "episode 153 of 1000\n",
            "epsilon: 0.46444185833082485\n",
            "gamma: 0.99\n",
            "Number of games in episode 153: 78\n",
            "Avg Action Loss, 13 batches: 0.04121477400454191\n",
            "Avg Block Loss, 8 batches: 0.08706926216837019\n",
            "Avg Challenge Loss, 9 batches: 0.0915639903396368\n",
            "Avg Card Loss, 11 batches: 0.06772272348065268\n",
            "0\n",
            "episode 154 of 1000\n",
            "epsilon: 0.46211964903917074\n",
            "gamma: 0.99\n",
            "Number of games in episode 154: 76\n",
            "Avg Action Loss, 12 batches: 0.03533894537637631\n",
            "Avg Block Loss, 8 batches: 0.05080182431265712\n",
            "Avg Challenge Loss, 8 batches: 0.052398496540263295\n",
            "Avg Card Loss, 12 batches: 0.034618339772957064\n",
            "0\n",
            "episode 155 of 1000\n",
            "epsilon: 0.4598090507939749\n",
            "gamma: 0.99\n",
            "Number of games in episode 155: 74\n",
            "Avg Action Loss, 12 batches: 0.026281119789928198\n",
            "Avg Block Loss, 8 batches: 0.045594547293148935\n",
            "Avg Challenge Loss, 9 batches: 0.047227217401895255\n",
            "Avg Card Loss, 11 batches: 0.03736319494518367\n",
            "0\n",
            "episode 156 of 1000\n",
            "epsilon: 0.457510005540005\n",
            "gamma: 0.99\n",
            "Number of games in episode 156: 76\n",
            "Avg Action Loss, 12 batches: 0.03529850331445535\n",
            "Avg Block Loss, 8 batches: 0.05643148091621697\n",
            "Avg Challenge Loss, 8 batches: 0.05697607179172337\n",
            "Avg Card Loss, 12 batches: 0.050120434684989355\n",
            "0\n",
            "episode 157 of 1000\n",
            "epsilon: 0.45522245551230495\n",
            "gamma: 0.99\n",
            "Number of games in episode 157: 68\n",
            "Avg Action Loss, 13 batches: 0.03190977298296415\n",
            "Avg Block Loss, 8 batches: 0.05657940299715847\n",
            "Avg Challenge Loss, 9 batches: 0.053223739481634565\n",
            "Avg Card Loss, 10 batches: 0.04951951347757131\n",
            "0\n",
            "episode 158 of 1000\n",
            "epsilon: 0.4529463432347434\n",
            "gamma: 0.99\n",
            "Number of games in episode 158: 77\n",
            "Avg Action Loss, 11 batches: 0.04154106280343099\n",
            "Avg Block Loss, 8 batches: 0.0579474417027086\n",
            "Avg Challenge Loss, 9 batches: 0.061970053447617426\n",
            "Avg Card Loss, 11 batches: 0.045009456828913906\n",
            "0\n",
            "episode 159 of 1000\n",
            "epsilon: 0.4506816115185697\n",
            "gamma: 0.99\n",
            "Number of games in episode 159: 72\n",
            "Avg Action Loss, 12 batches: 0.03238441414820651\n",
            "Avg Block Loss, 7 batches: 0.0765493248722383\n",
            "Avg Challenge Loss, 8 batches: 0.06897006719373167\n",
            "Avg Card Loss, 11 batches: 0.045583343620158055\n",
            "0\n",
            "episode 160 of 1000\n",
            "epsilon: 0.4484282034609769\n",
            "gamma: 0.99\n",
            "Number of games in episode 160: 75\n",
            "Avg Action Loss, 12 batches: 0.03949650718520085\n",
            "Avg Block Loss, 8 batches: 0.040740108699537814\n",
            "Avg Challenge Loss, 9 batches: 0.048382420713702835\n",
            "Avg Card Loss, 12 batches: 0.05816681085464855\n",
            "0\n",
            "win rate: 0.22\n",
            "episode 161 of 1000\n",
            "epsilon: 0.446186062443672\n",
            "gamma: 0.99\n",
            "Number of games in episode 161: 74\n",
            "Avg Action Loss, 12 batches: 0.05599461371699969\n",
            "Avg Block Loss, 8 batches: 0.06033508630935103\n",
            "Avg Challenge Loss, 9 batches: 0.0674066636711359\n",
            "Avg Card Loss, 11 batches: 0.04898176850243048\n",
            "0\n",
            "episode 162 of 1000\n",
            "epsilon: 0.4439551321314536\n",
            "gamma: 0.99\n",
            "Number of games in episode 162: 70\n",
            "Avg Action Loss, 12 batches: 0.03205674630589783\n",
            "Avg Block Loss, 8 batches: 0.04128406092058867\n",
            "Avg Challenge Loss, 8 batches: 0.043445863062515855\n",
            "Avg Card Loss, 10 batches: 0.02531311195343733\n",
            "0\n",
            "episode 163 of 1000\n",
            "epsilon: 0.4417353564707963\n",
            "gamma: 0.99\n",
            "Number of games in episode 163: 72\n",
            "Avg Action Loss, 12 batches: 0.029041450548296172\n",
            "Avg Block Loss, 7 batches: 0.06686342826911382\n",
            "Avg Challenge Loss, 7 batches: 0.07231114272560392\n",
            "Avg Card Loss, 11 batches: 0.054802488747306845\n",
            "0\n",
            "episode 164 of 1000\n",
            "epsilon: 0.43952667968844233\n",
            "gamma: 0.99\n",
            "Number of games in episode 164: 75\n",
            "Avg Action Loss, 12 batches: 0.03042254480533302\n",
            "Avg Block Loss, 8 batches: 0.0606367711443454\n",
            "Avg Challenge Loss, 8 batches: 0.060198191087692976\n",
            "Avg Card Loss, 11 batches: 0.06751584430987184\n",
            "0\n",
            "episode 165 of 1000\n",
            "epsilon: 0.43732904629000013\n",
            "gamma: 0.99\n",
            "Number of games in episode 165: 71\n",
            "Avg Action Loss, 13 batches: 0.04088526734938988\n",
            "Avg Block Loss, 8 batches: 0.07546227634884417\n",
            "Avg Challenge Loss, 8 batches: 0.07687546731904149\n",
            "Avg Card Loss, 11 batches: 0.0574410518377342\n",
            "0\n",
            "episode 166 of 1000\n",
            "epsilon: 0.4351424010585501\n",
            "gamma: 0.99\n",
            "Number of games in episode 166: 68\n",
            "Avg Action Loss, 12 batches: 0.02653441457853963\n",
            "Avg Block Loss, 8 batches: 0.0650707152672112\n",
            "Avg Challenge Loss, 8 batches: 0.06548269256018102\n",
            "Avg Card Loss, 10 batches: 0.05321202557533979\n",
            "0\n",
            "episode 167 of 1000\n",
            "epsilon: 0.43296668905325736\n",
            "gamma: 0.99\n",
            "Number of games in episode 167: 73\n",
            "Avg Action Loss, 13 batches: 0.03039509752908578\n",
            "Avg Block Loss, 9 batches: 0.04285939617289437\n",
            "Avg Challenge Loss, 9 batches: 0.038029120976312294\n",
            "Avg Card Loss, 10 batches: 0.06117161735892296\n",
            "0\n",
            "episode 168 of 1000\n",
            "epsilon: 0.43080185560799106\n",
            "gamma: 0.99\n",
            "Number of games in episode 168: 72\n",
            "Avg Action Loss, 12 batches: 0.03461773864304026\n",
            "Avg Block Loss, 7 batches: 0.05275501736572811\n",
            "Avg Challenge Loss, 8 batches: 0.061206530313938856\n",
            "Avg Card Loss, 11 batches: 0.03936295257881284\n",
            "0\n",
            "episode 169 of 1000\n",
            "epsilon: 0.4286478463299511\n",
            "gamma: 0.99\n",
            "Number of games in episode 169: 75\n",
            "Avg Action Loss, 11 batches: 0.037013758989897644\n",
            "Avg Block Loss, 7 batches: 0.055575247055717876\n",
            "Avg Challenge Loss, 8 batches: 0.060537540819495916\n",
            "Avg Card Loss, 12 batches: 0.038113862567115575\n",
            "0\n",
            "episode 170 of 1000\n",
            "epsilon: 0.42650460709830135\n",
            "gamma: 0.99\n",
            "Number of games in episode 170: 72\n",
            "Avg Action Loss, 12 batches: 0.03974033799022436\n",
            "Avg Block Loss, 8 batches: 0.051179615780711174\n",
            "Avg Challenge Loss, 8 batches: 0.05476733832620084\n",
            "Avg Card Loss, 11 batches: 0.058551447593014345\n",
            "0\n",
            "win rate: 0.17\n",
            "episode 171 of 1000\n",
            "epsilon: 0.42437208406280985\n",
            "gamma: 0.99\n",
            "Number of games in episode 171: 74\n",
            "Avg Action Loss, 12 batches: 0.031864603709739946\n",
            "Avg Block Loss, 9 batches: 0.07182672267986669\n",
            "Avg Challenge Loss, 9 batches: 0.07007895554933283\n",
            "Avg Card Loss, 11 batches: 0.04415133574300192\n",
            "0\n",
            "episode 172 of 1000\n",
            "epsilon: 0.4222502236424958\n",
            "gamma: 0.99\n",
            "Number of games in episode 172: 72\n",
            "Avg Action Loss, 11 batches: 0.03808109580793164\n",
            "Avg Block Loss, 7 batches: 0.057757849405918806\n",
            "Avg Challenge Loss, 8 batches: 0.06596794072538614\n",
            "Avg Card Loss, 11 batches: 0.04660488698970188\n",
            "0\n",
            "episode 173 of 1000\n",
            "epsilon: 0.42013897252428334\n",
            "gamma: 0.99\n",
            "Number of games in episode 173: 76\n",
            "Avg Action Loss, 10 batches: 0.036157237738370894\n",
            "Avg Block Loss, 7 batches: 0.0847139535471797\n",
            "Avg Challenge Loss, 7 batches: 0.08023317583969661\n",
            "Avg Card Loss, 13 batches: 0.04931601846160797\n",
            "0\n",
            "episode 174 of 1000\n",
            "epsilon: 0.4180382776616619\n",
            "gamma: 0.99\n",
            "Number of games in episode 174: 70\n",
            "Avg Action Loss, 12 batches: 0.03831804714476069\n",
            "Avg Block Loss, 8 batches: 0.0725024149287492\n",
            "Avg Challenge Loss, 9 batches: 0.07382017146382067\n",
            "Avg Card Loss, 10 batches: 0.06003633718937636\n",
            "0\n",
            "episode 175 of 1000\n",
            "epsilon: 0.4159480862733536\n",
            "gamma: 0.99\n",
            "Number of games in episode 175: 73\n",
            "Avg Action Loss, 12 batches: 0.03800265095196664\n",
            "Avg Block Loss, 8 batches: 0.06728329858742654\n",
            "Avg Challenge Loss, 9 batches: 0.0707326057470507\n",
            "Avg Card Loss, 11 batches: 0.04301694018596953\n",
            "0\n",
            "episode 176 of 1000\n",
            "epsilon: 0.41386834584198684\n",
            "gamma: 0.99\n",
            "Number of games in episode 176: 76\n",
            "Avg Action Loss, 13 batches: 0.030634769023610994\n",
            "Avg Block Loss, 8 batches: 0.06684988155029714\n",
            "Avg Challenge Loss, 8 batches: 0.07485806569457054\n",
            "Avg Card Loss, 11 batches: 0.06283131648193706\n",
            "0\n",
            "episode 177 of 1000\n",
            "epsilon: 0.4117990041127769\n",
            "gamma: 0.99\n",
            "Number of games in episode 177: 79\n",
            "Avg Action Loss, 13 batches: 0.03538246216395727\n",
            "Avg Block Loss, 8 batches: 0.07350899139419198\n",
            "Avg Challenge Loss, 9 batches: 0.07204556071923839\n",
            "Avg Card Loss, 11 batches: 0.03480258723720908\n",
            "0\n",
            "episode 178 of 1000\n",
            "epsilon: 0.40974000909221303\n",
            "gamma: 0.99\n",
            "Number of games in episode 178: 70\n",
            "Avg Action Loss, 11 batches: 0.04026224989105354\n",
            "Avg Block Loss, 8 batches: 0.0620221639983356\n",
            "Avg Challenge Loss, 9 batches: 0.06967161099116008\n",
            "Avg Card Loss, 10 batches: 0.04700665194541216\n",
            "0\n",
            "episode 179 of 1000\n",
            "epsilon: 0.40769130904675194\n",
            "gamma: 0.99\n",
            "Number of games in episode 179: 71\n",
            "Avg Action Loss, 12 batches: 0.03448571761449178\n",
            "Avg Block Loss, 8 batches: 0.08210595278069377\n",
            "Avg Challenge Loss, 9 batches: 0.07734541677766377\n",
            "Avg Card Loss, 10 batches: 0.05080540776252747\n",
            "0\n",
            "episode 180 of 1000\n",
            "epsilon: 0.40565285250151817\n",
            "gamma: 0.99\n",
            "Number of games in episode 180: 69\n",
            "Avg Action Loss, 12 batches: 0.0329446787169824\n",
            "Avg Block Loss, 7 batches: 0.058073261087494235\n",
            "Avg Challenge Loss, 7 batches: 0.05280809796282223\n",
            "Avg Card Loss, 10 batches: 0.05471411915495992\n",
            "0\n",
            "win rate: 0.29\n",
            "episode 181 of 1000\n",
            "epsilon: 0.4036245882390106\n",
            "gamma: 0.99\n",
            "Number of games in episode 181: 71\n",
            "Avg Action Loss, 13 batches: 0.029944467931412734\n",
            "Avg Block Loss, 8 batches: 0.06629844172857702\n",
            "Avg Challenge Loss, 9 batches: 0.06660699823664294\n",
            "Avg Card Loss, 11 batches: 0.05118765195154331\n",
            "0\n",
            "episode 182 of 1000\n",
            "epsilon: 0.4016064652978155\n",
            "gamma: 0.99\n",
            "Number of games in episode 182: 81\n",
            "Avg Action Loss, 12 batches: 0.03003671303546677\n",
            "Avg Block Loss, 9 batches: 0.06510182366602951\n",
            "Avg Challenge Loss, 10 batches: 0.07715409137308597\n",
            "Avg Card Loss, 12 batches: 0.046210023729751505\n",
            "0\n",
            "episode 183 of 1000\n",
            "epsilon: 0.3995984329713264\n",
            "gamma: 0.99\n",
            "Number of games in episode 183: 73\n",
            "Avg Action Loss, 12 batches: 0.028176462122549612\n",
            "Avg Block Loss, 8 batches: 0.045660536852665246\n",
            "Avg Challenge Loss, 9 batches: 0.05034213782184654\n",
            "Avg Card Loss, 12 batches: 0.05296818112644056\n",
            "0\n",
            "episode 184 of 1000\n",
            "epsilon: 0.3976004408064698\n",
            "gamma: 0.99\n",
            "Number of games in episode 184: 81\n",
            "Avg Action Loss, 12 batches: 0.027158829538772505\n",
            "Avg Block Loss, 9 batches: 0.07009823020133707\n",
            "Avg Challenge Loss, 9 batches: 0.06309049224687947\n",
            "Avg Card Loss, 12 batches: 0.05061888617152969\n",
            "0\n",
            "episode 185 of 1000\n",
            "epsilon: 0.39561243860243744\n",
            "gamma: 0.99\n",
            "Number of games in episode 185: 72\n",
            "Avg Action Loss, 12 batches: 0.031080620052913826\n",
            "Avg Block Loss, 8 batches: 0.06363908154889941\n",
            "Avg Challenge Loss, 9 batches: 0.0625122379925516\n",
            "Avg Card Loss, 11 batches: 0.043928080407733265\n",
            "0\n",
            "episode 186 of 1000\n",
            "epsilon: 0.3936343764094253\n",
            "gamma: 0.99\n",
            "Number of games in episode 186: 71\n",
            "Avg Action Loss, 12 batches: 0.044984118547290564\n",
            "Avg Block Loss, 8 batches: 0.06643328163772821\n",
            "Avg Challenge Loss, 9 batches: 0.06876476440164778\n",
            "Avg Card Loss, 11 batches: 0.04722034105692397\n",
            "0\n",
            "episode 187 of 1000\n",
            "epsilon: 0.39166620452737816\n",
            "gamma: 0.99\n",
            "Number of games in episode 187: 79\n",
            "Avg Action Loss, 12 batches: 0.02995432133320719\n",
            "Avg Block Loss, 7 batches: 0.05605939882142203\n",
            "Avg Challenge Loss, 8 batches: 0.06533851800486445\n",
            "Avg Card Loss, 13 batches: 0.03772149845742835\n",
            "0\n",
            "episode 188 of 1000\n",
            "epsilon: 0.3897078735047413\n",
            "gamma: 0.99\n",
            "Number of games in episode 188: 75\n",
            "Avg Action Loss, 11 batches: 0.031111536886204372\n",
            "Avg Block Loss, 7 batches: 0.059886506625584195\n",
            "Avg Challenge Loss, 8 batches: 0.06885359575971961\n",
            "Avg Card Loss, 12 batches: 0.03504106542095542\n",
            "0\n",
            "episode 189 of 1000\n",
            "epsilon: 0.3877593341372176\n",
            "gamma: 0.99\n",
            "Number of games in episode 189: 77\n",
            "Avg Action Loss, 12 batches: 0.02229079798174401\n",
            "Avg Block Loss, 8 batches: 0.05281026498414576\n",
            "Avg Challenge Loss, 8 batches: 0.050288609229028225\n",
            "Avg Card Loss, 12 batches: 0.03640745500645911\n",
            "0\n",
            "episode 190 of 1000\n",
            "epsilon: 0.3858205374665315\n",
            "gamma: 0.99\n",
            "Number of games in episode 190: 76\n",
            "Avg Action Loss, 12 batches: 0.018303796416148543\n",
            "Avg Block Loss, 8 batches: 0.07290265988558531\n",
            "Avg Challenge Loss, 8 batches: 0.07353465724736452\n",
            "Avg Card Loss, 12 batches: 0.048567806060115494\n",
            "0\n",
            "win rate: 0.2\n",
            "episode 191 of 1000\n",
            "epsilon: 0.38389143477919885\n",
            "gamma: 0.99\n",
            "Number of games in episode 191: 73\n",
            "Avg Action Loss, 13 batches: 0.03337681472588044\n",
            "Avg Block Loss, 8 batches: 0.05486318189650774\n",
            "Avg Challenge Loss, 8 batches: 0.060753360856324434\n",
            "Avg Card Loss, 10 batches: 0.0660649087280035\n",
            "0\n",
            "episode 192 of 1000\n",
            "epsilon: 0.3819719776053028\n",
            "gamma: 0.99\n",
            "Number of games in episode 192: 78\n",
            "Avg Action Loss, 12 batches: 0.02834724906521539\n",
            "Avg Block Loss, 8 batches: 0.05381027644034475\n",
            "Avg Challenge Loss, 9 batches: 0.05465554756422838\n",
            "Avg Card Loss, 12 batches: 0.03697511829280605\n",
            "0\n",
            "episode 193 of 1000\n",
            "epsilon: 0.3800621177172763\n",
            "gamma: 0.99\n",
            "Number of games in episode 193: 72\n",
            "Avg Action Loss, 12 batches: 0.04985889792442322\n",
            "Avg Block Loss, 8 batches: 0.07704254146665335\n",
            "Avg Challenge Loss, 9 batches: 0.08199760524763001\n",
            "Avg Card Loss, 10 batches: 0.04462603591382504\n",
            "0\n",
            "episode 194 of 1000\n",
            "epsilon: 0.37816180712868996\n",
            "gamma: 0.99\n",
            "Number of games in episode 194: 74\n",
            "Avg Action Loss, 13 batches: 0.034774365405050606\n",
            "Avg Block Loss, 8 batches: 0.07692999159917235\n",
            "Avg Challenge Loss, 8 batches: 0.07721827342174947\n",
            "Avg Card Loss, 10 batches: 0.07219784301705659\n",
            "0\n",
            "episode 195 of 1000\n",
            "epsilon: 0.37627099809304654\n",
            "gamma: 0.99\n",
            "Number of games in episode 195: 74\n",
            "Avg Action Loss, 13 batches: 0.04123460315167904\n",
            "Avg Block Loss, 9 batches: 0.06138150580227375\n",
            "Avg Challenge Loss, 10 batches: 0.06472646873444318\n",
            "Avg Card Loss, 11 batches: 0.057582891461524094\n",
            "0\n",
            "episode 196 of 1000\n",
            "epsilon: 0.3743896431025813\n",
            "gamma: 0.99\n",
            "Number of games in episode 196: 75\n",
            "Avg Action Loss, 11 batches: 0.02605097834020853\n",
            "Avg Block Loss, 7 batches: 0.07606920999075685\n",
            "Avg Challenge Loss, 8 batches: 0.0765927575994283\n",
            "Avg Card Loss, 10 batches: 0.053045582585036756\n",
            "0\n",
            "episode 197 of 1000\n",
            "epsilon: 0.37251769488706843\n",
            "gamma: 0.99\n",
            "Number of games in episode 197: 72\n",
            "Avg Action Loss, 13 batches: 0.028126689963615857\n",
            "Avg Block Loss, 8 batches: 0.07678633066825569\n",
            "Avg Challenge Loss, 8 batches: 0.07138793205376714\n",
            "Avg Card Loss, 11 batches: 0.03952955951998857\n",
            "0\n",
            "episode 198 of 1000\n",
            "epsilon: 0.3706551064126331\n",
            "gamma: 0.99\n",
            "Number of games in episode 198: 82\n",
            "Avg Action Loss, 12 batches: 0.03097128278265397\n",
            "Avg Block Loss, 8 batches: 0.06402607052586973\n",
            "Avg Challenge Loss, 9 batches: 0.05736838695075777\n",
            "Avg Card Loss, 12 batches: 0.04115639048783729\n",
            "0\n",
            "episode 199 of 1000\n",
            "epsilon: 0.36880183088056995\n",
            "gamma: 0.99\n",
            "Number of games in episode 199: 72\n",
            "Avg Action Loss, 12 batches: 0.03480862624322375\n",
            "Avg Block Loss, 8 batches: 0.07092701783403754\n",
            "Avg Challenge Loss, 9 batches: 0.07524004340585735\n",
            "Avg Card Loss, 10 batches: 0.05597089473158121\n",
            "0\n",
            "episode 200 of 1000\n",
            "epsilon: 0.3669578217261671\n",
            "gamma: 0.99\n",
            "Number of games in episode 200: 74\n",
            "Avg Action Loss, 13 batches: 0.04046583290283497\n",
            "Avg Block Loss, 8 batches: 0.08280685113277286\n",
            "Avg Challenge Loss, 9 batches: 0.08267595453394784\n",
            "Avg Card Loss, 11 batches: 0.05482473639263348\n",
            "0\n",
            "win rate: 0.2\n",
            "episode 201 of 1000\n",
            "epsilon: 0.36512303261753626\n",
            "gamma: 0.99\n",
            "Number of games in episode 201: 78\n",
            "Avg Action Loss, 12 batches: 0.03425206275035938\n",
            "Avg Block Loss, 8 batches: 0.0628912162501365\n",
            "Avg Challenge Loss, 8 batches: 0.061965790344402194\n",
            "Avg Card Loss, 12 batches: 0.04720731537478665\n",
            "0\n",
            "episode 202 of 1000\n",
            "epsilon: 0.3632974174544486\n",
            "gamma: 0.99\n",
            "Number of games in episode 202: 71\n",
            "Avg Action Loss, 12 batches: 0.029415288474410772\n",
            "Avg Block Loss, 8 batches: 0.07488200860098004\n",
            "Avg Challenge Loss, 8 batches: 0.07646767399273813\n",
            "Avg Card Loss, 10 batches: 0.028089415817521512\n",
            "0\n",
            "episode 203 of 1000\n",
            "epsilon: 0.3614809303671764\n",
            "gamma: 0.99\n",
            "Number of games in episode 203: 79\n",
            "Avg Action Loss, 13 batches: 0.03952694039505262\n",
            "Avg Block Loss, 8 batches: 0.05999784148298204\n",
            "Avg Challenge Loss, 8 batches: 0.05403051839675754\n",
            "Avg Card Loss, 12 batches: 0.04934208320143322\n",
            "0\n",
            "episode 204 of 1000\n",
            "epsilon: 0.3596735257153405\n",
            "gamma: 0.99\n",
            "Number of games in episode 204: 78\n",
            "Avg Action Loss, 11 batches: 0.03330998833883892\n",
            "Avg Block Loss, 7 batches: 0.08324921144438642\n",
            "Avg Challenge Loss, 7 batches: 0.08959190653903144\n",
            "Avg Card Loss, 12 batches: 0.04228953062556684\n",
            "0\n",
            "episode 205 of 1000\n",
            "epsilon: 0.3578751580867638\n",
            "gamma: 0.99\n",
            "Number of games in episode 205: 73\n",
            "Avg Action Loss, 11 batches: 0.027051333676684986\n",
            "Avg Block Loss, 8 batches: 0.0507173805963248\n",
            "Avg Challenge Loss, 8 batches: 0.053174556349404156\n",
            "Avg Card Loss, 10 batches: 0.045351942908018826\n",
            "0\n",
            "episode 206 of 1000\n",
            "epsilon: 0.35608578229633\n",
            "gamma: 0.99\n",
            "Number of games in episode 206: 73\n",
            "Avg Action Loss, 13 batches: 0.03715570099078692\n",
            "Avg Block Loss, 9 batches: 0.05191871016803715\n",
            "Avg Challenge Loss, 9 batches: 0.05631374422874716\n",
            "Avg Card Loss, 11 batches: 0.065745909020982\n",
            "0\n",
            "episode 207 of 1000\n",
            "epsilon: 0.3543053533848483\n",
            "gamma: 0.99\n",
            "Number of games in episode 207: 74\n",
            "Avg Action Loss, 11 batches: 0.03247526415031065\n",
            "Avg Block Loss, 7 batches: 0.06607083763395037\n",
            "Avg Challenge Loss, 7 batches: 0.06626302057078906\n",
            "Avg Card Loss, 12 batches: 0.038174848072230816\n",
            "0\n",
            "episode 208 of 1000\n",
            "epsilon: 0.35253382661792404\n",
            "gamma: 0.99\n",
            "Number of games in episode 208: 76\n",
            "Avg Action Loss, 12 batches: 0.029347283416427672\n",
            "Avg Block Loss, 9 batches: 0.046668357112341456\n",
            "Avg Challenge Loss, 9 batches: 0.05024252976808283\n",
            "Avg Card Loss, 11 batches: 0.029016757299276916\n",
            "0\n",
            "episode 209 of 1000\n",
            "epsilon: 0.3507711574848344\n",
            "gamma: 0.99\n",
            "Number of games in episode 209: 71\n",
            "Avg Action Loss, 12 batches: 0.026907815442730982\n",
            "Avg Block Loss, 8 batches: 0.05180570622906089\n",
            "Avg Challenge Loss, 9 batches: 0.05360648822453287\n",
            "Avg Card Loss, 12 batches: 0.050849055560926594\n",
            "0\n",
            "episode 210 of 1000\n",
            "epsilon: 0.34901730169741024\n",
            "gamma: 0.99\n",
            "Number of games in episode 210: 74\n",
            "Avg Action Loss, 12 batches: 0.02114872622769326\n",
            "Avg Block Loss, 7 batches: 0.05012731413756098\n",
            "Avg Challenge Loss, 7 batches: 0.05603259855083057\n",
            "Avg Card Loss, 11 batches: 0.03519339110194282\n",
            "0\n",
            "win rate: 0.34\n",
            "episode 211 of 1000\n",
            "epsilon: 0.3472722151889232\n",
            "gamma: 0.99\n",
            "Number of games in episode 211: 73\n",
            "Avg Action Loss, 12 batches: 0.035284737668310605\n",
            "Avg Block Loss, 8 batches: 0.07153140590526164\n",
            "Avg Challenge Loss, 8 batches: 0.07355631655082107\n",
            "Avg Card Loss, 11 batches: 0.040688068182630974\n",
            "0\n",
            "episode 212 of 1000\n",
            "epsilon: 0.3455358541129786\n",
            "gamma: 0.99\n",
            "Number of games in episode 212: 76\n",
            "Avg Action Loss, 11 batches: 0.0359538724984635\n",
            "Avg Block Loss, 7 batches: 0.06401841927851949\n",
            "Avg Challenge Loss, 8 batches: 0.06682855798862875\n",
            "Avg Card Loss, 11 batches: 0.046254376584494654\n",
            "0\n",
            "episode 213 of 1000\n",
            "epsilon: 0.3438081748424137\n",
            "gamma: 0.99\n",
            "Number of games in episode 213: 80\n",
            "Avg Action Loss, 12 batches: 0.031311475400192045\n",
            "Avg Block Loss, 8 batches: 0.044447774649597704\n",
            "Avg Challenge Loss, 9 batches: 0.0561145997295777\n",
            "Avg Card Loss, 13 batches: 0.04469555079077299\n",
            "0\n",
            "episode 214 of 1000\n",
            "epsilon: 0.3420891339682016\n",
            "gamma: 0.99\n",
            "Number of games in episode 214: 75\n",
            "Avg Action Loss, 12 batches: 0.03465045103803277\n",
            "Avg Block Loss, 8 batches: 0.08511458057910204\n",
            "Avg Challenge Loss, 9 batches: 0.08721834462549952\n",
            "Avg Card Loss, 11 batches: 0.06423134238205173\n",
            "0\n",
            "episode 215 of 1000\n",
            "epsilon: 0.3403786882983606\n",
            "gamma: 0.99\n",
            "Number of games in episode 215: 78\n",
            "Avg Action Loss, 11 batches: 0.033508261665701866\n",
            "Avg Block Loss, 7 batches: 0.07996120782835144\n",
            "Avg Challenge Loss, 8 batches: 0.07098182034678757\n",
            "Avg Card Loss, 12 batches: 0.05420499939161042\n",
            "0\n",
            "episode 216 of 1000\n",
            "epsilon: 0.3386767948568688\n",
            "gamma: 0.99\n",
            "Number of games in episode 216: 77\n",
            "Avg Action Loss, 12 batches: 0.02049923036247492\n",
            "Avg Block Loss, 8 batches: 0.03449102566810325\n",
            "Avg Challenge Loss, 9 batches: 0.03673622612324026\n",
            "Avg Card Loss, 12 batches: 0.032396968900381275\n",
            "0\n",
            "episode 217 of 1000\n",
            "epsilon: 0.33698341088258443\n",
            "gamma: 0.99\n",
            "Number of games in episode 217: 78\n",
            "Avg Action Loss, 12 batches: 0.04308568950121602\n",
            "Avg Block Loss, 7 batches: 0.042735545496855466\n",
            "Avg Challenge Loss, 8 batches: 0.05154684605076909\n",
            "Avg Card Loss, 12 batches: 0.03612067538779229\n",
            "0\n",
            "episode 218 of 1000\n",
            "epsilon: 0.3352984938281715\n",
            "gamma: 0.99\n",
            "Number of games in episode 218: 72\n",
            "Avg Action Loss, 12 batches: 0.032274345867335796\n",
            "Avg Block Loss, 7 batches: 0.06526127963193826\n",
            "Avg Challenge Loss, 8 batches: 0.06603067414835095\n",
            "Avg Card Loss, 11 batches: 0.02603225443850864\n",
            "0\n",
            "episode 219 of 1000\n",
            "epsilon: 0.33362200135903064\n",
            "gamma: 0.99\n",
            "Number of games in episode 219: 78\n",
            "Avg Action Loss, 12 batches: 0.03211164720899736\n",
            "Avg Block Loss, 7 batches: 0.04495839907654694\n",
            "Avg Challenge Loss, 8 batches: 0.0501004820689559\n",
            "Avg Card Loss, 12 batches: 0.03821823621789614\n",
            "0\n",
            "episode 220 of 1000\n",
            "epsilon: 0.33195389135223546\n",
            "gamma: 0.99\n",
            "Number of games in episode 220: 77\n",
            "Avg Action Loss, 12 batches: 0.03761935761819283\n",
            "Avg Block Loss, 7 batches: 0.07487393250422818\n",
            "Avg Challenge Loss, 7 batches: 0.0839401198817151\n",
            "Avg Card Loss, 11 batches: 0.0367775358767672\n",
            "0\n",
            "win rate: 0.25\n",
            "episode 221 of 1000\n",
            "epsilon: 0.3302941218954743\n",
            "gamma: 0.99\n",
            "Number of games in episode 221: 75\n",
            "Avg Action Loss, 12 batches: 0.034671351624031864\n",
            "Avg Block Loss, 8 batches: 0.03838264406658709\n",
            "Avg Challenge Loss, 8 batches: 0.0432372895302251\n",
            "Avg Card Loss, 12 batches: 0.03897650276000301\n",
            "0\n",
            "episode 222 of 1000\n",
            "epsilon: 0.32864265128599696\n",
            "gamma: 0.99\n",
            "Number of games in episode 222: 70\n",
            "Avg Action Loss, 12 batches: 0.02932726397799949\n",
            "Avg Block Loss, 7 batches: 0.049442718869873455\n",
            "Avg Challenge Loss, 8 batches: 0.057480036513879895\n",
            "Avg Card Loss, 11 batches: 0.03506642436100678\n",
            "0\n",
            "episode 223 of 1000\n",
            "epsilon: 0.326999438029567\n",
            "gamma: 0.99\n",
            "Number of games in episode 223: 76\n",
            "Avg Action Loss, 12 batches: 0.030438670112440985\n",
            "Avg Block Loss, 8 batches: 0.06281192722963169\n",
            "Avg Challenge Loss, 9 batches: 0.06194937167068323\n",
            "Avg Card Loss, 10 batches: 0.06414230037480592\n",
            "0\n",
            "episode 224 of 1000\n",
            "epsilon: 0.3253644408394192\n",
            "gamma: 0.99\n",
            "Number of games in episode 224: 73\n",
            "Avg Action Loss, 12 batches: 0.03445274289697409\n",
            "Avg Block Loss, 7 batches: 0.05324440875223705\n",
            "Avg Challenge Loss, 8 batches: 0.056334329303354025\n",
            "Avg Card Loss, 12 batches: 0.03786642608853678\n",
            "0\n",
            "episode 225 of 1000\n",
            "epsilon: 0.3237376186352221\n",
            "gamma: 0.99\n",
            "Number of games in episode 225: 70\n",
            "Avg Action Loss, 13 batches: 0.04600682897636524\n",
            "Avg Block Loss, 8 batches: 0.06791205797344446\n",
            "Avg Challenge Loss, 9 batches: 0.07571840037902196\n",
            "Avg Card Loss, 10 batches: 0.058538306318223474\n",
            "0\n",
            "episode 226 of 1000\n",
            "epsilon: 0.322118930542046\n",
            "gamma: 0.99\n",
            "Number of games in episode 226: 75\n",
            "Avg Action Loss, 13 batches: 0.03956964834091755\n",
            "Avg Block Loss, 8 batches: 0.04394329083152115\n",
            "Avg Challenge Loss, 9 batches: 0.06247165737052759\n",
            "Avg Card Loss, 11 batches: 0.04415756036442789\n",
            "0\n",
            "episode 227 of 1000\n",
            "epsilon: 0.32050833588933575\n",
            "gamma: 0.99\n",
            "Number of games in episode 227: 74\n",
            "Avg Action Loss, 13 batches: 0.03000646077383023\n",
            "Avg Block Loss, 8 batches: 0.051358852069824934\n",
            "Avg Challenge Loss, 9 batches: 0.06157036746541659\n",
            "Avg Card Loss, 11 batches: 0.04350974321873351\n",
            "0\n",
            "episode 228 of 1000\n",
            "epsilon: 0.31890579420988907\n",
            "gamma: 0.99\n",
            "Number of games in episode 228: 76\n",
            "Avg Action Loss, 13 batches: 0.03429689917426843\n",
            "Avg Block Loss, 8 batches: 0.07591996470000595\n",
            "Avg Challenge Loss, 9 batches: 0.07663131174114016\n",
            "Avg Card Loss, 10 batches: 0.06311867311596871\n",
            "0\n",
            "episode 229 of 1000\n",
            "epsilon: 0.3173112652388396\n",
            "gamma: 0.99\n",
            "Number of games in episode 229: 77\n",
            "Avg Action Loss, 11 batches: 0.04142737193879756\n",
            "Avg Block Loss, 7 batches: 0.06131514521049602\n",
            "Avg Challenge Loss, 8 batches: 0.06817590910941362\n",
            "Avg Card Loss, 13 batches: 0.029608643452565257\n",
            "0\n",
            "episode 230 of 1000\n",
            "epsilon: 0.3157247089126454\n",
            "gamma: 0.99\n",
            "Number of games in episode 230: 65\n",
            "Avg Action Loss, 12 batches: 0.041706111592551075\n",
            "Avg Block Loss, 7 batches: 0.06606293496276651\n",
            "Avg Challenge Loss, 8 batches: 0.06654476490803063\n",
            "Avg Card Loss, 9 batches: 0.06574291135701868\n",
            "0\n",
            "win rate: 0.14\n",
            "episode 231 of 1000\n",
            "epsilon: 0.3141460853680822\n",
            "gamma: 0.99\n",
            "Number of games in episode 231: 69\n",
            "Avg Action Loss, 11 batches: 0.03681916895915161\n",
            "Avg Block Loss, 7 batches: 0.046966582270605226\n",
            "Avg Challenge Loss, 8 batches: 0.05739771807566285\n",
            "Avg Card Loss, 10 batches: 0.03943769922479987\n",
            "0\n",
            "episode 232 of 1000\n",
            "epsilon: 0.3125753549412418\n",
            "gamma: 0.99\n",
            "Number of games in episode 232: 74\n",
            "Avg Action Loss, 12 batches: 0.038323084358125925\n",
            "Avg Block Loss, 7 batches: 0.06397123262286186\n",
            "Avg Challenge Loss, 8 batches: 0.0639376665931195\n",
            "Avg Card Loss, 11 batches: 0.05719713011587208\n",
            "0\n",
            "episode 233 of 1000\n",
            "epsilon: 0.31101247816653554\n",
            "gamma: 0.99\n",
            "Number of games in episode 233: 71\n",
            "Avg Action Loss, 13 batches: 0.03567916255157728\n",
            "Avg Block Loss, 9 batches: 0.06138451283590661\n",
            "Avg Challenge Loss, 9 batches: 0.06242830782300896\n",
            "Avg Card Loss, 10 batches: 0.04968928168527782\n",
            "0\n",
            "episode 234 of 1000\n",
            "epsilon: 0.30945741577570285\n",
            "gamma: 0.99\n",
            "Number of games in episode 234: 76\n",
            "Avg Action Loss, 12 batches: 0.04804841801524162\n",
            "Avg Block Loss, 8 batches: 0.046023024478927255\n",
            "Avg Challenge Loss, 8 batches: 0.056875084643252194\n",
            "Avg Card Loss, 11 batches: 0.059851799329573456\n",
            "0\n",
            "episode 235 of 1000\n",
            "epsilon: 0.3079101286968243\n",
            "gamma: 0.99\n",
            "Number of games in episode 235: 83\n",
            "Avg Action Loss, 11 batches: 0.04169753481718627\n",
            "Avg Block Loss, 7 batches: 0.08618464054805892\n",
            "Avg Challenge Loss, 8 batches: 0.07965885661542416\n",
            "Avg Card Loss, 12 batches: 0.03480020654387772\n",
            "0\n",
            "episode 236 of 1000\n",
            "epsilon: 0.3063705780533402\n",
            "gamma: 0.99\n",
            "Number of games in episode 236: 76\n",
            "Avg Action Loss, 12 batches: 0.0338476886972785\n",
            "Avg Block Loss, 8 batches: 0.0727327010827139\n",
            "Avg Challenge Loss, 9 batches: 0.08498821179899904\n",
            "Avg Card Loss, 11 batches: 0.054290649396452034\n",
            "0\n",
            "episode 237 of 1000\n",
            "epsilon: 0.30483872516307353\n",
            "gamma: 0.99\n",
            "Number of games in episode 237: 71\n",
            "Avg Action Loss, 12 batches: 0.03064002046206345\n",
            "Avg Block Loss, 8 batches: 0.03618196159368381\n",
            "Avg Challenge Loss, 8 batches: 0.043592675239779055\n",
            "Avg Card Loss, 11 batches: 0.03654081831601533\n",
            "0\n",
            "episode 238 of 1000\n",
            "epsilon: 0.3033145315372582\n",
            "gamma: 0.99\n",
            "Number of games in episode 238: 78\n",
            "Avg Action Loss, 12 batches: 0.03775860524425904\n",
            "Avg Block Loss, 8 batches: 0.09144309302791953\n",
            "Avg Challenge Loss, 9 batches: 0.09766663010749552\n",
            "Avg Card Loss, 11 batches: 0.06001958835192702\n",
            "0\n",
            "episode 239 of 1000\n",
            "epsilon: 0.3017979588795719\n",
            "gamma: 0.99\n",
            "Number of games in episode 239: 76\n",
            "Avg Action Loss, 12 batches: 0.024200994831820328\n",
            "Avg Block Loss, 8 batches: 0.0697379163466394\n",
            "Avg Challenge Loss, 8 batches: 0.07813200855161995\n",
            "Avg Card Loss, 11 batches: 0.05616070766171271\n",
            "0\n",
            "episode 240 of 1000\n",
            "epsilon: 0.30028896908517405\n",
            "gamma: 0.99\n",
            "Number of games in episode 240: 72\n",
            "Avg Action Loss, 11 batches: 0.029647880657152695\n",
            "Avg Block Loss, 7 batches: 0.07049252797982522\n",
            "Avg Challenge Loss, 8 batches: 0.07711340056266636\n",
            "Avg Card Loss, 11 batches: 0.03542256842113354\n",
            "0\n",
            "win rate: 0.28\n",
            "episode 241 of 1000\n",
            "epsilon: 0.2987875242397482\n",
            "gamma: 0.99\n",
            "Number of games in episode 241: 74\n",
            "Avg Action Loss, 11 batches: 0.03002718040211634\n",
            "Avg Block Loss, 8 batches: 0.04578507610131055\n",
            "Avg Challenge Loss, 8 batches: 0.059602567460387945\n",
            "Avg Card Loss, 11 batches: 0.04777804601260207\n",
            "0\n",
            "episode 242 of 1000\n",
            "epsilon: 0.29729358661854943\n",
            "gamma: 0.99\n",
            "Number of games in episode 242: 75\n",
            "Avg Action Loss, 12 batches: 0.027722071545819443\n",
            "Avg Block Loss, 8 batches: 0.05765534262172878\n",
            "Avg Challenge Loss, 9 batches: 0.0565192892940508\n",
            "Avg Card Loss, 11 batches: 0.055557671849700535\n",
            "0\n",
            "episode 243 of 1000\n",
            "epsilon: 0.29580711868545667\n",
            "gamma: 0.99\n",
            "Number of games in episode 243: 78\n",
            "Avg Action Loss, 11 batches: 0.04412726956334981\n",
            "Avg Block Loss, 8 batches: 0.10155228455550969\n",
            "Avg Challenge Loss, 9 batches: 0.1076616785592503\n",
            "Avg Card Loss, 12 batches: 0.06830698259485264\n",
            "0\n",
            "episode 244 of 1000\n",
            "epsilon: 0.2943280830920294\n",
            "gamma: 0.99\n",
            "Number of games in episode 244: 71\n",
            "Avg Action Loss, 13 batches: 0.030893182238707177\n",
            "Avg Block Loss, 9 batches: 0.044579338230606586\n",
            "Avg Challenge Loss, 9 batches: 0.042907841089699\n",
            "Avg Card Loss, 10 batches: 0.03587503861635923\n",
            "0\n",
            "episode 245 of 1000\n",
            "epsilon: 0.29285644267656924\n",
            "gamma: 0.99\n",
            "Number of games in episode 245: 69\n",
            "Avg Action Loss, 11 batches: 0.03224983743645928\n",
            "Avg Block Loss, 8 batches: 0.06522920785937458\n",
            "Avg Challenge Loss, 8 batches: 0.06911688833497465\n",
            "Avg Card Loss, 11 batches: 0.05874730976806446\n",
            "0\n",
            "episode 246 of 1000\n",
            "epsilon: 0.2913921604631864\n",
            "gamma: 0.99\n",
            "Number of games in episode 246: 73\n",
            "Avg Action Loss, 13 batches: 0.03145981393754482\n",
            "Avg Block Loss, 8 batches: 0.06307441019453108\n",
            "Avg Challenge Loss, 8 batches: 0.0691787701798603\n",
            "Avg Card Loss, 11 batches: 0.049146146259524605\n",
            "0\n",
            "episode 247 of 1000\n",
            "epsilon: 0.28993519966087045\n",
            "gamma: 0.99\n",
            "Number of games in episode 247: 72\n",
            "Avg Action Loss, 12 batches: 0.035072686073059835\n",
            "Avg Block Loss, 8 batches: 0.07866386603564024\n",
            "Avg Challenge Loss, 9 batches: 0.0754769260270728\n",
            "Avg Card Loss, 12 batches: 0.04923745609509448\n",
            "0\n",
            "episode 248 of 1000\n",
            "epsilon: 0.2884855236625661\n",
            "gamma: 0.99\n",
            "Number of games in episode 248: 74\n",
            "Avg Action Loss, 12 batches: 0.037360683626805745\n",
            "Avg Block Loss, 9 batches: 0.0460637208695213\n",
            "Avg Challenge Loss, 9 batches: 0.044749649345046945\n",
            "Avg Card Loss, 11 batches: 0.04759107847613367\n",
            "0\n",
            "episode 249 of 1000\n",
            "epsilon: 0.28704309604425327\n",
            "gamma: 0.99\n",
            "Number of games in episode 249: 72\n",
            "Avg Action Loss, 13 batches: 0.03574747799967344\n",
            "Avg Block Loss, 8 batches: 0.05168434395454824\n",
            "Avg Challenge Loss, 8 batches: 0.05498711275868118\n",
            "Avg Card Loss, 11 batches: 0.05502725824375044\n",
            "0\n",
            "episode 250 of 1000\n",
            "epsilon: 0.285607880564032\n",
            "gamma: 0.99\n",
            "Number of games in episode 250: 78\n",
            "Avg Action Loss, 11 batches: 0.026273638687350533\n",
            "Avg Block Loss, 8 batches: 0.08610134769696742\n",
            "Avg Challenge Loss, 9 batches: 0.08821706101298332\n",
            "Avg Card Loss, 12 batches: 0.04376563988626003\n",
            "0\n",
            "win rate: 0.21\n",
            "episode 251 of 1000\n",
            "epsilon: 0.28417984116121187\n",
            "gamma: 0.99\n",
            "Number of games in episode 251: 75\n",
            "Avg Action Loss, 13 batches: 0.018660379716983207\n",
            "Avg Block Loss, 9 batches: 0.04708668103234635\n",
            "Avg Challenge Loss, 9 batches: 0.049502781902750335\n",
            "Avg Card Loss, 11 batches: 0.037064540538598194\n",
            "0\n",
            "episode 252 of 1000\n",
            "epsilon: 0.2827589419554058\n",
            "gamma: 0.99\n",
            "Number of games in episode 252: 76\n",
            "Avg Action Loss, 13 batches: 0.03957532496693043\n",
            "Avg Block Loss, 9 batches: 0.06003777599996991\n",
            "Avg Challenge Loss, 10 batches: 0.06074200458824634\n",
            "Avg Card Loss, 10 batches: 0.060602863878011705\n",
            "0\n",
            "episode 253 of 1000\n",
            "epsilon: 0.28134514724562876\n",
            "gamma: 0.99\n",
            "Number of games in episode 253: 72\n",
            "Avg Action Loss, 12 batches: 0.03733658495669564\n",
            "Avg Block Loss, 8 batches: 0.06703087082132697\n",
            "Avg Challenge Loss, 9 batches: 0.07154740786386861\n",
            "Avg Card Loss, 10 batches: 0.053690292546525596\n",
            "0\n",
            "episode 254 of 1000\n",
            "epsilon: 0.2799384215094006\n",
            "gamma: 0.99\n",
            "Number of games in episode 254: 70\n",
            "Avg Action Loss, 12 batches: 0.031884591560810804\n",
            "Avg Block Loss, 8 batches: 0.09063712251372635\n",
            "Avg Challenge Loss, 9 batches: 0.08263626239366001\n",
            "Avg Card Loss, 9 batches: 0.05700756340391106\n",
            "0\n",
            "episode 255 of 1000\n",
            "epsilon: 0.27853872940185365\n",
            "gamma: 0.99\n",
            "Number of games in episode 255: 74\n",
            "Avg Action Loss, 11 batches: 0.02672991893169555\n",
            "Avg Block Loss, 7 batches: 0.07295261855636324\n",
            "Avg Challenge Loss, 7 batches: 0.06634854259235519\n",
            "Avg Card Loss, 11 batches: 0.04106941002167084\n",
            "0\n",
            "episode 256 of 1000\n",
            "epsilon: 0.27714603575484437\n",
            "gamma: 0.99\n",
            "Number of games in episode 256: 75\n",
            "Avg Action Loss, 11 batches: 0.02707966281609102\n",
            "Avg Block Loss, 8 batches: 0.03670128993690014\n",
            "Avg Challenge Loss, 8 batches: 0.03960331145208329\n",
            "Avg Card Loss, 11 batches: 0.030613120645284653\n",
            "0\n",
            "episode 257 of 1000\n",
            "epsilon: 0.2757603055760701\n",
            "gamma: 0.99\n",
            "Number of games in episode 257: 76\n",
            "Avg Action Loss, 12 batches: 0.03870985431907078\n",
            "Avg Block Loss, 8 batches: 0.04493191582150757\n",
            "Avg Challenge Loss, 9 batches: 0.054292802388469376\n",
            "Avg Card Loss, 11 batches: 0.04642398113554174\n",
            "0\n",
            "episode 258 of 1000\n",
            "epsilon: 0.2743815040481898\n",
            "gamma: 0.99\n",
            "Number of games in episode 258: 67\n",
            "Avg Action Loss, 11 batches: 0.03490572181445631\n",
            "Avg Block Loss, 7 batches: 0.06616228679195046\n",
            "Avg Challenge Loss, 7 batches: 0.07420171132045132\n",
            "Avg Card Loss, 11 batches: 0.0464282776652412\n",
            "0\n",
            "episode 259 of 1000\n",
            "epsilon: 0.2730095965279488\n",
            "gamma: 0.99\n",
            "Number of games in episode 259: 75\n",
            "Avg Action Loss, 12 batches: 0.03718542563728988\n",
            "Avg Block Loss, 8 batches: 0.05088602867908776\n",
            "Avg Challenge Loss, 9 batches: 0.05564491295566162\n",
            "Avg Card Loss, 11 batches: 0.03378601469607516\n",
            "0\n",
            "episode 260 of 1000\n",
            "epsilon: 0.27164454854530906\n",
            "gamma: 0.99\n",
            "Number of games in episode 260: 72\n",
            "Avg Action Loss, 13 batches: 0.0343784002157358\n",
            "Avg Block Loss, 8 batches: 0.03615719568915665\n",
            "Avg Challenge Loss, 8 batches: 0.0382444771239534\n",
            "Avg Card Loss, 11 batches: 0.04082764981483871\n",
            "0\n",
            "win rate: 0.21\n",
            "episode 261 of 1000\n",
            "epsilon: 0.2702863258025825\n",
            "gamma: 0.99\n",
            "Number of games in episode 261: 74\n",
            "Avg Action Loss, 11 batches: 0.03724851476197893\n",
            "Avg Block Loss, 7 batches: 0.0522781064999955\n",
            "Avg Challenge Loss, 8 batches: 0.0597010210622102\n",
            "Avg Card Loss, 11 batches: 0.03740620858628641\n",
            "0\n",
            "episode 262 of 1000\n",
            "epsilon: 0.2689348941735696\n",
            "gamma: 0.99\n",
            "Number of games in episode 262: 76\n",
            "Avg Action Loss, 13 batches: 0.045418654926694356\n",
            "Avg Block Loss, 8 batches: 0.06122151995077729\n",
            "Avg Challenge Loss, 9 batches: 0.0696209106180403\n",
            "Avg Card Loss, 11 batches: 0.032411129721863705\n",
            "0\n",
            "episode 263 of 1000\n",
            "epsilon: 0.26759021970270175\n",
            "gamma: 0.99\n",
            "Number of games in episode 263: 72\n",
            "Avg Action Loss, 12 batches: 0.04757409527276953\n",
            "Avg Block Loss, 8 batches: 0.05989994644187391\n",
            "Avg Challenge Loss, 9 batches: 0.0671271723177698\n",
            "Avg Card Loss, 10 batches: 0.05626102481037378\n",
            "0\n",
            "episode 264 of 1000\n",
            "epsilon: 0.2662522686041882\n",
            "gamma: 0.99\n",
            "Number of games in episode 264: 71\n",
            "Avg Action Loss, 12 batches: 0.03923823622365793\n",
            "Avg Block Loss, 8 batches: 0.06286164093762636\n",
            "Avg Challenge Loss, 8 batches: 0.06991575798019767\n",
            "Avg Card Loss, 11 batches: 0.044835244813425976\n",
            "0\n",
            "episode 265 of 1000\n",
            "epsilon: 0.2649210072611673\n",
            "gamma: 0.99\n",
            "Number of games in episode 265: 80\n",
            "Avg Action Loss, 12 batches: 0.04233338451012969\n",
            "Avg Block Loss, 8 batches: 0.09493977064266801\n",
            "Avg Challenge Loss, 9 batches: 0.09484552426470651\n",
            "Avg Card Loss, 12 batches: 0.052618471905589104\n",
            "0\n",
            "episode 266 of 1000\n",
            "epsilon: 0.26359640222486147\n",
            "gamma: 0.99\n",
            "Number of games in episode 266: 67\n",
            "Avg Action Loss, 13 batches: 0.02900922656632387\n",
            "Avg Block Loss, 8 batches: 0.07150677964091301\n",
            "Avg Challenge Loss, 9 batches: 0.0727785923000839\n",
            "Avg Card Loss, 9 batches: 0.05411063589983516\n",
            "0\n",
            "episode 267 of 1000\n",
            "epsilon: 0.26227842021373715\n",
            "gamma: 0.99\n",
            "Number of games in episode 267: 75\n",
            "Avg Action Loss, 12 batches: 0.02650564183325817\n",
            "Avg Block Loss, 8 batches: 0.049766633310355246\n",
            "Avg Challenge Loss, 8 batches: 0.045714714215137064\n",
            "Avg Card Loss, 12 batches: 0.040975042463590704\n",
            "0\n",
            "episode 268 of 1000\n",
            "epsilon: 0.2609670281126685\n",
            "gamma: 0.99\n",
            "Number of games in episode 268: 68\n",
            "Avg Action Loss, 13 batches: 0.03261346162225191\n",
            "Avg Block Loss, 8 batches: 0.0641993002500385\n",
            "Avg Challenge Loss, 9 batches: 0.06774795759055349\n",
            "Avg Card Loss, 10 batches: 0.052571815252304074\n",
            "0\n",
            "episode 269 of 1000\n",
            "epsilon: 0.25966219297210513\n",
            "gamma: 0.99\n",
            "Number of games in episode 269: 74\n",
            "Avg Action Loss, 12 batches: 0.04335441145425042\n",
            "Avg Block Loss, 8 batches: 0.0750991115346551\n",
            "Avg Challenge Loss, 9 batches: 0.07862902970777617\n",
            "Avg Card Loss, 10 batches: 0.04802284701727331\n",
            "0\n",
            "episode 270 of 1000\n",
            "epsilon: 0.2583638820072446\n",
            "gamma: 0.99\n",
            "Number of games in episode 270: 75\n",
            "Avg Action Loss, 12 batches: 0.04198411285566787\n",
            "Avg Block Loss, 7 batches: 0.037249641093824594\n",
            "Avg Challenge Loss, 8 batches: 0.04813947482034564\n",
            "Avg Card Loss, 11 batches: 0.03362319305200468\n",
            "0\n",
            "win rate: 0.26\n",
            "episode 271 of 1000\n",
            "epsilon: 0.2570720625972084\n",
            "gamma: 0.99\n",
            "Number of games in episode 271: 64\n",
            "Avg Action Loss, 12 batches: 0.03511935177569588\n",
            "Avg Block Loss, 8 batches: 0.05155596602708101\n",
            "Avg Challenge Loss, 8 batches: 0.055420571472495794\n",
            "Avg Card Loss, 10 batches: 0.03562395721673965\n",
            "0\n",
            "episode 272 of 1000\n",
            "epsilon: 0.25578670228422234\n",
            "gamma: 0.99\n",
            "Number of games in episode 272: 70\n",
            "Avg Action Loss, 13 batches: 0.03618948261898298\n",
            "Avg Block Loss, 7 batches: 0.0880971769137042\n",
            "Avg Challenge Loss, 8 batches: 0.08242878352757543\n",
            "Avg Card Loss, 9 batches: 0.07245164240399997\n",
            "0\n",
            "episode 273 of 1000\n",
            "epsilon: 0.25450776877280124\n",
            "gamma: 0.99\n",
            "Number of games in episode 273: 75\n",
            "Avg Action Loss, 12 batches: 0.047187015414237976\n",
            "Avg Block Loss, 7 batches: 0.06497808652264732\n",
            "Avg Challenge Loss, 8 batches: 0.06600957317277789\n",
            "Avg Card Loss, 11 batches: 0.042034303430806504\n",
            "0\n",
            "episode 274 of 1000\n",
            "epsilon: 0.2532352299289372\n",
            "gamma: 0.99\n",
            "Number of games in episode 274: 78\n",
            "Avg Action Loss, 12 batches: 0.027026628842577338\n",
            "Avg Block Loss, 8 batches: 0.06948255992028862\n",
            "Avg Challenge Loss, 9 batches: 0.07445223753650983\n",
            "Avg Card Loss, 12 batches: 0.03290598055658241\n",
            "0\n",
            "episode 275 of 1000\n",
            "epsilon: 0.2519690537792925\n",
            "gamma: 0.99\n",
            "Number of games in episode 275: 72\n",
            "Avg Action Loss, 12 batches: 0.04144337447360158\n",
            "Avg Block Loss, 7 batches: 0.046176890177386146\n",
            "Avg Challenge Loss, 7 batches: 0.05729582240538938\n",
            "Avg Card Loss, 10 batches: 0.026680076075717808\n",
            "0\n",
            "episode 276 of 1000\n",
            "epsilon: 0.2507092085103961\n",
            "gamma: 0.99\n",
            "Number of games in episode 276: 74\n",
            "Avg Action Loss, 11 batches: 0.03507970180362463\n",
            "Avg Block Loss, 8 batches: 0.0372755469288677\n",
            "Avg Challenge Loss, 9 batches: 0.05189207258323828\n",
            "Avg Card Loss, 11 batches: 0.05169150229035453\n",
            "0\n",
            "episode 277 of 1000\n",
            "epsilon: 0.2494556624678441\n",
            "gamma: 0.99\n",
            "Number of games in episode 277: 68\n",
            "Avg Action Loss, 12 batches: 0.04229556008552512\n",
            "Avg Block Loss, 7 batches: 0.04376243454005037\n",
            "Avg Challenge Loss, 8 batches: 0.061287764459848404\n",
            "Avg Card Loss, 10 batches: 0.026926496415399015\n",
            "0\n",
            "episode 278 of 1000\n",
            "epsilon: 0.24820838415550486\n",
            "gamma: 0.99\n",
            "Number of games in episode 278: 72\n",
            "Avg Action Loss, 12 batches: 0.0324458833783865\n",
            "Avg Block Loss, 8 batches: 0.05664816981879994\n",
            "Avg Challenge Loss, 8 batches: 0.055141969001851976\n",
            "Avg Card Loss, 11 batches: 0.045182887761091646\n",
            "0\n",
            "episode 279 of 1000\n",
            "epsilon: 0.24696734223472733\n",
            "gamma: 0.99\n",
            "Number of games in episode 279: 64\n",
            "Avg Action Loss, 11 batches: 0.03362667924639853\n",
            "Avg Block Loss, 7 batches: 0.08444374799728394\n",
            "Avg Challenge Loss, 8 batches: 0.08681856328621507\n",
            "Avg Card Loss, 10 batches: 0.04667936032637954\n",
            "0\n",
            "episode 280 of 1000\n",
            "epsilon: 0.2457325055235537\n",
            "gamma: 0.99\n",
            "Number of games in episode 280: 74\n",
            "Avg Action Loss, 12 batches: 0.03396483495210608\n",
            "Avg Block Loss, 8 batches: 0.050762612372636795\n",
            "Avg Challenge Loss, 9 batches: 0.059231774571041264\n",
            "Avg Card Loss, 12 batches: 0.03782753095341226\n",
            "0\n",
            "win rate: 0.28\n",
            "episode 281 of 1000\n",
            "epsilon: 0.24450384299593592\n",
            "gamma: 0.99\n",
            "Number of games in episode 281: 78\n",
            "Avg Action Loss, 12 batches: 0.026514560411063332\n",
            "Avg Block Loss, 7 batches: 0.11692684995276588\n",
            "Avg Challenge Loss, 8 batches: 0.11072350991889834\n",
            "Avg Card Loss, 11 batches: 0.0619473945857449\n",
            "0\n",
            "episode 282 of 1000\n",
            "epsilon: 0.24328132378095624\n",
            "gamma: 0.99\n",
            "Number of games in episode 282: 76\n",
            "Avg Action Loss, 13 batches: 0.039691110069935136\n",
            "Avg Block Loss, 9 batches: 0.06435254443850782\n",
            "Avg Challenge Loss, 9 batches: 0.07109519383973545\n",
            "Avg Card Loss, 11 batches: 0.05265547419813546\n",
            "0\n",
            "episode 283 of 1000\n",
            "epsilon: 0.24206491716205145\n",
            "gamma: 0.99\n",
            "Number of games in episode 283: 72\n",
            "Avg Action Loss, 12 batches: 0.029933857809131343\n",
            "Avg Block Loss, 8 batches: 0.06512603757437319\n",
            "Avg Challenge Loss, 8 batches: 0.06781857460737228\n",
            "Avg Card Loss, 11 batches: 0.03860987655141137\n",
            "0\n",
            "episode 284 of 1000\n",
            "epsilon: 0.2408545925762412\n",
            "gamma: 0.99\n",
            "Number of games in episode 284: 74\n",
            "Avg Action Loss, 12 batches: 0.03370313912940522\n",
            "Avg Block Loss, 8 batches: 0.04940476221963763\n",
            "Avg Challenge Loss, 9 batches: 0.06348937066892783\n",
            "Avg Card Loss, 11 batches: 0.04213337168436159\n",
            "0\n",
            "episode 285 of 1000\n",
            "epsilon: 0.23965031961336\n",
            "gamma: 0.99\n",
            "Number of games in episode 285: 75\n",
            "Avg Action Loss, 11 batches: 0.0297387682155452\n",
            "Avg Block Loss, 8 batches: 0.06255740532651544\n",
            "Avg Challenge Loss, 8 batches: 0.06541070423554629\n",
            "Avg Card Loss, 11 batches: 0.04619499503380873\n",
            "0\n",
            "episode 286 of 1000\n",
            "epsilon: 0.2384520680152932\n",
            "gamma: 0.99\n",
            "Number of games in episode 286: 70\n",
            "Avg Action Loss, 11 batches: 0.02326154505664652\n",
            "Avg Block Loss, 7 batches: 0.06251445332808155\n",
            "Avg Challenge Loss, 8 batches: 0.07222305587492883\n",
            "Avg Card Loss, 11 batches: 0.05862269127233462\n",
            "0\n",
            "episode 287 of 1000\n",
            "epsilon: 0.23725980767521673\n",
            "gamma: 0.99\n",
            "Number of games in episode 287: 82\n",
            "Avg Action Loss, 13 batches: 0.027564726268442776\n",
            "Avg Block Loss, 8 batches: 0.06168912455905229\n",
            "Avg Challenge Loss, 9 batches: 0.06503918104701573\n",
            "Avg Card Loss, 12 batches: 0.04240550314231465\n",
            "0\n",
            "episode 288 of 1000\n",
            "epsilon: 0.23607350863684065\n",
            "gamma: 0.99\n",
            "Number of games in episode 288: 69\n",
            "Avg Action Loss, 12 batches: 0.031090467392156523\n",
            "Avg Block Loss, 8 batches: 0.05949499434791505\n",
            "Avg Challenge Loss, 9 batches: 0.060218703312178455\n",
            "Avg Card Loss, 11 batches: 0.04506172121248462\n",
            "0\n",
            "episode 289 of 1000\n",
            "epsilon: 0.23489314109365644\n",
            "gamma: 0.99\n",
            "Number of games in episode 289: 76\n",
            "Avg Action Loss, 12 batches: 0.027218946799015004\n",
            "Avg Block Loss, 8 batches: 0.0853991472395137\n",
            "Avg Challenge Loss, 9 batches: 0.08875584892100757\n",
            "Avg Card Loss, 11 batches: 0.0431238707493652\n",
            "0\n",
            "episode 290 of 1000\n",
            "epsilon: 0.23371867538818816\n",
            "gamma: 0.99\n",
            "Number of games in episode 290: 74\n",
            "Avg Action Loss, 11 batches: 0.025333283638412304\n",
            "Avg Block Loss, 8 batches: 0.06533249269705266\n",
            "Avg Challenge Loss, 8 batches: 0.06495947996154428\n",
            "Avg Card Loss, 12 batches: 0.03157779671406994\n",
            "0\n",
            "win rate: 0.23\n",
            "episode 291 of 1000\n",
            "epsilon: 0.23255008201124722\n",
            "gamma: 0.99\n",
            "Number of games in episode 291: 79\n",
            "Avg Action Loss, 12 batches: 0.04208594731365641\n",
            "Avg Block Loss, 8 batches: 0.05733146658167243\n",
            "Avg Challenge Loss, 8 batches: 0.06064165732823312\n",
            "Avg Card Loss, 13 batches: 0.042826903625749625\n",
            "0\n",
            "episode 292 of 1000\n",
            "epsilon: 0.231387331601191\n",
            "gamma: 0.99\n",
            "Number of games in episode 292: 77\n",
            "Avg Action Loss, 12 batches: 0.029590518213808537\n",
            "Avg Block Loss, 7 batches: 0.03457368165254593\n",
            "Avg Challenge Loss, 8 batches: 0.04539882973767817\n",
            "Avg Card Loss, 12 batches: 0.04729899438098073\n",
            "0\n",
            "episode 293 of 1000\n",
            "epsilon: 0.23023039494318503\n",
            "gamma: 0.99\n",
            "Number of games in episode 293: 78\n",
            "Avg Action Loss, 12 batches: 0.027266291591028374\n",
            "Avg Block Loss, 8 batches: 0.07056018029106781\n",
            "Avg Challenge Loss, 8 batches: 0.07430985197424889\n",
            "Avg Card Loss, 12 batches: 0.049894825322553515\n",
            "0\n",
            "episode 294 of 1000\n",
            "epsilon: 0.2290792429684691\n",
            "gamma: 0.99\n",
            "Number of games in episode 294: 77\n",
            "Avg Action Loss, 12 batches: 0.03060742498685916\n",
            "Avg Block Loss, 8 batches: 0.07340720004867762\n",
            "Avg Challenge Loss, 9 batches: 0.06623131740424368\n",
            "Avg Card Loss, 11 batches: 0.05742452247068286\n",
            "0\n",
            "episode 295 of 1000\n",
            "epsilon: 0.22793384675362674\n",
            "gamma: 0.99\n",
            "Number of games in episode 295: 74\n",
            "Avg Action Loss, 11 batches: 0.03509811041030017\n",
            "Avg Block Loss, 7 batches: 0.03880147276712315\n",
            "Avg Challenge Loss, 7 batches: 0.030186906856085573\n",
            "Avg Card Loss, 12 batches: 0.020582098960100364\n",
            "0\n",
            "episode 296 of 1000\n",
            "epsilon: 0.22679417751985861\n",
            "gamma: 0.99\n",
            "Number of games in episode 296: 70\n",
            "Avg Action Loss, 13 batches: 0.05025652667077688\n",
            "Avg Block Loss, 8 batches: 0.04698747186921537\n",
            "Avg Challenge Loss, 9 batches: 0.054545069113373756\n",
            "Avg Card Loss, 10 batches: 0.06869849096983671\n",
            "0\n",
            "episode 297 of 1000\n",
            "epsilon: 0.22566020663225933\n",
            "gamma: 0.99\n",
            "Number of games in episode 297: 73\n",
            "Avg Action Loss, 12 batches: 0.03448451845906675\n",
            "Avg Block Loss, 7 batches: 0.06906698417982884\n",
            "Avg Challenge Loss, 8 batches: 0.07163121085613966\n",
            "Avg Card Loss, 11 batches: 0.04824662386355075\n",
            "0\n",
            "episode 298 of 1000\n",
            "epsilon: 0.22453190559909803\n",
            "gamma: 0.99\n",
            "Number of games in episode 298: 73\n",
            "Avg Action Loss, 11 batches: 0.03310174503448335\n",
            "Avg Block Loss, 7 batches: 0.03851600104410734\n",
            "Avg Challenge Loss, 8 batches: 0.03371985099511221\n",
            "Avg Card Loss, 11 batches: 0.038238493386994706\n",
            "0\n",
            "episode 299 of 1000\n",
            "epsilon: 0.22340924607110255\n",
            "gamma: 0.99\n",
            "Number of games in episode 299: 76\n",
            "Avg Action Loss, 12 batches: 0.0297409378302594\n",
            "Avg Block Loss, 8 batches: 0.04236323363147676\n",
            "Avg Challenge Loss, 9 batches: 0.051536760499907866\n",
            "Avg Card Loss, 11 batches: 0.06683246545832265\n",
            "0\n",
            "episode 300 of 1000\n",
            "epsilon: 0.22229219984074702\n",
            "gamma: 0.99\n",
            "Number of games in episode 300: 76\n",
            "Avg Action Loss, 13 batches: 0.03893747101896084\n",
            "Avg Block Loss, 8 batches: 0.056691064266487956\n",
            "Avg Challenge Loss, 9 batches: 0.0592967135210832\n",
            "Avg Card Loss, 11 batches: 0.06293212944133715\n",
            "0\n",
            "win rate: 0.21\n",
            "episode 301 of 1000\n",
            "epsilon: 0.2211807388415433\n",
            "gamma: 0.99\n",
            "Number of games in episode 301: 72\n",
            "Avg Action Loss, 14 batches: 0.041405263756002696\n",
            "Avg Block Loss, 8 batches: 0.050271847168914974\n",
            "Avg Challenge Loss, 8 batches: 0.05610474449349567\n",
            "Avg Card Loss, 10 batches: 0.04702369859442115\n",
            "0\n",
            "episode 302 of 1000\n",
            "epsilon: 0.22007483514733558\n",
            "gamma: 0.99\n",
            "Number of games in episode 302: 77\n",
            "Avg Action Loss, 12 batches: 0.03794522024691105\n",
            "Avg Block Loss, 7 batches: 0.07672994716891221\n",
            "Avg Challenge Loss, 8 batches: 0.08067008364014328\n",
            "Avg Card Loss, 12 batches: 0.05280132972014447\n",
            "0\n",
            "episode 303 of 1000\n",
            "epsilon: 0.2189744609715989\n",
            "gamma: 0.99\n",
            "Number of games in episode 303: 74\n",
            "Avg Action Loss, 13 batches: 0.02194170983364949\n",
            "Avg Block Loss, 8 batches: 0.05905481765512377\n",
            "Avg Challenge Loss, 9 batches: 0.06519830185506079\n",
            "Avg Card Loss, 10 batches: 0.04625003328546882\n",
            "0\n",
            "episode 304 of 1000\n",
            "epsilon: 0.2178795886667409\n",
            "gamma: 0.99\n",
            "Number of games in episode 304: 76\n",
            "Avg Action Loss, 13 batches: 0.025987733155488968\n",
            "Avg Block Loss, 9 batches: 0.04989406787272957\n",
            "Avg Challenge Loss, 10 batches: 0.05769531074911356\n",
            "Avg Card Loss, 11 batches: 0.05828664621168917\n",
            "0\n",
            "episode 305 of 1000\n",
            "epsilon: 0.2167901907234072\n",
            "gamma: 0.99\n",
            "Number of games in episode 305: 75\n",
            "Avg Action Loss, 12 batches: 0.035149708933507405\n",
            "Avg Block Loss, 8 batches: 0.07523568044416606\n",
            "Avg Challenge Loss, 9 batches: 0.07089690408772892\n",
            "Avg Card Loss, 10 batches: 0.0558028545230627\n",
            "0\n",
            "episode 306 of 1000\n",
            "epsilon: 0.21570623976979014\n",
            "gamma: 0.99\n",
            "Number of games in episode 306: 72\n",
            "Avg Action Loss, 12 batches: 0.03589546882236997\n",
            "Avg Block Loss, 8 batches: 0.05299356335308403\n",
            "Avg Challenge Loss, 9 batches: 0.05685449308819241\n",
            "Avg Card Loss, 11 batches: 0.03525102180852131\n",
            "0\n",
            "episode 307 of 1000\n",
            "epsilon: 0.21462770857094118\n",
            "gamma: 0.99\n",
            "Number of games in episode 307: 73\n",
            "Avg Action Loss, 12 batches: 0.043423618810872235\n",
            "Avg Block Loss, 8 batches: 0.07923353696241975\n",
            "Avg Challenge Loss, 9 batches: 0.08961110955311193\n",
            "Avg Card Loss, 11 batches: 0.03448377376083623\n",
            "0\n",
            "episode 308 of 1000\n",
            "epsilon: 0.21355457002808648\n",
            "gamma: 0.99\n",
            "Number of games in episode 308: 73\n",
            "Avg Action Loss, 12 batches: 0.026976802619174123\n",
            "Avg Block Loss, 8 batches: 0.07116944005247205\n",
            "Avg Challenge Loss, 8 batches: 0.07880067871883512\n",
            "Avg Card Loss, 12 batches: 0.047378830378875136\n",
            "0\n",
            "episode 309 of 1000\n",
            "epsilon: 0.21248679717794605\n",
            "gamma: 0.99\n",
            "Number of games in episode 309: 77\n",
            "Avg Action Loss, 12 batches: 0.03861370822414756\n",
            "Avg Block Loss, 8 batches: 0.068334745708853\n",
            "Avg Challenge Loss, 9 batches: 0.07128132465812895\n",
            "Avg Card Loss, 12 batches: 0.049793354235589504\n",
            "0\n",
            "episode 310 of 1000\n",
            "epsilon: 0.21142436319205632\n",
            "gamma: 0.99\n",
            "Number of games in episode 310: 69\n",
            "Avg Action Loss, 12 batches: 0.03343667152027289\n",
            "Avg Block Loss, 7 batches: 0.07005114321197782\n",
            "Avg Challenge Loss, 8 batches: 0.07159088202752173\n",
            "Avg Card Loss, 10 batches: 0.045698579866439104\n",
            "0\n",
            "win rate: 0.21\n",
            "episode 311 of 1000\n",
            "epsilon: 0.21036724137609603\n",
            "gamma: 0.99\n",
            "Number of games in episode 311: 77\n",
            "Avg Action Loss, 13 batches: 0.03772006370127201\n",
            "Avg Block Loss, 9 batches: 0.09201770420703623\n",
            "Avg Challenge Loss, 10 batches: 0.10023242235183716\n",
            "Avg Card Loss, 11 batches: 0.07214306836778467\n",
            "0\n",
            "episode 312 of 1000\n",
            "epsilon: 0.20931540516921554\n",
            "gamma: 0.99\n",
            "Number of games in episode 312: 70\n",
            "Avg Action Loss, 13 batches: 0.03948077430518774\n",
            "Avg Block Loss, 8 batches: 0.07813566806726158\n",
            "Avg Challenge Loss, 9 batches: 0.0855447542336252\n",
            "Avg Card Loss, 10 batches: 0.07163601173087955\n",
            "0\n",
            "episode 313 of 1000\n",
            "epsilon: 0.20826882814336947\n",
            "gamma: 0.99\n",
            "Number of games in episode 313: 75\n",
            "Avg Action Loss, 13 batches: 0.0305981977054706\n",
            "Avg Block Loss, 9 batches: 0.05673213768750429\n",
            "Avg Challenge Loss, 9 batches: 0.05581915626923243\n",
            "Avg Card Loss, 11 batches: 0.047228794058107516\n",
            "0\n",
            "episode 314 of 1000\n",
            "epsilon: 0.20722748400265262\n",
            "gamma: 0.99\n",
            "Number of games in episode 314: 73\n",
            "Avg Action Loss, 12 batches: 0.026525755723317463\n",
            "Avg Block Loss, 8 batches: 0.055379909463226795\n",
            "Avg Challenge Loss, 8 batches: 0.05243555223569274\n",
            "Avg Card Loss, 11 batches: 0.04804715249602767\n",
            "0\n",
            "episode 315 of 1000\n",
            "epsilon: 0.20619134658263935\n",
            "gamma: 0.99\n",
            "Number of games in episode 315: 70\n",
            "Avg Action Loss, 12 batches: 0.03533643150391678\n",
            "Avg Block Loss, 8 batches: 0.05443147092591971\n",
            "Avg Challenge Loss, 9 batches: 0.05941082599262396\n",
            "Avg Card Loss, 11 batches: 0.042827336117625237\n",
            "0\n",
            "episode 316 of 1000\n",
            "epsilon: 0.20516038984972615\n",
            "gamma: 0.99\n",
            "Number of games in episode 316: 74\n",
            "Avg Action Loss, 12 batches: 0.0319375378312543\n",
            "Avg Block Loss, 8 batches: 0.04482492315582931\n",
            "Avg Challenge Loss, 9 batches: 0.048207918285495706\n",
            "Avg Card Loss, 12 batches: 0.051593850987652935\n",
            "0\n",
            "episode 317 of 1000\n",
            "epsilon: 0.2041345879004775\n",
            "gamma: 0.99\n",
            "Number of games in episode 317: 77\n",
            "Avg Action Loss, 12 batches: 0.03870544504995147\n",
            "Avg Block Loss, 8 batches: 0.06233750772662461\n",
            "Avg Challenge Loss, 8 batches: 0.07017655251547694\n",
            "Avg Card Loss, 11 batches: 0.05079468407414176\n",
            "0\n",
            "episode 318 of 1000\n",
            "epsilon: 0.2031139149609751\n",
            "gamma: 0.99\n",
            "Number of games in episode 318: 77\n",
            "Avg Action Loss, 12 batches: 0.047365978049735226\n",
            "Avg Block Loss, 8 batches: 0.062382904812693596\n",
            "Avg Challenge Loss, 9 batches: 0.07040851832264\n",
            "Avg Card Loss, 11 batches: 0.03342983486469497\n",
            "0\n",
            "episode 319 of 1000\n",
            "epsilon: 0.20209834538617025\n",
            "gamma: 0.99\n",
            "Number of games in episode 319: 75\n",
            "Avg Action Loss, 11 batches: 0.030229805122044952\n",
            "Avg Block Loss, 7 batches: 0.062315759382077625\n",
            "Avg Challenge Loss, 8 batches: 0.06133986939676106\n",
            "Avg Card Loss, 12 batches: 0.018194649775978178\n",
            "0\n",
            "episode 320 of 1000\n",
            "epsilon: 0.2010878536592394\n",
            "gamma: 0.99\n",
            "Number of games in episode 320: 73\n",
            "Avg Action Loss, 13 batches: 0.02674425708559843\n",
            "Avg Block Loss, 8 batches: 0.051504588103853166\n",
            "Avg Challenge Loss, 8 batches: 0.05888197524473071\n",
            "Avg Card Loss, 10 batches: 0.040143183618783954\n",
            "0\n",
            "win rate: 0.25\n",
            "episode 321 of 1000\n",
            "epsilon: 0.2000824143909432\n",
            "gamma: 0.99\n",
            "Number of games in episode 321: 80\n",
            "Avg Action Loss, 12 batches: 0.032287144723037876\n",
            "Avg Block Loss, 8 batches: 0.08023157413117588\n",
            "Avg Challenge Loss, 9 batches: 0.08986518676910135\n",
            "Avg Card Loss, 12 batches: 0.06221003698495527\n",
            "0\n",
            "episode 322 of 1000\n",
            "epsilon: 0.19908200231898848\n",
            "gamma: 0.99\n",
            "Number of games in episode 322: 77\n",
            "Avg Action Loss, 12 batches: 0.03515528220062455\n",
            "Avg Block Loss, 8 batches: 0.05401188158430159\n",
            "Avg Challenge Loss, 9 batches: 0.06427997661133607\n",
            "Avg Card Loss, 12 batches: 0.034421280802538\n",
            "0\n",
            "episode 323 of 1000\n",
            "epsilon: 0.19808659230739353\n",
            "gamma: 0.99\n",
            "Number of games in episode 323: 76\n",
            "Avg Action Loss, 12 batches: 0.029292254165435832\n",
            "Avg Block Loss, 8 batches: 0.07320206076838076\n",
            "Avg Challenge Loss, 9 batches: 0.07397652831342486\n",
            "Avg Card Loss, 11 batches: 0.05283779100599614\n",
            "0\n",
            "episode 324 of 1000\n",
            "epsilon: 0.19709615934585656\n",
            "gamma: 0.99\n",
            "Number of games in episode 324: 79\n",
            "Avg Action Loss, 13 batches: 0.03340788701405892\n",
            "Avg Block Loss, 8 batches: 0.056530622066929936\n",
            "Avg Challenge Loss, 9 batches: 0.06369783625834519\n",
            "Avg Card Loss, 11 batches: 0.04219282943416725\n",
            "0\n",
            "episode 325 of 1000\n",
            "epsilon: 0.19611067854912728\n",
            "gamma: 0.99\n",
            "Number of games in episode 325: 77\n",
            "Avg Action Loss, 12 batches: 0.02718917067007472\n",
            "Avg Block Loss, 7 batches: 0.05261506446238075\n",
            "Avg Challenge Loss, 8 batches: 0.04963519796729088\n",
            "Avg Card Loss, 11 batches: 0.03557102771645242\n",
            "0\n",
            "episode 326 of 1000\n",
            "epsilon: 0.19513012515638165\n",
            "gamma: 0.99\n",
            "Number of games in episode 326: 73\n",
            "Avg Action Loss, 12 batches: 0.02527031092904508\n",
            "Avg Block Loss, 8 batches: 0.05404933076351881\n",
            "Avg Challenge Loss, 8 batches: 0.05489595839753747\n",
            "Avg Card Loss, 11 batches: 0.05697926582599228\n",
            "0\n",
            "episode 327 of 1000\n",
            "epsilon: 0.19415447453059972\n",
            "gamma: 0.99\n",
            "Number of games in episode 327: 75\n",
            "Avg Action Loss, 12 batches: 0.042656471176693835\n",
            "Avg Block Loss, 9 batches: 0.10152838254968326\n",
            "Avg Challenge Loss, 9 batches: 0.09939036518335342\n",
            "Avg Card Loss, 11 batches: 0.06547398289496248\n",
            "0\n",
            "episode 328 of 1000\n",
            "epsilon: 0.19318370215794672\n",
            "gamma: 0.99\n",
            "Number of games in episode 328: 71\n",
            "Avg Action Loss, 13 batches: 0.03011486980204399\n",
            "Avg Block Loss, 9 batches: 0.07081404783659512\n",
            "Avg Challenge Loss, 9 batches: 0.060306533757183284\n",
            "Avg Card Loss, 10 batches: 0.052900906465947625\n",
            "0\n",
            "episode 329 of 1000\n",
            "epsilon: 0.192217783647157\n",
            "gamma: 0.99\n",
            "Number of games in episode 329: 71\n",
            "Avg Action Loss, 12 batches: 0.02494007667216162\n",
            "Avg Block Loss, 8 batches: 0.08175548678264022\n",
            "Avg Challenge Loss, 9 batches: 0.07768350715438525\n",
            "Avg Card Loss, 10 batches: 0.054540382511913774\n",
            "0\n",
            "episode 330 of 1000\n",
            "epsilon: 0.1912566947289212\n",
            "gamma: 0.99\n",
            "Number of games in episode 330: 69\n",
            "Avg Action Loss, 12 batches: 0.025406927646448214\n",
            "Avg Block Loss, 8 batches: 0.042237774236127734\n",
            "Avg Challenge Loss, 8 batches: 0.047531490912660956\n",
            "Avg Card Loss, 10 batches: 0.042529331520199776\n",
            "0\n",
            "win rate: 0.18\n",
            "episode 331 of 1000\n",
            "epsilon: 0.1903004112552766\n",
            "gamma: 0.99\n",
            "Number of games in episode 331: 76\n",
            "Avg Action Loss, 12 batches: 0.023640451952815056\n",
            "Avg Block Loss, 7 batches: 0.06196938667978559\n",
            "Avg Challenge Loss, 7 batches: 0.07064656274659294\n",
            "Avg Card Loss, 12 batches: 0.039088049825901784\n",
            "0\n",
            "episode 332 of 1000\n",
            "epsilon: 0.18934890919900021\n",
            "gamma: 0.99\n",
            "Number of games in episode 332: 71\n",
            "Avg Action Loss, 12 batches: 0.039969115518033504\n",
            "Avg Block Loss, 7 batches: 0.08970487437077931\n",
            "Avg Challenge Loss, 8 batches: 0.0845087650232017\n",
            "Avg Card Loss, 10 batches: 0.05282433116808534\n",
            "0\n",
            "episode 333 of 1000\n",
            "epsilon: 0.18840216465300522\n",
            "gamma: 0.99\n",
            "Number of games in episode 333: 75\n",
            "Avg Action Loss, 12 batches: 0.0324123272827516\n",
            "Avg Block Loss, 7 batches: 0.057516969208206446\n",
            "Avg Challenge Loss, 8 batches: 0.0670645198551938\n",
            "Avg Card Loss, 12 batches: 0.03462088077018658\n",
            "0\n",
            "episode 334 of 1000\n",
            "epsilon: 0.18746015382974018\n",
            "gamma: 0.99\n",
            "Number of games in episode 334: 75\n",
            "Avg Action Loss, 11 batches: 0.03909338265657425\n",
            "Avg Block Loss, 8 batches: 0.05768773052841425\n",
            "Avg Challenge Loss, 8 batches: 0.06881942600011826\n",
            "Avg Card Loss, 12 batches: 0.035187616963715605\n",
            "0\n",
            "episode 335 of 1000\n",
            "epsilon: 0.1865228530605915\n",
            "gamma: 0.99\n",
            "Number of games in episode 335: 73\n",
            "Avg Action Loss, 11 batches: 0.03039860894734209\n",
            "Avg Block Loss, 8 batches: 0.03746849356684834\n",
            "Avg Challenge Loss, 8 batches: 0.04474005405791104\n",
            "Avg Card Loss, 10 batches: 0.040413754526525736\n",
            "0\n",
            "episode 336 of 1000\n",
            "epsilon: 0.18559023879528855\n",
            "gamma: 0.99\n",
            "Number of games in episode 336: 79\n",
            "Avg Action Loss, 11 batches: 0.043499758805740966\n",
            "Avg Block Loss, 8 batches: 0.053353266790509224\n",
            "Avg Challenge Loss, 9 batches: 0.06849920904884736\n",
            "Avg Card Loss, 13 batches: 0.03794695150393706\n",
            "0\n",
            "episode 337 of 1000\n",
            "epsilon: 0.1846622876013121\n",
            "gamma: 0.99\n",
            "Number of games in episode 337: 73\n",
            "Avg Action Loss, 12 batches: 0.0376530255501469\n",
            "Avg Block Loss, 8 batches: 0.05673375225160271\n",
            "Avg Challenge Loss, 9 batches: 0.0591393636746539\n",
            "Avg Card Loss, 10 batches: 0.04831651980057359\n",
            "0\n",
            "episode 338 of 1000\n",
            "epsilon: 0.18373897616330553\n",
            "gamma: 0.99\n",
            "Number of games in episode 338: 74\n",
            "Avg Action Loss, 12 batches: 0.04192401065180699\n",
            "Avg Block Loss, 8 batches: 0.06949526269454509\n",
            "Avg Challenge Loss, 8 batches: 0.0739798208232969\n",
            "Avg Card Loss, 11 batches: 0.04769960265945305\n",
            "0\n",
            "episode 339 of 1000\n",
            "epsilon: 0.182820281282489\n",
            "gamma: 0.99\n",
            "Number of games in episode 339: 68\n",
            "Avg Action Loss, 12 batches: 0.054515843434880175\n",
            "Avg Block Loss, 8 batches: 0.07115713733946905\n",
            "Avg Challenge Loss, 9 batches: 0.08800916228857306\n",
            "Avg Card Loss, 10 batches: 0.06113409483805299\n",
            "0\n",
            "episode 340 of 1000\n",
            "epsilon: 0.18190617987607657\n",
            "gamma: 0.99\n",
            "Number of games in episode 340: 71\n",
            "Avg Action Loss, 12 batches: 0.0229895180867364\n",
            "Avg Block Loss, 8 batches: 0.037861355231143534\n",
            "Avg Challenge Loss, 9 batches: 0.04130151164200571\n",
            "Avg Card Loss, 10 batches: 0.030929952999576925\n",
            "0\n",
            "win rate: 0.12\n",
            "episode 341 of 1000\n",
            "epsilon: 0.18099664897669618\n",
            "gamma: 0.99\n",
            "Number of games in episode 341: 75\n",
            "Avg Action Loss, 13 batches: 0.02051278938933347\n",
            "Avg Block Loss, 8 batches: 0.06399642932228744\n",
            "Avg Challenge Loss, 9 batches: 0.05530173062450356\n",
            "Avg Card Loss, 11 batches: 0.04876522165299817\n",
            "0\n",
            "episode 342 of 1000\n",
            "epsilon: 0.1800916657318127\n",
            "gamma: 0.99\n",
            "Number of games in episode 342: 78\n",
            "Avg Action Loss, 11 batches: 0.04399209740487012\n",
            "Avg Block Loss, 7 batches: 0.08097363263368607\n",
            "Avg Challenge Loss, 8 batches: 0.08928403072059155\n",
            "Avg Card Loss, 11 batches: 0.035522991622036156\n",
            "0\n",
            "episode 343 of 1000\n",
            "epsilon: 0.17919120740315364\n",
            "gamma: 0.99\n",
            "Number of games in episode 343: 74\n",
            "Avg Action Loss, 13 batches: 0.03162491214103424\n",
            "Avg Block Loss, 8 batches: 0.04863521322840825\n",
            "Avg Challenge Loss, 9 batches: 0.055653943887187377\n",
            "Avg Card Loss, 10 batches: 0.0380402775015682\n",
            "0\n",
            "episode 344 of 1000\n",
            "epsilon: 0.17829525136613786\n",
            "gamma: 0.99\n",
            "Number of games in episode 344: 75\n",
            "Avg Action Loss, 12 batches: 0.037917328028318785\n",
            "Avg Block Loss, 8 batches: 0.0723396890098229\n",
            "Avg Challenge Loss, 9 batches: 0.08363782490293185\n",
            "Avg Card Loss, 11 batches: 0.05086544308472763\n",
            "0\n",
            "episode 345 of 1000\n",
            "epsilon: 0.17740377510930716\n",
            "gamma: 0.99\n",
            "Number of games in episode 345: 69\n",
            "Avg Action Loss, 12 batches: 0.03126677071365217\n",
            "Avg Block Loss, 8 batches: 0.04239829245489091\n",
            "Avg Challenge Loss, 8 batches: 0.04788680269848555\n",
            "Avg Card Loss, 10 batches: 0.044999216310679914\n",
            "0\n",
            "episode 346 of 1000\n",
            "epsilon: 0.17651675623376062\n",
            "gamma: 0.99\n",
            "Number of games in episode 346: 78\n",
            "Avg Action Loss, 12 batches: 0.026418719013842445\n",
            "Avg Block Loss, 7 batches: 0.06719201297632285\n",
            "Avg Challenge Loss, 8 batches: 0.06431180983781815\n",
            "Avg Card Loss, 11 batches: 0.04318253251469948\n",
            "0\n",
            "episode 347 of 1000\n",
            "epsilon: 0.1756341724525918\n",
            "gamma: 0.99\n",
            "Number of games in episode 347: 83\n",
            "Avg Action Loss, 12 batches: 0.045871260187899075\n",
            "Avg Block Loss, 8 batches: 0.05981617164798081\n",
            "Avg Challenge Loss, 9 batches: 0.08096400193042225\n",
            "Avg Card Loss, 12 batches: 0.060223478746290006\n",
            "0\n",
            "episode 348 of 1000\n",
            "epsilon: 0.17475600159032884\n",
            "gamma: 0.99\n",
            "Number of games in episode 348: 80\n",
            "Avg Action Loss, 12 batches: 0.03195769704567889\n",
            "Avg Block Loss, 8 batches: 0.059146714862436056\n",
            "Avg Challenge Loss, 9 batches: 0.05587885363234414\n",
            "Avg Card Loss, 11 batches: 0.05530653572217985\n",
            "0\n",
            "episode 349 of 1000\n",
            "epsilon: 0.17388222158237718\n",
            "gamma: 0.99\n",
            "Number of games in episode 349: 79\n",
            "Avg Action Loss, 12 batches: 0.03196933182577292\n",
            "Avg Block Loss, 8 batches: 0.06611398560926318\n",
            "Avg Challenge Loss, 8 batches: 0.05807537346845493\n",
            "Avg Card Loss, 12 batches: 0.037057320664947234\n",
            "0\n",
            "episode 350 of 1000\n",
            "epsilon: 0.1730128104744653\n",
            "gamma: 0.99\n",
            "Number of games in episode 350: 76\n",
            "Avg Action Loss, 12 batches: 0.03642888782390704\n",
            "Avg Block Loss, 7 batches: 0.05477922622646604\n",
            "Avg Challenge Loss, 8 batches: 0.050992373493500054\n",
            "Avg Card Loss, 11 batches: 0.027202953245829452\n",
            "0\n",
            "win rate: 0.23\n",
            "episode 351 of 1000\n",
            "epsilon: 0.17214774642209296\n",
            "gamma: 0.99\n",
            "Number of games in episode 351: 73\n",
            "Avg Action Loss, 12 batches: 0.034170858561992645\n",
            "Avg Block Loss, 9 batches: 0.05920096548895041\n",
            "Avg Challenge Loss, 9 batches: 0.0675403620633814\n",
            "Avg Card Loss, 11 batches: 0.05149773440577767\n",
            "0\n",
            "episode 352 of 1000\n",
            "epsilon: 0.1712870076899825\n",
            "gamma: 0.99\n",
            "Number of games in episode 352: 74\n",
            "Avg Action Loss, 13 batches: 0.028503217949317053\n",
            "Avg Block Loss, 8 batches: 0.07269303663633764\n",
            "Avg Challenge Loss, 9 batches: 0.0754753936909967\n",
            "Avg Card Loss, 11 batches: 0.03822369687259197\n",
            "0\n",
            "episode 353 of 1000\n",
            "epsilon: 0.17043057265153258\n",
            "gamma: 0.99\n",
            "Number of games in episode 353: 75\n",
            "Avg Action Loss, 11 batches: 0.033684396438977936\n",
            "Avg Block Loss, 8 batches: 0.05504875665064901\n",
            "Avg Challenge Loss, 8 batches: 0.06211936380714178\n",
            "Avg Card Loss, 12 batches: 0.04664516899113854\n",
            "0\n",
            "episode 354 of 1000\n",
            "epsilon: 0.16957841978827493\n",
            "gamma: 0.99\n",
            "Number of games in episode 354: 72\n",
            "Avg Action Loss, 12 batches: 0.02828979433979839\n",
            "Avg Block Loss, 7 batches: 0.07774461745949728\n",
            "Avg Challenge Loss, 8 batches: 0.07446810393594205\n",
            "Avg Card Loss, 11 batches: 0.03662759692154147\n",
            "0\n",
            "episode 355 of 1000\n",
            "epsilon: 0.16873052768933355\n",
            "gamma: 0.99\n",
            "Number of games in episode 355: 71\n",
            "Avg Action Loss, 11 batches: 0.027346706356514584\n",
            "Avg Block Loss, 7 batches: 0.04957106203905174\n",
            "Avg Challenge Loss, 8 batches: 0.061687662499025464\n",
            "Avg Card Loss, 11 batches: 0.0326128643090752\n",
            "0\n",
            "episode 356 of 1000\n",
            "epsilon: 0.1678868750508869\n",
            "gamma: 0.99\n",
            "Number of games in episode 356: 77\n",
            "Avg Action Loss, 12 batches: 0.040074975385020174\n",
            "Avg Block Loss, 8 batches: 0.04357093735598028\n",
            "Avg Challenge Loss, 9 batches: 0.04576010277701749\n",
            "Avg Card Loss, 12 batches: 0.049356058317547045\n",
            "0\n",
            "episode 357 of 1000\n",
            "epsilon: 0.16704744067563246\n",
            "gamma: 0.99\n",
            "Number of games in episode 357: 70\n",
            "Avg Action Loss, 12 batches: 0.03619772723565499\n",
            "Avg Block Loss, 8 batches: 0.07599367736838758\n",
            "Avg Challenge Loss, 8 batches: 0.075110669946298\n",
            "Avg Card Loss, 11 batches: 0.05184006441215223\n",
            "0\n",
            "episode 358 of 1000\n",
            "epsilon: 0.1662122034722543\n",
            "gamma: 0.99\n",
            "Number of games in episode 358: 74\n",
            "Avg Action Loss, 11 batches: 0.029561552219092846\n",
            "Avg Block Loss, 7 batches: 0.03956499083765915\n",
            "Avg Challenge Loss, 8 batches: 0.040414017043076456\n",
            "Avg Card Loss, 11 batches: 0.024397136389531872\n",
            "0\n",
            "episode 359 of 1000\n",
            "epsilon: 0.16538114245489302\n",
            "gamma: 0.99\n",
            "Number of games in episode 359: 67\n",
            "Avg Action Loss, 12 batches: 0.0450685815885663\n",
            "Avg Block Loss, 8 batches: 0.051259225932881236\n",
            "Avg Challenge Loss, 8 batches: 0.05932693276554346\n",
            "Avg Card Loss, 10 batches: 0.05201129917986691\n",
            "0\n",
            "episode 360 of 1000\n",
            "epsilon: 0.16455423674261854\n",
            "gamma: 0.99\n",
            "Number of games in episode 360: 77\n",
            "Avg Action Loss, 13 batches: 0.030079803644464567\n",
            "Avg Block Loss, 8 batches: 0.06024650298058987\n",
            "Avg Challenge Loss, 9 batches: 0.059592876583337784\n",
            "Avg Card Loss, 11 batches: 0.048460673964159054\n",
            "0\n",
            "win rate: 0.22\n",
            "episode 361 of 1000\n",
            "epsilon: 0.16373146555890544\n",
            "gamma: 0.99\n",
            "Number of games in episode 361: 73\n",
            "Avg Action Loss, 12 batches: 0.039657669297109045\n",
            "Avg Block Loss, 8 batches: 0.05676269019022584\n",
            "Avg Challenge Loss, 8 batches: 0.057727300096303225\n",
            "Avg Card Loss, 11 batches: 0.06288562156260014\n",
            "0\n",
            "episode 362 of 1000\n",
            "epsilon: 0.16291280823111093\n",
            "gamma: 0.99\n",
            "Number of games in episode 362: 77\n",
            "Avg Action Loss, 13 batches: 0.049613953496401124\n",
            "Avg Block Loss, 8 batches: 0.0581939029507339\n",
            "Avg Challenge Loss, 9 batches: 0.06539201156960593\n",
            "Avg Card Loss, 11 batches: 0.046876945956186813\n",
            "0\n",
            "episode 363 of 1000\n",
            "epsilon: 0.16209824418995536\n",
            "gamma: 0.99\n",
            "Number of games in episode 363: 76\n",
            "Avg Action Loss, 13 batches: 0.039508452638983727\n",
            "Avg Block Loss, 8 batches: 0.05235976597759873\n",
            "Avg Challenge Loss, 9 batches: 0.055433992503417864\n",
            "Avg Card Loss, 11 batches: 0.04418931308795105\n",
            "0\n",
            "episode 364 of 1000\n",
            "epsilon: 0.16128775296900558\n",
            "gamma: 0.99\n",
            "Number of games in episode 364: 71\n",
            "Avg Action Loss, 11 batches: 0.029011822601949625\n",
            "Avg Block Loss, 8 batches: 0.05677158711478114\n",
            "Avg Challenge Loss, 8 batches: 0.05981941637583077\n",
            "Avg Card Loss, 11 batches: 0.04340092211284421\n",
            "0\n",
            "episode 365 of 1000\n",
            "epsilon: 0.16048131420416054\n",
            "gamma: 0.99\n",
            "Number of games in episode 365: 71\n",
            "Avg Action Loss, 12 batches: 0.03574003248165051\n",
            "Avg Block Loss, 8 batches: 0.06552925962023437\n",
            "Avg Challenge Loss, 8 batches: 0.06431981432251632\n",
            "Avg Card Loss, 11 batches: 0.043693154880946335\n",
            "0\n",
            "episode 366 of 1000\n",
            "epsilon: 0.15967890763313974\n",
            "gamma: 0.99\n",
            "Number of games in episode 366: 76\n",
            "Avg Action Loss, 12 batches: 0.03263572650030255\n",
            "Avg Block Loss, 8 batches: 0.06962479138746858\n",
            "Avg Challenge Loss, 9 batches: 0.06101931222817964\n",
            "Avg Card Loss, 11 batches: 0.05630533058534969\n",
            "0\n",
            "episode 367 of 1000\n",
            "epsilon: 0.15888051309497406\n",
            "gamma: 0.99\n",
            "Number of games in episode 367: 75\n",
            "Avg Action Loss, 12 batches: 0.023601187160238624\n",
            "Avg Block Loss, 7 batches: 0.04885097752724375\n",
            "Avg Challenge Loss, 8 batches: 0.0491336191771552\n",
            "Avg Card Loss, 12 batches: 0.04824647353962064\n",
            "0\n",
            "episode 368 of 1000\n",
            "epsilon: 0.1580861105294992\n",
            "gamma: 0.99\n",
            "Number of games in episode 368: 73\n",
            "Avg Action Loss, 12 batches: 0.03133000526577234\n",
            "Avg Block Loss, 9 batches: 0.055288294019798435\n",
            "Avg Challenge Loss, 9 batches: 0.05799882714119223\n",
            "Avg Card Loss, 11 batches: 0.036254778927700085\n",
            "0\n",
            "episode 369 of 1000\n",
            "epsilon: 0.1572956799768517\n",
            "gamma: 0.99\n",
            "Number of games in episode 369: 77\n",
            "Avg Action Loss, 12 batches: 0.024820070946589112\n",
            "Avg Block Loss, 8 batches: 0.06549997010733932\n",
            "Avg Challenge Loss, 8 batches: 0.07652140548452735\n",
            "Avg Card Loss, 11 batches: 0.05409133197231726\n",
            "0\n",
            "episode 370 of 1000\n",
            "epsilon: 0.15650920157696743\n",
            "gamma: 0.99\n",
            "Number of games in episode 370: 69\n",
            "Avg Action Loss, 11 batches: 0.032787605273452675\n",
            "Avg Block Loss, 7 batches: 0.0477005663727011\n",
            "Avg Challenge Loss, 7 batches: 0.060991448749388964\n",
            "Avg Card Loss, 11 batches: 0.03211674324914136\n",
            "0\n",
            "win rate: 0.24\n",
            "episode 371 of 1000\n",
            "epsilon: 0.1557266555690826\n",
            "gamma: 0.99\n",
            "Number of games in episode 371: 73\n",
            "Avg Action Loss, 12 batches: 0.030094967456534505\n",
            "Avg Block Loss, 8 batches: 0.06167848769109696\n",
            "Avg Challenge Loss, 8 batches: 0.05752380995545536\n",
            "Avg Card Loss, 11 batches: 0.041266419078138744\n",
            "0\n",
            "episode 372 of 1000\n",
            "epsilon: 0.1549480222912372\n",
            "gamma: 0.99\n",
            "Number of games in episode 372: 76\n",
            "Avg Action Loss, 12 batches: 0.03110590282206734\n",
            "Avg Block Loss, 8 batches: 0.0727261221036315\n",
            "Avg Challenge Loss, 8 batches: 0.0719135720282793\n",
            "Avg Card Loss, 11 batches: 0.041270416314628994\n",
            "0\n",
            "episode 373 of 1000\n",
            "epsilon: 0.15417328217978102\n",
            "gamma: 0.99\n",
            "Number of games in episode 373: 70\n",
            "Avg Action Loss, 13 batches: 0.031039250584749076\n",
            "Avg Block Loss, 8 batches: 0.06712006311863661\n",
            "Avg Challenge Loss, 8 batches: 0.0715710159856826\n",
            "Avg Card Loss, 11 batches: 0.03382387367839163\n",
            "0\n",
            "episode 374 of 1000\n",
            "epsilon: 0.1534024157688821\n",
            "gamma: 0.99\n",
            "Number of games in episode 374: 69\n",
            "Avg Action Loss, 12 batches: 0.037474823805193104\n",
            "Avg Block Loss, 7 batches: 0.06719038050089564\n",
            "Avg Challenge Loss, 8 batches: 0.07436282560229301\n",
            "Avg Card Loss, 10 batches: 0.03310964675620198\n",
            "0\n",
            "episode 375 of 1000\n",
            "epsilon: 0.1526354036900377\n",
            "gamma: 0.99\n",
            "Number of games in episode 375: 77\n",
            "Avg Action Loss, 12 batches: 0.029445280088111758\n",
            "Avg Block Loss, 7 batches: 0.07445935319576945\n",
            "Avg Challenge Loss, 7 batches: 0.07308071904948779\n",
            "Avg Card Loss, 10 batches: 0.04984817737713456\n",
            "0\n",
            "episode 376 of 1000\n",
            "epsilon: 0.1518722266715875\n",
            "gamma: 0.99\n",
            "Number of games in episode 376: 78\n",
            "Avg Action Loss, 11 batches: 0.03531432490457188\n",
            "Avg Block Loss, 8 batches: 0.09308634256012738\n",
            "Avg Challenge Loss, 8 batches: 0.10832659294828773\n",
            "Avg Card Loss, 13 batches: 0.04540552564251881\n",
            "0\n",
            "episode 377 of 1000\n",
            "epsilon: 0.15111286553822956\n",
            "gamma: 0.99\n",
            "Number of games in episode 377: 69\n",
            "Avg Action Loss, 12 batches: 0.03778031972857813\n",
            "Avg Block Loss, 7 batches: 0.06777774116822652\n",
            "Avg Challenge Loss, 8 batches: 0.07445106562227011\n",
            "Avg Card Loss, 10 batches: 0.05405199844390154\n",
            "0\n",
            "episode 378 of 1000\n",
            "epsilon: 0.15035730121053842\n",
            "gamma: 0.99\n",
            "Number of games in episode 378: 75\n",
            "Avg Action Loss, 12 batches: 0.03618423305063819\n",
            "Avg Block Loss, 7 batches: 0.035346957721880505\n",
            "Avg Challenge Loss, 8 batches: 0.05624658730812371\n",
            "Avg Card Loss, 11 batches: 0.04783718296411363\n",
            "0\n",
            "episode 379 of 1000\n",
            "epsilon: 0.14960551470448571\n",
            "gamma: 0.99\n",
            "Number of games in episode 379: 74\n",
            "Avg Action Loss, 12 batches: 0.02530632067161302\n",
            "Avg Block Loss, 8 batches: 0.048064744216389954\n",
            "Avg Challenge Loss, 8 batches: 0.043023040518164635\n",
            "Avg Card Loss, 12 batches: 0.04093421366997063\n",
            "0\n",
            "episode 380 of 1000\n",
            "epsilon: 0.14885748713096328\n",
            "gamma: 0.99\n",
            "Number of games in episode 380: 78\n",
            "Avg Action Loss, 12 batches: 0.029756887660672266\n",
            "Avg Block Loss, 8 batches: 0.05393183301202953\n",
            "Avg Challenge Loss, 9 batches: 0.06320794112980366\n",
            "Avg Card Loss, 11 batches: 0.04318930869075385\n",
            "0\n",
            "win rate: 0.19\n",
            "episode 381 of 1000\n",
            "epsilon: 0.14811319969530845\n",
            "gamma: 0.99\n",
            "Number of games in episode 381: 75\n",
            "Avg Action Loss, 11 batches: 0.031456067311492836\n",
            "Avg Block Loss, 7 batches: 0.06280659139156342\n",
            "Avg Challenge Loss, 7 batches: 0.07052527421287128\n",
            "Avg Card Loss, 11 batches: 0.03649792756715959\n",
            "0\n",
            "episode 382 of 1000\n",
            "epsilon: 0.1473726336968319\n",
            "gamma: 0.99\n",
            "Number of games in episode 382: 73\n",
            "Avg Action Loss, 12 batches: 0.02673985475363831\n",
            "Avg Block Loss, 8 batches: 0.03699116938514635\n",
            "Avg Challenge Loss, 9 batches: 0.038032581822739706\n",
            "Avg Card Loss, 11 batches: 0.047233510233292524\n",
            "0\n",
            "episode 383 of 1000\n",
            "epsilon: 0.14663577052834775\n",
            "gamma: 0.99\n",
            "Number of games in episode 383: 76\n",
            "Avg Action Loss, 11 batches: 0.030826204922050238\n",
            "Avg Block Loss, 8 batches: 0.06061115034390241\n",
            "Avg Challenge Loss, 8 batches: 0.06548719154670835\n",
            "Avg Card Loss, 12 batches: 0.0541309320833534\n",
            "0\n",
            "episode 384 of 1000\n",
            "epsilon: 0.14590259167570602\n",
            "gamma: 0.99\n",
            "Number of games in episode 384: 72\n",
            "Avg Action Loss, 12 batches: 0.018383282275560003\n",
            "Avg Block Loss, 7 batches: 0.07309300612126078\n",
            "Avg Challenge Loss, 8 batches: 0.06611789960879833\n",
            "Avg Card Loss, 11 batches: 0.05079597480256449\n",
            "0\n",
            "episode 385 of 1000\n",
            "epsilon: 0.1451730787173275\n",
            "gamma: 0.99\n",
            "Number of games in episode 385: 76\n",
            "Avg Action Loss, 11 batches: 0.02813259300521829\n",
            "Avg Block Loss, 7 batches: 0.052648023036973815\n",
            "Avg Challenge Loss, 8 batches: 0.05737484048586339\n",
            "Avg Card Loss, 12 batches: 0.04619686414177219\n",
            "0\n",
            "episode 386 of 1000\n",
            "epsilon: 0.14444721332374086\n",
            "gamma: 0.99\n",
            "Number of games in episode 386: 68\n",
            "Avg Action Loss, 11 batches: 0.029993812722915954\n",
            "Avg Block Loss, 8 batches: 0.06315341149456799\n",
            "Avg Challenge Loss, 8 batches: 0.06540966802276671\n",
            "Avg Card Loss, 10 batches: 0.039566741324961185\n",
            "0\n",
            "episode 387 of 1000\n",
            "epsilon: 0.14372497725712216\n",
            "gamma: 0.99\n",
            "Number of games in episode 387: 74\n",
            "Avg Action Loss, 12 batches: 0.034031990837926664\n",
            "Avg Block Loss, 7 batches: 0.03825240582227707\n",
            "Avg Challenge Loss, 8 batches: 0.04300605005118996\n",
            "Avg Card Loss, 11 batches: 0.039261387322436676\n",
            "0\n",
            "episode 388 of 1000\n",
            "epsilon: 0.14300635237083656\n",
            "gamma: 0.99\n",
            "Number of games in episode 388: 70\n",
            "Avg Action Loss, 12 batches: 0.026705768366809934\n",
            "Avg Block Loss, 7 batches: 0.056589061660426\n",
            "Avg Challenge Loss, 7 batches: 0.052807893337947984\n",
            "Avg Card Loss, 10 batches: 0.03866682853549719\n",
            "0\n",
            "episode 389 of 1000\n",
            "epsilon: 0.14229132060898236\n",
            "gamma: 0.99\n",
            "Number of games in episode 389: 79\n",
            "Avg Action Loss, 13 batches: 0.043562483615600146\n",
            "Avg Block Loss, 9 batches: 0.05974489967856142\n",
            "Avg Challenge Loss, 10 batches: 0.060574991954490544\n",
            "Avg Card Loss, 11 batches: 0.04402819817716425\n",
            "0\n",
            "episode 390 of 1000\n",
            "epsilon: 0.14157986400593744\n",
            "gamma: 0.99\n",
            "Number of games in episode 390: 75\n",
            "Avg Action Loss, 13 batches: 0.042777313062777884\n",
            "Avg Block Loss, 8 batches: 0.08748436102177948\n",
            "Avg Challenge Loss, 8 batches: 0.07801157934591174\n",
            "Avg Card Loss, 11 batches: 0.051469364542175426\n",
            "0\n",
            "win rate: 0.29\n",
            "episode 391 of 1000\n",
            "epsilon: 0.14087196468590776\n",
            "gamma: 0.99\n",
            "Number of games in episode 391: 71\n",
            "Avg Action Loss, 12 batches: 0.032339595103015505\n",
            "Avg Block Loss, 8 batches: 0.04511046630796045\n",
            "Avg Challenge Loss, 8 batches: 0.04600020090583712\n",
            "Avg Card Loss, 11 batches: 0.03406804529103366\n",
            "0\n",
            "episode 392 of 1000\n",
            "epsilon: 0.14016760486247823\n",
            "gamma: 0.99\n",
            "Number of games in episode 392: 75\n",
            "Avg Action Loss, 11 batches: 0.04733787536282431\n",
            "Avg Block Loss, 7 batches: 0.07142279323722635\n",
            "Avg Challenge Loss, 8 batches: 0.0685200144071132\n",
            "Avg Card Loss, 11 batches: 0.04054564243944531\n",
            "0\n",
            "episode 393 of 1000\n",
            "epsilon: 0.13946676683816583\n",
            "gamma: 0.99\n",
            "Number of games in episode 393: 75\n",
            "Avg Action Loss, 13 batches: 0.037415417878387064\n",
            "Avg Block Loss, 9 batches: 0.07400833318630855\n",
            "Avg Challenge Loss, 9 batches: 0.07455502077937126\n",
            "Avg Card Loss, 10 batches: 0.05759121421724558\n",
            "0\n",
            "episode 394 of 1000\n",
            "epsilon: 0.138769433003975\n",
            "gamma: 0.99\n",
            "Number of games in episode 394: 75\n",
            "Avg Action Loss, 11 batches: 0.040832525796510956\n",
            "Avg Block Loss, 7 batches: 0.08623540241803442\n",
            "Avg Challenge Loss, 8 batches: 0.09153260476887226\n",
            "Avg Card Loss, 12 batches: 0.03862898140990486\n",
            "0\n",
            "episode 395 of 1000\n",
            "epsilon: 0.13807558583895513\n",
            "gamma: 0.99\n",
            "Number of games in episode 395: 75\n",
            "Avg Action Loss, 12 batches: 0.028024866168076795\n",
            "Avg Block Loss, 8 batches: 0.061767582898028195\n",
            "Avg Challenge Loss, 9 batches: 0.06630802547766103\n",
            "Avg Card Loss, 11 batches: 0.050137568773193794\n",
            "0\n",
            "episode 396 of 1000\n",
            "epsilon: 0.13738520790976036\n",
            "gamma: 0.99\n",
            "Number of games in episode 396: 74\n",
            "Avg Action Loss, 11 batches: 0.027821535667912525\n",
            "Avg Block Loss, 8 batches: 0.07001268526073545\n",
            "Avg Challenge Loss, 8 batches: 0.06814152863807976\n",
            "Avg Card Loss, 11 batches: 0.04863482193005356\n",
            "0\n",
            "episode 397 of 1000\n",
            "epsilon: 0.13669828187021155\n",
            "gamma: 0.99\n",
            "Number of games in episode 397: 80\n",
            "Avg Action Loss, 11 batches: 0.03527952849187634\n",
            "Avg Block Loss, 8 batches: 0.07300541747827083\n",
            "Avg Challenge Loss, 8 batches: 0.07252341881394386\n",
            "Avg Card Loss, 12 batches: 0.050999749141434826\n",
            "0\n",
            "episode 398 of 1000\n",
            "epsilon: 0.13601479046086049\n",
            "gamma: 0.99\n",
            "Number of games in episode 398: 73\n",
            "Avg Action Loss, 11 batches: 0.033921926875006066\n",
            "Avg Block Loss, 8 batches: 0.08224271703511477\n",
            "Avg Challenge Loss, 8 batches: 0.06807417958043516\n",
            "Avg Card Loss, 11 batches: 0.055243600752543316\n",
            "0\n",
            "episode 399 of 1000\n",
            "epsilon: 0.1353347165085562\n",
            "gamma: 0.99\n",
            "Number of games in episode 399: 79\n",
            "Avg Action Loss, 12 batches: 0.0175857530751576\n",
            "Avg Block Loss, 8 batches: 0.06289471220225096\n",
            "Avg Challenge Loss, 8 batches: 0.04999076371314004\n",
            "Avg Card Loss, 11 batches: 0.04732487206770615\n",
            "0\n",
            "episode 400 of 1000\n",
            "epsilon: 0.1346580429260134\n",
            "gamma: 0.99\n",
            "Number of games in episode 400: 71\n",
            "Avg Action Loss, 11 batches: 0.023855367421426556\n",
            "Avg Block Loss, 7 batches: 0.07498585299721786\n",
            "Avg Challenge Loss, 8 batches: 0.07004915876314044\n",
            "Avg Card Loss, 12 batches: 0.03408722664850453\n",
            "0\n",
            "win rate: 0.2\n",
            "episode 401 of 1000\n",
            "epsilon: 0.13398475271138335\n",
            "gamma: 0.99\n",
            "Number of games in episode 401: 81\n",
            "Avg Action Loss, 12 batches: 0.032479879911988974\n",
            "Avg Block Loss, 8 batches: 0.08023128216154873\n",
            "Avg Challenge Loss, 8 batches: 0.07961623533628881\n",
            "Avg Card Loss, 13 batches: 0.0465449823353153\n",
            "0\n",
            "episode 402 of 1000\n",
            "epsilon: 0.13331482894782642\n",
            "gamma: 0.99\n",
            "Number of games in episode 402: 78\n",
            "Avg Action Loss, 11 batches: 0.034685628657991234\n",
            "Avg Block Loss, 7 batches: 0.07674728679869856\n",
            "Avg Challenge Loss, 8 batches: 0.07012440077960491\n",
            "Avg Card Loss, 11 batches: 0.033827268560840326\n",
            "0\n",
            "episode 403 of 1000\n",
            "epsilon: 0.13264825480308728\n",
            "gamma: 0.99\n",
            "Number of games in episode 403: 75\n",
            "Avg Action Loss, 12 batches: 0.04887616525714596\n",
            "Avg Block Loss, 8 batches: 0.0697559155523777\n",
            "Avg Challenge Loss, 8 batches: 0.08410509559325874\n",
            "Avg Card Loss, 11 batches: 0.05358207064934752\n",
            "0\n",
            "episode 404 of 1000\n",
            "epsilon: 0.13198501352907185\n",
            "gamma: 0.99\n",
            "Number of games in episode 404: 80\n",
            "Avg Action Loss, 11 batches: 0.031017499721863052\n",
            "Avg Block Loss, 7 batches: 0.07729399683220047\n",
            "Avg Challenge Loss, 8 batches: 0.07313107431400567\n",
            "Avg Card Loss, 13 batches: 0.05167596364537111\n",
            "0\n",
            "episode 405 of 1000\n",
            "epsilon: 0.1313250884614265\n",
            "gamma: 0.99\n",
            "Number of games in episode 405: 73\n",
            "Avg Action Loss, 12 batches: 0.037703733115146555\n",
            "Avg Block Loss, 9 batches: 0.05890628550615576\n",
            "Avg Challenge Loss, 9 batches: 0.07103983147276773\n",
            "Avg Card Loss, 10 batches: 0.04222635477781296\n",
            "0\n",
            "episode 406 of 1000\n",
            "epsilon: 0.13066846301911936\n",
            "gamma: 0.99\n",
            "Number of games in episode 406: 74\n",
            "Avg Action Loss, 12 batches: 0.02677832458478709\n",
            "Avg Block Loss, 7 batches: 0.06910479753943426\n",
            "Avg Challenge Loss, 8 batches: 0.06963092484511435\n",
            "Avg Card Loss, 12 batches: 0.04418014110221217\n",
            "0\n",
            "episode 407 of 1000\n",
            "epsilon: 0.13001512070402377\n",
            "gamma: 0.99\n",
            "Number of games in episode 407: 75\n",
            "Avg Action Loss, 11 batches: 0.03544911031018604\n",
            "Avg Block Loss, 7 batches: 0.08680332505277225\n",
            "Avg Challenge Loss, 8 batches: 0.079795700032264\n",
            "Avg Card Loss, 10 batches: 0.034446855541318655\n",
            "0\n",
            "episode 408 of 1000\n",
            "epsilon: 0.12936504510050365\n",
            "gamma: 0.99\n",
            "Number of games in episode 408: 77\n",
            "Avg Action Loss, 12 batches: 0.04108993507300814\n",
            "Avg Block Loss, 7 batches: 0.05966314487159252\n",
            "Avg Challenge Loss, 8 batches: 0.07199593225959688\n",
            "Avg Card Loss, 12 batches: 0.048643541522324085\n",
            "0\n",
            "episode 409 of 1000\n",
            "epsilon: 0.12871821987500112\n",
            "gamma: 0.99\n",
            "Number of games in episode 409: 73\n",
            "Avg Action Loss, 12 batches: 0.029606455238536\n",
            "Avg Block Loss, 8 batches: 0.053513916907832026\n",
            "Avg Challenge Loss, 8 batches: 0.05040340358391404\n",
            "Avg Card Loss, 11 batches: 0.05176403864540837\n",
            "0\n",
            "episode 410 of 1000\n",
            "epsilon: 0.12807462877562611\n",
            "gamma: 0.99\n",
            "Number of games in episode 410: 71\n",
            "Avg Action Loss, 12 batches: 0.03693753737024963\n",
            "Avg Block Loss, 8 batches: 0.06617972231470048\n",
            "Avg Challenge Loss, 8 batches: 0.06279995804652572\n",
            "Avg Card Loss, 10 batches: 0.03659620564430952\n",
            "0\n",
            "win rate: 0.23\n",
            "episode 411 of 1000\n",
            "epsilon: 0.12743425563174798\n",
            "gamma: 0.99\n",
            "Number of games in episode 411: 78\n",
            "Avg Action Loss, 12 batches: 0.03272155160084367\n",
            "Avg Block Loss, 8 batches: 0.05599016835913062\n",
            "Avg Challenge Loss, 9 batches: 0.05875845129291216\n",
            "Avg Card Loss, 11 batches: 0.058099657466465775\n",
            "0\n",
            "episode 412 of 1000\n",
            "epsilon: 0.12679708435358925\n",
            "gamma: 0.99\n",
            "Number of games in episode 412: 73\n",
            "Avg Action Loss, 12 batches: 0.0238651666441001\n",
            "Avg Block Loss, 7 batches: 0.07481347117573023\n",
            "Avg Challenge Loss, 8 batches: 0.07638808060437441\n",
            "Avg Card Loss, 11 batches: 0.04062761343084276\n",
            "0\n",
            "episode 413 of 1000\n",
            "epsilon: 0.1261630989318213\n",
            "gamma: 0.99\n",
            "Number of games in episode 413: 75\n",
            "Avg Action Loss, 12 batches: 0.030772112930814426\n",
            "Avg Block Loss, 7 batches: 0.07020603120326996\n",
            "Avg Challenge Loss, 8 batches: 0.07129703357350081\n",
            "Avg Card Loss, 11 batches: 0.03751561189578338\n",
            "0\n",
            "episode 414 of 1000\n",
            "epsilon: 0.1255322834371622\n",
            "gamma: 0.99\n",
            "Number of games in episode 414: 70\n",
            "Avg Action Loss, 11 batches: 0.029394667002965103\n",
            "Avg Block Loss, 8 batches: 0.06780957872979343\n",
            "Avg Challenge Loss, 9 batches: 0.0723153328936961\n",
            "Avg Card Loss, 10 batches: 0.03510585417971015\n",
            "0\n",
            "episode 415 of 1000\n",
            "epsilon: 0.12490462201997637\n",
            "gamma: 0.99\n",
            "Number of games in episode 415: 73\n",
            "Avg Action Loss, 12 batches: 0.0301610273309052\n",
            "Avg Block Loss, 8 batches: 0.09918268723413348\n",
            "Avg Challenge Loss, 8 batches: 0.0972434722352773\n",
            "Avg Card Loss, 11 batches: 0.047968165102330124\n",
            "0\n",
            "episode 416 of 1000\n",
            "epsilon: 0.1242800989098765\n",
            "gamma: 0.99\n",
            "Number of games in episode 416: 80\n",
            "Avg Action Loss, 13 batches: 0.03581161517649889\n",
            "Avg Block Loss, 9 batches: 0.051119860209938556\n",
            "Avg Challenge Loss, 9 batches: 0.05805642261273331\n",
            "Avg Card Loss, 12 batches: 0.04709888187547525\n",
            "0\n",
            "episode 417 of 1000\n",
            "epsilon: 0.12365869841532712\n",
            "gamma: 0.99\n",
            "Number of games in episode 417: 74\n",
            "Avg Action Loss, 12 batches: 0.04045255552046001\n",
            "Avg Block Loss, 8 batches: 0.06271709967404604\n",
            "Avg Challenge Loss, 8 batches: 0.06417704746127129\n",
            "Avg Card Loss, 12 batches: 0.03585614279533426\n",
            "0\n",
            "episode 418 of 1000\n",
            "epsilon: 0.12304040492325048\n",
            "gamma: 0.99\n",
            "Number of games in episode 418: 72\n",
            "Avg Action Loss, 12 batches: 0.03465667739510536\n",
            "Avg Block Loss, 8 batches: 0.06745755043812096\n",
            "Avg Challenge Loss, 8 batches: 0.056877021910622716\n",
            "Avg Card Loss, 11 batches: 0.048590233803472736\n",
            "0\n",
            "episode 419 of 1000\n",
            "epsilon: 0.12242520289863423\n",
            "gamma: 0.99\n",
            "Number of games in episode 419: 72\n",
            "Avg Action Loss, 12 batches: 0.021562205084289115\n",
            "Avg Block Loss, 7 batches: 0.054382033579583676\n",
            "Avg Challenge Loss, 7 batches: 0.0589571406266519\n",
            "Avg Card Loss, 11 batches: 0.04024797974323684\n",
            "0\n",
            "episode 420 of 1000\n",
            "epsilon: 0.12181307688414106\n",
            "gamma: 0.99\n",
            "Number of games in episode 420: 75\n",
            "Avg Action Loss, 11 batches: 0.026969419622963123\n",
            "Avg Block Loss, 9 batches: 0.05371400212041206\n",
            "Avg Challenge Loss, 10 batches: 0.05587185304611921\n",
            "Avg Card Loss, 12 batches: 0.05065305080885688\n",
            "0\n",
            "win rate: 0.23\n",
            "episode 421 of 1000\n",
            "epsilon: 0.12120401149972035\n",
            "gamma: 0.99\n",
            "Number of games in episode 421: 74\n",
            "Avg Action Loss, 12 batches: 0.03218150231987238\n",
            "Avg Block Loss, 7 batches: 0.02424091739313943\n",
            "Avg Challenge Loss, 8 batches: 0.026046667713671923\n",
            "Avg Card Loss, 12 batches: 0.02365317246100555\n",
            "0\n",
            "episode 422 of 1000\n",
            "epsilon: 0.12059799144222175\n",
            "gamma: 0.99\n",
            "Number of games in episode 422: 70\n",
            "Avg Action Loss, 11 batches: 0.03762285885485736\n",
            "Avg Block Loss, 7 batches: 0.08049972954073123\n",
            "Avg Challenge Loss, 8 batches: 0.08924794476479292\n",
            "Avg Card Loss, 10 batches: 0.04125669682398438\n",
            "0\n",
            "episode 423 of 1000\n",
            "epsilon: 0.11999500148501063\n",
            "gamma: 0.99\n",
            "Number of games in episode 423: 73\n",
            "Avg Action Loss, 11 batches: 0.030039385638453743\n",
            "Avg Block Loss, 7 batches: 0.07201721998197692\n",
            "Avg Challenge Loss, 8 batches: 0.07059196848422289\n",
            "Avg Card Loss, 11 batches: 0.036403756313533944\n",
            "0\n",
            "episode 424 of 1000\n",
            "epsilon: 0.11939502647758558\n",
            "gamma: 0.99\n",
            "Number of games in episode 424: 70\n",
            "Avg Action Loss, 11 batches: 0.03276046018370173\n",
            "Avg Block Loss, 8 batches: 0.04766780626960099\n",
            "Avg Challenge Loss, 8 batches: 0.0497743571177125\n",
            "Avg Card Loss, 10 batches: 0.03003616016358137\n",
            "0\n",
            "episode 425 of 1000\n",
            "epsilon: 0.11879805134519765\n",
            "gamma: 0.99\n",
            "Number of games in episode 425: 71\n",
            "Avg Action Loss, 11 batches: 0.04222034950825301\n",
            "Avg Block Loss, 7 batches: 0.0302143892539399\n",
            "Avg Challenge Loss, 8 batches: 0.04845755361020565\n",
            "Avg Card Loss, 11 batches: 0.028692185794087974\n",
            "0\n",
            "episode 426 of 1000\n",
            "epsilon: 0.11820406108847166\n",
            "gamma: 0.99\n",
            "Number of games in episode 426: 84\n",
            "Avg Action Loss, 12 batches: 0.03185791281672815\n",
            "Avg Block Loss, 8 batches: 0.07583529502153397\n",
            "Avg Challenge Loss, 9 batches: 0.07573757072289784\n",
            "Avg Card Loss, 14 batches: 0.03502476953768304\n",
            "0\n",
            "episode 427 of 1000\n",
            "epsilon: 0.1176130407830293\n",
            "gamma: 0.99\n",
            "Number of games in episode 427: 78\n",
            "Avg Action Loss, 11 batches: 0.021094868848608297\n",
            "Avg Block Loss, 7 batches: 0.06483410032732147\n",
            "Avg Challenge Loss, 8 batches: 0.06608594779390842\n",
            "Avg Card Loss, 13 batches: 0.039153483911202505\n",
            "0\n",
            "episode 428 of 1000\n",
            "epsilon: 0.11702497557911415\n",
            "gamma: 0.99\n",
            "Number of games in episode 428: 79\n",
            "Avg Action Loss, 12 batches: 0.02805371015953521\n",
            "Avg Block Loss, 9 batches: 0.06455026670462555\n",
            "Avg Challenge Loss, 9 batches: 0.06681943208807045\n",
            "Avg Card Loss, 12 batches: 0.04108856525272131\n",
            "0\n",
            "episode 429 of 1000\n",
            "epsilon: 0.11643985070121858\n",
            "gamma: 0.99\n",
            "Number of games in episode 429: 75\n",
            "Avg Action Loss, 12 batches: 0.027490853526008625\n",
            "Avg Block Loss, 7 batches: 0.06758403458765574\n",
            "Avg Challenge Loss, 7 batches: 0.06991851861987795\n",
            "Avg Card Loss, 12 batches: 0.0324404050091592\n",
            "0\n",
            "episode 430 of 1000\n",
            "epsilon: 0.11585765144771248\n",
            "gamma: 0.99\n",
            "Number of games in episode 430: 72\n",
            "Avg Action Loss, 12 batches: 0.03792846699555715\n",
            "Avg Block Loss, 9 batches: 0.05361970979720354\n",
            "Avg Challenge Loss, 10 batches: 0.05958322286605835\n",
            "Avg Card Loss, 11 batches: 0.045601358213885265\n",
            "0\n",
            "win rate: 0.19\n",
            "episode 431 of 1000\n",
            "epsilon: 0.11527836319047392\n",
            "gamma: 0.99\n",
            "Number of games in episode 431: 67\n",
            "Avg Action Loss, 10 batches: 0.03588557709008455\n",
            "Avg Block Loss, 7 batches: 0.055915223700659614\n",
            "Avg Challenge Loss, 7 batches: 0.05449688740606819\n",
            "Avg Card Loss, 10 batches: 0.043762602377682924\n",
            "0\n",
            "episode 432 of 1000\n",
            "epsilon: 0.11470197137452155\n",
            "gamma: 0.99\n",
            "Number of games in episode 432: 78\n",
            "Avg Action Loss, 12 batches: 0.0287404281552881\n",
            "Avg Block Loss, 8 batches: 0.07052327645942569\n",
            "Avg Challenge Loss, 9 batches: 0.07131204816202323\n",
            "Avg Card Loss, 12 batches: 0.043386992804395653\n",
            "0\n",
            "episode 433 of 1000\n",
            "epsilon: 0.11412846151764894\n",
            "gamma: 0.99\n",
            "Number of games in episode 433: 77\n",
            "Avg Action Loss, 13 batches: 0.04003420214240368\n",
            "Avg Block Loss, 9 batches: 0.059256193745467395\n",
            "Avg Challenge Loss, 9 batches: 0.05527025523285071\n",
            "Avg Card Loss, 11 batches: 0.04531755501573736\n",
            "0\n",
            "episode 434 of 1000\n",
            "epsilon: 0.1135578192100607\n",
            "gamma: 0.99\n",
            "Number of games in episode 434: 74\n",
            "Avg Action Loss, 12 batches: 0.051814266480505466\n",
            "Avg Block Loss, 8 batches: 0.039548828499391675\n",
            "Avg Challenge Loss, 10 batches: 0.051607468631118536\n",
            "Avg Card Loss, 12 batches: 0.0431730888861542\n",
            "0\n",
            "episode 435 of 1000\n",
            "epsilon: 0.11299003011401039\n",
            "gamma: 0.99\n",
            "Number of games in episode 435: 73\n",
            "Avg Action Loss, 12 batches: 0.02800693145642678\n",
            "Avg Block Loss, 7 batches: 0.05514859714146171\n",
            "Avg Challenge Loss, 8 batches: 0.059561986941844225\n",
            "Avg Card Loss, 11 batches: 0.05475495764139024\n",
            "0\n",
            "episode 436 of 1000\n",
            "epsilon: 0.11242507996344034\n",
            "gamma: 0.99\n",
            "Number of games in episode 436: 78\n",
            "Avg Action Loss, 12 batches: 0.029002850720038015\n",
            "Avg Block Loss, 8 batches: 0.07524832559283823\n",
            "Avg Challenge Loss, 9 batches: 0.06831946161886056\n",
            "Avg Card Loss, 11 batches: 0.044849244031039154\n",
            "0\n",
            "episode 437 of 1000\n",
            "epsilon: 0.11186295456362313\n",
            "gamma: 0.99\n",
            "Number of games in episode 437: 75\n",
            "Avg Action Loss, 12 batches: 0.031591534769783415\n",
            "Avg Block Loss, 8 batches: 0.0479829553514719\n",
            "Avg Challenge Loss, 8 batches: 0.05489631509408355\n",
            "Avg Card Loss, 11 batches: 0.03835206360302188\n",
            "0\n",
            "episode 438 of 1000\n",
            "epsilon: 0.11130363979080501\n",
            "gamma: 0.99\n",
            "Number of games in episode 438: 65\n",
            "Avg Action Loss, 12 batches: 0.04490026831626892\n",
            "Avg Block Loss, 7 batches: 0.06749677352075066\n",
            "Avg Challenge Loss, 8 batches: 0.07796209864318371\n",
            "Avg Card Loss, 9 batches: 0.05103676372932063\n",
            "0\n",
            "episode 439 of 1000\n",
            "epsilon: 0.11074712159185099\n",
            "gamma: 0.99\n",
            "Number of games in episode 439: 70\n",
            "Avg Action Loss, 12 batches: 0.037726079346612096\n",
            "Avg Block Loss, 8 batches: 0.07224801369011402\n",
            "Avg Challenge Loss, 9 batches: 0.0747461879832877\n",
            "Avg Card Loss, 10 batches: 0.05353446137160063\n",
            "0\n",
            "episode 440 of 1000\n",
            "epsilon: 0.11019338598389174\n",
            "gamma: 0.99\n",
            "Number of games in episode 440: 82\n",
            "Avg Action Loss, 10 batches: 0.027467375807464122\n",
            "Avg Block Loss, 7 batches: 0.05002425744065216\n",
            "Avg Challenge Loss, 8 batches: 0.05501714418642223\n",
            "Avg Card Loss, 13 batches: 0.03207892892309106\n",
            "0\n",
            "win rate: 0.23\n",
            "episode 441 of 1000\n",
            "epsilon: 0.10964241905397228\n",
            "gamma: 0.99\n",
            "Number of games in episode 441: 73\n",
            "Avg Action Loss, 12 batches: 0.01938865310512483\n",
            "Avg Block Loss, 6 batches: 0.05693716757620374\n",
            "Avg Challenge Loss, 7 batches: 0.0623325239866972\n",
            "Avg Card Loss, 11 batches: 0.036062159152193504\n",
            "0\n",
            "episode 442 of 1000\n",
            "epsilon: 0.10909420695870241\n",
            "gamma: 0.99\n",
            "Number of games in episode 442: 76\n",
            "Avg Action Loss, 12 batches: 0.0250168974744156\n",
            "Avg Block Loss, 8 batches: 0.0409616818651557\n",
            "Avg Challenge Loss, 9 batches: 0.04480453684098191\n",
            "Avg Card Loss, 12 batches: 0.03667960626383623\n",
            "0\n",
            "episode 443 of 1000\n",
            "epsilon: 0.1085487359239089\n",
            "gamma: 0.99\n",
            "Number of games in episode 443: 78\n",
            "Avg Action Loss, 13 batches: 0.03880533647651856\n",
            "Avg Block Loss, 9 batches: 0.06603413613306151\n",
            "Avg Challenge Loss, 10 batches: 0.06663166023790837\n",
            "Avg Card Loss, 12 batches: 0.0485271824679027\n",
            "0\n",
            "episode 444 of 1000\n",
            "epsilon: 0.10800599224428936\n",
            "gamma: 0.99\n",
            "Number of games in episode 444: 79\n",
            "Avg Action Loss, 12 batches: 0.02847826372211178\n",
            "Avg Block Loss, 8 batches: 0.060232107527554035\n",
            "Avg Challenge Loss, 8 batches: 0.06690148683264852\n",
            "Avg Card Loss, 11 batches: 0.03718085430393165\n",
            "0\n",
            "episode 445 of 1000\n",
            "epsilon: 0.10746596228306791\n",
            "gamma: 0.99\n",
            "Number of games in episode 445: 69\n",
            "Avg Action Loss, 12 batches: 0.03465675089197854\n",
            "Avg Block Loss, 8 batches: 0.04566301580052823\n",
            "Avg Challenge Loss, 8 batches: 0.0497499022167176\n",
            "Avg Card Loss, 10 batches: 0.03678963854908943\n",
            "0\n",
            "episode 446 of 1000\n",
            "epsilon: 0.10692863247165257\n",
            "gamma: 0.99\n",
            "Number of games in episode 446: 75\n",
            "Avg Action Loss, 11 batches: 0.03850167007608847\n",
            "Avg Block Loss, 8 batches: 0.07380461902357638\n",
            "Avg Challenge Loss, 8 batches: 0.06659064569976181\n",
            "Avg Card Loss, 11 batches: 0.03191268835640089\n",
            "0\n",
            "episode 447 of 1000\n",
            "epsilon: 0.1063939893092943\n",
            "gamma: 0.99\n",
            "Number of games in episode 447: 76\n",
            "Avg Action Loss, 12 batches: 0.026572670127886038\n",
            "Avg Block Loss, 9 batches: 0.08843489405181673\n",
            "Avg Challenge Loss, 9 batches: 0.0823303848091099\n",
            "Avg Card Loss, 11 batches: 0.05041539529338479\n",
            "0\n",
            "episode 448 of 1000\n",
            "epsilon: 0.10586201936274783\n",
            "gamma: 0.99\n",
            "Number of games in episode 448: 72\n",
            "Avg Action Loss, 12 batches: 0.03401726003115376\n",
            "Avg Block Loss, 8 batches: 0.05939533293712884\n",
            "Avg Challenge Loss, 8 batches: 0.06955332402139902\n",
            "Avg Card Loss, 11 batches: 0.052093133330345154\n",
            "0\n",
            "episode 449 of 1000\n",
            "epsilon: 0.10533270926593409\n",
            "gamma: 0.99\n",
            "Number of games in episode 449: 79\n",
            "Avg Action Loss, 11 batches: 0.031196151521395554\n",
            "Avg Block Loss, 8 batches: 0.0627415725030005\n",
            "Avg Challenge Loss, 8 batches: 0.05992130213417113\n",
            "Avg Card Loss, 12 batches: 0.03125175014914324\n",
            "0\n",
            "episode 450 of 1000\n",
            "epsilon: 0.10480604571960442\n",
            "gamma: 0.99\n",
            "Number of games in episode 450: 77\n",
            "Avg Action Loss, 12 batches: 0.03064495239717265\n",
            "Avg Block Loss, 8 batches: 0.07389756408520043\n",
            "Avg Challenge Loss, 9 batches: 0.07162856910791662\n",
            "Avg Card Loss, 11 batches: 0.04738615059547804\n",
            "0\n",
            "win rate: 0.25\n",
            "episode 451 of 1000\n",
            "epsilon: 0.1042820154910064\n",
            "gamma: 0.99\n",
            "Number of games in episode 451: 79\n",
            "Avg Action Loss, 13 batches: 0.027576197046213426\n",
            "Avg Block Loss, 8 batches: 0.06259091140236706\n",
            "Avg Challenge Loss, 9 batches: 0.06156682719786962\n",
            "Avg Card Loss, 11 batches: 0.04541383192620494\n",
            "0\n",
            "episode 452 of 1000\n",
            "epsilon: 0.10376060541355137\n",
            "gamma: 0.99\n",
            "Number of games in episode 452: 74\n",
            "Avg Action Loss, 12 batches: 0.03608659150389334\n",
            "Avg Block Loss, 9 batches: 0.049934815201494426\n",
            "Avg Challenge Loss, 9 batches: 0.05110862685574426\n",
            "Avg Card Loss, 10 batches: 0.04042404447682202\n",
            "0\n",
            "episode 453 of 1000\n",
            "epsilon: 0.1032418023864836\n",
            "gamma: 0.99\n",
            "Number of games in episode 453: 72\n",
            "Avg Action Loss, 12 batches: 0.03614947393847009\n",
            "Avg Block Loss, 7 batches: 0.07118410377630166\n",
            "Avg Challenge Loss, 8 batches: 0.0654933387413621\n",
            "Avg Card Loss, 10 batches: 0.034273111773654816\n",
            "0\n",
            "episode 454 of 1000\n",
            "epsilon: 0.10272559337455119\n",
            "gamma: 0.99\n",
            "Number of games in episode 454: 72\n",
            "Avg Action Loss, 11 batches: 0.03149182349443436\n",
            "Avg Block Loss, 7 batches: 0.063141705468297\n",
            "Avg Challenge Loss, 8 batches: 0.06912736059166491\n",
            "Avg Card Loss, 11 batches: 0.03650100279430097\n",
            "0\n",
            "episode 455 of 1000\n",
            "epsilon: 0.10221196540767843\n",
            "gamma: 0.99\n",
            "Number of games in episode 455: 80\n",
            "Avg Action Loss, 11 batches: 0.029307713901454754\n",
            "Avg Block Loss, 8 batches: 0.04273717640899122\n",
            "Avg Challenge Loss, 9 batches: 0.052461422979831696\n",
            "Avg Card Loss, 13 batches: 0.037150358208096944\n",
            "0\n",
            "episode 456 of 1000\n",
            "epsilon: 0.10170090558064004\n",
            "gamma: 0.99\n",
            "Number of games in episode 456: 79\n",
            "Avg Action Loss, 12 batches: 0.03254578200479349\n",
            "Avg Block Loss, 8 batches: 0.055243162671104074\n",
            "Avg Challenge Loss, 9 batches: 0.07542556731237306\n",
            "Avg Card Loss, 11 batches: 0.03742441636594859\n",
            "0\n",
            "episode 457 of 1000\n",
            "epsilon: 0.10119240105273684\n",
            "gamma: 0.99\n",
            "Number of games in episode 457: 70\n",
            "Avg Action Loss, 13 batches: 0.03285026407012573\n",
            "Avg Block Loss, 8 batches: 0.056126573821529746\n",
            "Avg Challenge Loss, 9 batches: 0.06440001767542627\n",
            "Avg Card Loss, 9 batches: 0.03535316698253155\n",
            "0\n",
            "episode 458 of 1000\n",
            "epsilon: 0.10068643904747315\n",
            "gamma: 0.99\n",
            "Number of games in episode 458: 73\n",
            "Avg Action Loss, 12 batches: 0.033275380924654506\n",
            "Avg Block Loss, 8 batches: 0.06779318512417376\n",
            "Avg Challenge Loss, 9 batches: 0.07203640871577793\n",
            "Avg Card Loss, 10 batches: 0.03189036720432341\n",
            "0\n",
            "episode 459 of 1000\n",
            "epsilon: 0.10018300685223579\n",
            "gamma: 0.99\n",
            "Number of games in episode 459: 75\n",
            "Avg Action Loss, 12 batches: 0.032071852358058095\n",
            "Avg Block Loss, 8 batches: 0.07636873912997544\n",
            "Avg Challenge Loss, 9 batches: 0.08206898305151197\n",
            "Avg Card Loss, 10 batches: 0.04433292560279369\n",
            "0\n",
            "episode 460 of 1000\n",
            "epsilon: 0.0996820918179746\n",
            "gamma: 0.99\n",
            "Number of games in episode 460: 79\n",
            "Avg Action Loss, 12 batches: 0.03486309697230657\n",
            "Avg Block Loss, 8 batches: 0.05852373829111457\n",
            "Avg Challenge Loss, 9 batches: 0.05841110646724701\n",
            "Avg Card Loss, 12 batches: 0.04594987099214146\n",
            "0\n",
            "win rate: 0.22\n",
            "episode 461 of 1000\n",
            "epsilon: 0.09918368135888474\n",
            "gamma: 0.99\n",
            "Number of games in episode 461: 71\n",
            "Avg Action Loss, 13 batches: 0.03132518911017822\n",
            "Avg Block Loss, 8 batches: 0.06276674522086978\n",
            "Avg Challenge Loss, 8 batches: 0.06586435798089951\n",
            "Avg Card Loss, 10 batches: 0.04227174511179328\n",
            "0\n",
            "episode 462 of 1000\n",
            "epsilon: 0.09868776295209031\n",
            "gamma: 0.99\n",
            "Number of games in episode 462: 76\n",
            "Avg Action Loss, 13 batches: 0.035973938110356145\n",
            "Avg Block Loss, 8 batches: 0.07820199034176767\n",
            "Avg Challenge Loss, 9 batches: 0.07791569704810779\n",
            "Avg Card Loss, 11 batches: 0.05424118702384559\n",
            "0\n",
            "episode 463 of 1000\n",
            "epsilon: 0.09819432413732986\n",
            "gamma: 0.99\n",
            "Number of games in episode 463: 73\n",
            "Avg Action Loss, 12 batches: 0.03499851484472553\n",
            "Avg Block Loss, 8 batches: 0.07947317650541663\n",
            "Avg Challenge Loss, 8 batches: 0.09353834670037031\n",
            "Avg Card Loss, 11 batches: 0.06126767142929814\n",
            "0\n",
            "episode 464 of 1000\n",
            "epsilon: 0.09770335251664321\n",
            "gamma: 0.99\n",
            "Number of games in episode 464: 75\n",
            "Avg Action Loss, 13 batches: 0.03366970650565166\n",
            "Avg Block Loss, 8 batches: 0.0837232144549489\n",
            "Avg Challenge Loss, 9 batches: 0.09675239926824968\n",
            "Avg Card Loss, 12 batches: 0.053990694461390376\n",
            "0\n",
            "episode 465 of 1000\n",
            "epsilon: 0.09721483575406\n",
            "gamma: 0.99\n",
            "Number of games in episode 465: 78\n",
            "Avg Action Loss, 12 batches: 0.03162740675422052\n",
            "Avg Block Loss, 8 batches: 0.06061657681129873\n",
            "Avg Challenge Loss, 9 batches: 0.07251868314213222\n",
            "Avg Card Loss, 11 batches: 0.04957864856855436\n",
            "0\n",
            "episode 466 of 1000\n",
            "epsilon: 0.09672876157528969\n",
            "gamma: 0.99\n",
            "Number of games in episode 466: 71\n",
            "Avg Action Loss, 11 batches: 0.04407221620733088\n",
            "Avg Block Loss, 7 batches: 0.05751903009201799\n",
            "Avg Challenge Loss, 7 batches: 0.06440495406942708\n",
            "Avg Card Loss, 11 batches: 0.02807132713496685\n",
            "0\n",
            "episode 467 of 1000\n",
            "epsilon: 0.09624511776741324\n",
            "gamma: 0.99\n",
            "Number of games in episode 467: 74\n",
            "Avg Action Loss, 12 batches: 0.029200786181415122\n",
            "Avg Block Loss, 9 batches: 0.06586068992813428\n",
            "Avg Challenge Loss, 9 batches: 0.07192378284202681\n",
            "Avg Card Loss, 10 batches: 0.0330590314231813\n",
            "0\n",
            "episode 468 of 1000\n",
            "epsilon: 0.09576389217857617\n",
            "gamma: 0.99\n",
            "Number of games in episode 468: 77\n",
            "Avg Action Loss, 12 batches: 0.027686998480930924\n",
            "Avg Block Loss, 8 batches: 0.05720793385989964\n",
            "Avg Challenge Loss, 8 batches: 0.05866889236494899\n",
            "Avg Card Loss, 11 batches: 0.04452953767031431\n",
            "0\n",
            "episode 469 of 1000\n",
            "epsilon: 0.09528507271768329\n",
            "gamma: 0.99\n",
            "Number of games in episode 469: 80\n",
            "Avg Action Loss, 11 batches: 0.024028131399642338\n",
            "Avg Block Loss, 7 batches: 0.08648881954806191\n",
            "Avg Challenge Loss, 8 batches: 0.08688516356050968\n",
            "Avg Card Loss, 12 batches: 0.035008447943255305\n",
            "0\n",
            "episode 470 of 1000\n",
            "epsilon: 0.09480864735409487\n",
            "gamma: 0.99\n",
            "Number of games in episode 470: 70\n",
            "Avg Action Loss, 12 batches: 0.029298920960476\n",
            "Avg Block Loss, 7 batches: 0.04983007428901536\n",
            "Avg Challenge Loss, 8 batches: 0.048679493833333254\n",
            "Avg Card Loss, 11 batches: 0.032961454073136505\n",
            "0\n",
            "win rate: 0.19\n",
            "episode 471 of 1000\n",
            "epsilon: 0.0943346041173244\n",
            "gamma: 0.99\n",
            "Number of games in episode 471: 79\n",
            "Avg Action Loss, 12 batches: 0.022389345647146303\n",
            "Avg Block Loss, 7 batches: 0.06630578716950757\n",
            "Avg Challenge Loss, 8 batches: 0.05342159536667168\n",
            "Avg Card Loss, 12 batches: 0.05067416427967449\n",
            "0\n",
            "episode 472 of 1000\n",
            "epsilon: 0.09386293109673778\n",
            "gamma: 0.99\n",
            "Number of games in episode 472: 80\n",
            "Avg Action Loss, 11 batches: 0.03295866522768682\n",
            "Avg Block Loss, 7 batches: 0.0413775946944952\n",
            "Avg Challenge Loss, 8 batches: 0.05857855686917901\n",
            "Avg Card Loss, 13 batches: 0.039696791782402076\n",
            "0\n",
            "episode 473 of 1000\n",
            "epsilon: 0.09339361644125409\n",
            "gamma: 0.99\n",
            "Number of games in episode 473: 71\n",
            "Avg Action Loss, 12 batches: 0.03911357675679028\n",
            "Avg Block Loss, 8 batches: 0.04725458403117955\n",
            "Avg Challenge Loss, 9 batches: 0.060624866435925163\n",
            "Avg Card Loss, 9 batches: 0.046801280023323164\n",
            "0\n",
            "episode 474 of 1000\n",
            "epsilon: 0.09292664835904782\n",
            "gamma: 0.99\n",
            "Number of games in episode 474: 82\n",
            "Avg Action Loss, 12 batches: 0.027572744836409886\n",
            "Avg Block Loss, 8 batches: 0.0879450828069821\n",
            "Avg Challenge Loss, 9 batches: 0.09008864520324601\n",
            "Avg Card Loss, 12 batches: 0.046072562031137444\n",
            "0\n",
            "episode 475 of 1000\n",
            "epsilon: 0.09246201511725258\n",
            "gamma: 0.99\n",
            "Number of games in episode 475: 74\n",
            "Avg Action Loss, 13 batches: 0.03560161361327538\n",
            "Avg Block Loss, 8 batches: 0.07036716304719448\n",
            "Avg Challenge Loss, 9 batches: 0.07909903923670451\n",
            "Avg Card Loss, 10 batches: 0.047712290287017824\n",
            "0\n",
            "episode 476 of 1000\n",
            "epsilon: 0.09199970504166631\n",
            "gamma: 0.99\n",
            "Number of games in episode 476: 74\n",
            "Avg Action Loss, 12 batches: 0.04141516673068205\n",
            "Avg Block Loss, 8 batches: 0.09347765205893666\n",
            "Avg Challenge Loss, 9 batches: 0.09185728306571643\n",
            "Avg Card Loss, 11 batches: 0.03267903249202804\n",
            "0\n",
            "episode 477 of 1000\n",
            "epsilon: 0.09153970651645797\n",
            "gamma: 0.99\n",
            "Number of games in episode 477: 76\n",
            "Avg Action Loss, 12 batches: 0.032630473763371505\n",
            "Avg Block Loss, 8 batches: 0.05822324869222939\n",
            "Avg Challenge Loss, 9 batches: 0.06942296566234694\n",
            "Avg Card Loss, 11 batches: 0.056968229060823265\n",
            "0\n",
            "episode 478 of 1000\n",
            "epsilon: 0.09108200798387568\n",
            "gamma: 0.99\n",
            "Number of games in episode 478: 75\n",
            "Avg Action Loss, 12 batches: 0.03138134418986738\n",
            "Avg Block Loss, 7 batches: 0.047994649436857016\n",
            "Avg Challenge Loss, 7 batches: 0.053092875118766515\n",
            "Avg Card Loss, 11 batches: 0.037799665789035236\n",
            "0\n",
            "episode 479 of 1000\n",
            "epsilon: 0.0906265979439563\n",
            "gamma: 0.99\n",
            "Number of games in episode 479: 73\n",
            "Avg Action Loss, 12 batches: 0.03551762690767646\n",
            "Avg Block Loss, 8 batches: 0.07664269342785701\n",
            "Avg Challenge Loss, 9 batches: 0.07185228189660443\n",
            "Avg Card Loss, 11 batches: 0.05789474279365756\n",
            "0\n",
            "episode 480 of 1000\n",
            "epsilon: 0.09017346495423652\n",
            "gamma: 0.99\n",
            "Number of games in episode 480: 74\n",
            "Avg Action Loss, 11 batches: 0.025761470791291107\n",
            "Avg Block Loss, 6 batches: 0.07565206661820412\n",
            "Avg Challenge Loss, 6 batches: 0.06919204847266276\n",
            "Avg Card Loss, 11 batches: 0.039163171483034435\n",
            "0\n",
            "win rate: 0.24\n",
            "episode 481 of 1000\n",
            "epsilon: 0.08972259762946533\n",
            "gamma: 0.99\n",
            "Number of games in episode 481: 72\n",
            "Avg Action Loss, 13 batches: 0.04890229467016\n",
            "Avg Block Loss, 9 batches: 0.07906350865960121\n",
            "Avg Challenge Loss, 9 batches: 0.08923599703444375\n",
            "Avg Card Loss, 10 batches: 0.038349969405680896\n",
            "0\n",
            "episode 482 of 1000\n",
            "epsilon: 0.089273984641318\n",
            "gamma: 0.99\n",
            "Number of games in episode 482: 75\n",
            "Avg Action Loss, 14 batches: 0.022618951076375588\n",
            "Avg Block Loss, 10 batches: 0.06380177605897189\n",
            "Avg Challenge Loss, 11 batches: 0.07009156607091427\n",
            "Avg Card Loss, 10 batches: 0.06184588652104139\n",
            "0\n",
            "episode 483 of 1000\n",
            "epsilon: 0.0888276147181114\n",
            "gamma: 0.99\n",
            "Number of games in episode 483: 72\n",
            "Avg Action Loss, 11 batches: 0.02746753284538334\n",
            "Avg Block Loss, 8 batches: 0.03739577147644013\n",
            "Avg Challenge Loss, 8 batches: 0.04248425713740289\n",
            "Avg Card Loss, 11 batches: 0.04264131146059795\n",
            "0\n",
            "episode 484 of 1000\n",
            "epsilon: 0.08838347664452084\n",
            "gamma: 0.99\n",
            "Number of games in episode 484: 75\n",
            "Avg Action Loss, 12 batches: 0.03446825469533602\n",
            "Avg Block Loss, 8 batches: 0.07667720643803477\n",
            "Avg Challenge Loss, 9 batches: 0.08564979334672292\n",
            "Avg Card Loss, 11 batches: 0.02790410960601135\n",
            "0\n",
            "episode 485 of 1000\n",
            "epsilon: 0.08794155926129824\n",
            "gamma: 0.99\n",
            "Number of games in episode 485: 70\n",
            "Avg Action Loss, 10 batches: 0.025178275257349014\n",
            "Avg Block Loss, 6 batches: 0.03808943508192897\n",
            "Avg Challenge Loss, 7 batches: 0.0440934422825064\n",
            "Avg Card Loss, 12 batches: 0.0299176467427363\n",
            "0\n",
            "episode 486 of 1000\n",
            "epsilon: 0.08750185146499175\n",
            "gamma: 0.99\n",
            "Number of games in episode 486: 75\n",
            "Avg Action Loss, 12 batches: 0.03694438779105743\n",
            "Avg Block Loss, 8 batches: 0.0535273861605674\n",
            "Avg Challenge Loss, 8 batches: 0.06228041066788137\n",
            "Avg Card Loss, 11 batches: 0.04778270694342526\n",
            "0\n",
            "episode 487 of 1000\n",
            "epsilon: 0.08706434220766679\n",
            "gamma: 0.99\n",
            "Number of games in episode 487: 74\n",
            "Avg Action Loss, 12 batches: 0.02842771322078382\n",
            "Avg Block Loss, 7 batches: 0.06557858788541385\n",
            "Avg Challenge Loss, 8 batches: 0.06510379863902926\n",
            "Avg Card Loss, 10 batches: 0.04164613950997591\n",
            "0\n",
            "episode 488 of 1000\n",
            "epsilon: 0.08662902049662846\n",
            "gamma: 0.99\n",
            "Number of games in episode 488: 80\n",
            "Avg Action Loss, 12 batches: 0.042647603705214955\n",
            "Avg Block Loss, 8 batches: 0.07428602641448379\n",
            "Avg Challenge Loss, 9 batches: 0.0847102664411068\n",
            "Avg Card Loss, 12 batches: 0.04886794703391691\n",
            "0\n",
            "episode 489 of 1000\n",
            "epsilon: 0.08619587539414532\n",
            "gamma: 0.99\n",
            "Number of games in episode 489: 80\n",
            "Avg Action Loss, 12 batches: 0.05026054847985506\n",
            "Avg Block Loss, 8 batches: 0.06706248549744487\n",
            "Avg Challenge Loss, 9 batches: 0.0900308671924803\n",
            "Avg Card Loss, 12 batches: 0.04329278368580466\n",
            "0\n",
            "episode 490 of 1000\n",
            "epsilon: 0.08576489601717459\n",
            "gamma: 0.99\n",
            "Number of games in episode 490: 70\n",
            "Avg Action Loss, 12 batches: 0.04083021404221654\n",
            "Avg Block Loss, 8 batches: 0.041331645916216075\n",
            "Avg Challenge Loss, 9 batches: 0.05259948906799158\n",
            "Avg Card Loss, 10 batches: 0.04974432177841663\n",
            "0\n",
            "win rate: 0.25\n",
            "episode 491 of 1000\n",
            "epsilon: 0.08533607153708872\n",
            "gamma: 0.99\n",
            "Number of games in episode 491: 76\n",
            "Avg Action Loss, 12 batches: 0.03372025225932399\n",
            "Avg Block Loss, 8 batches: 0.060517050442285836\n",
            "Avg Challenge Loss, 9 batches: 0.06665647547278139\n",
            "Avg Card Loss, 12 batches: 0.05366195132955909\n",
            "0\n",
            "episode 492 of 1000\n",
            "epsilon: 0.08490939117940327\n",
            "gamma: 0.99\n",
            "Number of games in episode 492: 78\n",
            "Avg Action Loss, 12 batches: 0.03964032744988799\n",
            "Avg Block Loss, 8 batches: 0.06285036448389292\n",
            "Avg Challenge Loss, 8 batches: 0.0782642257399857\n",
            "Avg Card Loss, 12 batches: 0.04107223373527328\n",
            "0\n",
            "episode 493 of 1000\n",
            "epsilon: 0.08448484422350626\n",
            "gamma: 0.99\n",
            "Number of games in episode 493: 77\n",
            "Avg Action Loss, 13 batches: 0.02092228665088232\n",
            "Avg Block Loss, 8 batches: 0.05396241182461381\n",
            "Avg Challenge Loss, 9 batches: 0.05788527905113167\n",
            "Avg Card Loss, 12 batches: 0.03815257712267339\n",
            "0\n",
            "episode 494 of 1000\n",
            "epsilon: 0.08406242000238873\n",
            "gamma: 0.99\n",
            "Number of games in episode 494: 74\n",
            "Avg Action Loss, 12 batches: 0.03892677789554\n",
            "Avg Block Loss, 8 batches: 0.06314844801090658\n",
            "Avg Challenge Loss, 9 batches: 0.07383852462387747\n",
            "Avg Card Loss, 11 batches: 0.035869690250944004\n",
            "0\n",
            "episode 495 of 1000\n",
            "epsilon: 0.08364210790237678\n",
            "gamma: 0.99\n",
            "Number of games in episode 495: 76\n",
            "Avg Action Loss, 13 batches: 0.0373651936936837\n",
            "Avg Block Loss, 8 batches: 0.059203451965004206\n",
            "Avg Challenge Loss, 9 batches: 0.05618035503559642\n",
            "Avg Card Loss, 11 batches: 0.03374583172527226\n",
            "0\n",
            "episode 496 of 1000\n",
            "epsilon: 0.0832238973628649\n",
            "gamma: 0.99\n",
            "Number of games in episode 496: 75\n",
            "Avg Action Loss, 11 batches: 0.029172973178157754\n",
            "Avg Block Loss, 8 batches: 0.08509388379752636\n",
            "Avg Challenge Loss, 8 batches: 0.09526347066275775\n",
            "Avg Card Loss, 12 batches: 0.030170009083425004\n",
            "0\n",
            "episode 497 of 1000\n",
            "epsilon: 0.08280777787605056\n",
            "gamma: 0.99\n",
            "Number of games in episode 497: 79\n",
            "Avg Action Loss, 12 batches: 0.024764936106900375\n",
            "Avg Block Loss, 8 batches: 0.06712188210804015\n",
            "Avg Challenge Loss, 9 batches: 0.06340899753073852\n",
            "Avg Card Loss, 13 batches: 0.030000315298541233\n",
            "0\n",
            "episode 498 of 1000\n",
            "epsilon: 0.08239373898667031\n",
            "gamma: 0.99\n",
            "Number of games in episode 498: 75\n",
            "Avg Action Loss, 11 batches: 0.02580735986967656\n",
            "Avg Block Loss, 8 batches: 0.055423269513994455\n",
            "Avg Challenge Loss, 8 batches: 0.05791345797479153\n",
            "Avg Card Loss, 11 batches: 0.03975513162599369\n",
            "0\n",
            "episode 499 of 1000\n",
            "epsilon: 0.08198177029173696\n",
            "gamma: 0.99\n",
            "Number of games in episode 499: 76\n",
            "Avg Action Loss, 12 batches: 0.02764378914919992\n",
            "Avg Block Loss, 8 batches: 0.04829499707557261\n",
            "Avg Challenge Loss, 8 batches: 0.048496522940695286\n",
            "Avg Card Loss, 11 batches: 0.05017579651691697\n",
            "0\n",
            "episode 500 of 1000\n",
            "epsilon: 0.08157186144027828\n",
            "gamma: 0.99\n",
            "Number of games in episode 500: 78\n",
            "Avg Action Loss, 11 batches: 0.028171345676210793\n",
            "Avg Block Loss, 8 batches: 0.05775029677897692\n",
            "Avg Challenge Loss, 9 batches: 0.06005440124620994\n",
            "Avg Card Loss, 12 batches: 0.036500956145270415\n",
            "0\n",
            "win rate: 0.2\n",
            "episode 501 of 1000\n",
            "epsilon: 0.0811640021330769\n",
            "gamma: 0.99\n",
            "Number of games in episode 501: 81\n",
            "Avg Action Loss, 11 batches: 0.02479868483814326\n",
            "Avg Block Loss, 8 batches: 0.043535572942346334\n",
            "Avg Challenge Loss, 8 batches: 0.05102806130889803\n",
            "Avg Card Loss, 13 batches: 0.035682594331984334\n",
            "0\n",
            "episode 502 of 1000\n",
            "epsilon: 0.08075818212241151\n",
            "gamma: 0.99\n",
            "Number of games in episode 502: 71\n",
            "Avg Action Loss, 12 batches: 0.038138185045681894\n",
            "Avg Block Loss, 8 batches: 0.0558301811106503\n",
            "Avg Challenge Loss, 8 batches: 0.06526848301291466\n",
            "Avg Card Loss, 11 batches: 0.038517686453732575\n",
            "0\n",
            "episode 503 of 1000\n",
            "epsilon: 0.08035439121179945\n",
            "gamma: 0.99\n",
            "Number of games in episode 503: 77\n",
            "Avg Action Loss, 12 batches: 0.03817681331808368\n",
            "Avg Block Loss, 8 batches: 0.05578181892633438\n",
            "Avg Challenge Loss, 9 batches: 0.0584507253434923\n",
            "Avg Card Loss, 12 batches: 0.05782676829646031\n",
            "0\n",
            "episode 504 of 1000\n",
            "epsilon: 0.07995261925574046\n",
            "gamma: 0.99\n",
            "Number of games in episode 504: 73\n",
            "Avg Action Loss, 12 batches: 0.03869715084632238\n",
            "Avg Block Loss, 8 batches: 0.05133653013035655\n",
            "Avg Challenge Loss, 9 batches: 0.06345486454665661\n",
            "Avg Card Loss, 11 batches: 0.02362464000047608\n",
            "0\n",
            "episode 505 of 1000\n",
            "epsilon: 0.07955285615946175\n",
            "gamma: 0.99\n",
            "Number of games in episode 505: 70\n",
            "Avg Action Loss, 12 batches: 0.03431843228948613\n",
            "Avg Block Loss, 7 batches: 0.05042574009192841\n",
            "Avg Challenge Loss, 8 batches: 0.054299667943269014\n",
            "Avg Card Loss, 10 batches: 0.039785140007734296\n",
            "0\n",
            "episode 506 of 1000\n",
            "epsilon: 0.07915509187866444\n",
            "gamma: 0.99\n",
            "Number of games in episode 506: 73\n",
            "Avg Action Loss, 11 batches: 0.039786945435811176\n",
            "Avg Block Loss, 7 batches: 0.0723093676247767\n",
            "Avg Challenge Loss, 8 batches: 0.07011543167755008\n",
            "Avg Card Loss, 11 batches: 0.05059740269048647\n",
            "0\n",
            "episode 507 of 1000\n",
            "epsilon: 0.07875931641927113\n",
            "gamma: 0.99\n",
            "Number of games in episode 507: 78\n",
            "Avg Action Loss, 11 batches: 0.031340505589138375\n",
            "Avg Block Loss, 8 batches: 0.05853243335150182\n",
            "Avg Challenge Loss, 8 batches: 0.06340881111100316\n",
            "Avg Card Loss, 12 batches: 0.03253533225506544\n",
            "0\n",
            "episode 508 of 1000\n",
            "epsilon: 0.07836551983717477\n",
            "gamma: 0.99\n",
            "Number of games in episode 508: 75\n",
            "Avg Action Loss, 13 batches: 0.027133909412301503\n",
            "Avg Block Loss, 9 batches: 0.06858213354522984\n",
            "Avg Challenge Loss, 9 batches: 0.08050463379671176\n",
            "Avg Card Loss, 11 batches: 0.04593724190172824\n",
            "0\n",
            "episode 509 of 1000\n",
            "epsilon: 0.07797369223798889\n",
            "gamma: 0.99\n",
            "Number of games in episode 509: 75\n",
            "Avg Action Loss, 11 batches: 0.03081509843468666\n",
            "Avg Block Loss, 8 batches: 0.04915381933096796\n",
            "Avg Challenge Loss, 8 batches: 0.05013845523353666\n",
            "Avg Card Loss, 12 batches: 0.033507045513639845\n",
            "0\n",
            "episode 510 of 1000\n",
            "epsilon: 0.07758382377679894\n",
            "gamma: 0.99\n",
            "Number of games in episode 510: 82\n",
            "Avg Action Loss, 11 batches: 0.023227184375917368\n",
            "Avg Block Loss, 7 batches: 0.09619592263230256\n",
            "Avg Challenge Loss, 8 batches: 0.09538484050426632\n",
            "Avg Card Loss, 12 batches: 0.04874530842062086\n",
            "0\n",
            "win rate: 0.17\n",
            "episode 511 of 1000\n",
            "epsilon: 0.07719590465791494\n",
            "gamma: 0.99\n",
            "Number of games in episode 511: 80\n",
            "Avg Action Loss, 11 batches: 0.032003559595481915\n",
            "Avg Block Loss, 8 batches: 0.05902055185288191\n",
            "Avg Challenge Loss, 8 batches: 0.06072829244658351\n",
            "Avg Card Loss, 12 batches: 0.037659449386410415\n",
            "0\n",
            "episode 512 of 1000\n",
            "epsilon: 0.07680992513462537\n",
            "gamma: 0.99\n",
            "Number of games in episode 512: 75\n",
            "Avg Action Loss, 11 batches: 0.02772184596820311\n",
            "Avg Block Loss, 7 batches: 0.0703794372134975\n",
            "Avg Challenge Loss, 8 batches: 0.06706414115615189\n",
            "Avg Card Loss, 12 batches: 0.029257874004542828\n",
            "0\n",
            "episode 513 of 1000\n",
            "epsilon: 0.07642587550895225\n",
            "gamma: 0.99\n",
            "Number of games in episode 513: 70\n",
            "Avg Action Loss, 12 batches: 0.02923777528728048\n",
            "Avg Block Loss, 8 batches: 0.0658217784948647\n",
            "Avg Challenge Loss, 8 batches: 0.073815573239699\n",
            "Avg Card Loss, 10 batches: 0.05171947376802564\n",
            "0\n",
            "episode 514 of 1000\n",
            "epsilon: 0.07604374613140748\n",
            "gamma: 0.99\n",
            "Number of games in episode 514: 74\n",
            "Avg Action Loss, 12 batches: 0.031262337851027645\n",
            "Avg Block Loss, 7 batches: 0.03855533871267523\n",
            "Avg Challenge Loss, 8 batches: 0.03667882282752544\n",
            "Avg Card Loss, 12 batches: 0.045616329569990434\n",
            "0\n",
            "episode 515 of 1000\n",
            "epsilon: 0.07566352740075044\n",
            "gamma: 0.99\n",
            "Number of games in episode 515: 75\n",
            "Avg Action Loss, 13 batches: 0.02940929767030936\n",
            "Avg Block Loss, 8 batches: 0.04503211367409676\n",
            "Avg Challenge Loss, 9 batches: 0.04159526309619347\n",
            "Avg Card Loss, 11 batches: 0.03800221041522243\n",
            "0\n",
            "episode 516 of 1000\n",
            "epsilon: 0.07528520976374668\n",
            "gamma: 0.99\n",
            "Number of games in episode 516: 74\n",
            "Avg Action Loss, 13 batches: 0.025773987388954714\n",
            "Avg Block Loss, 9 batches: 0.058655095390147634\n",
            "Avg Challenge Loss, 9 batches: 0.05876186634931299\n",
            "Avg Card Loss, 11 batches: 0.04721430731429295\n",
            "0\n",
            "episode 517 of 1000\n",
            "epsilon: 0.07490878371492794\n",
            "gamma: 0.99\n",
            "Number of games in episode 517: 76\n",
            "Avg Action Loss, 12 batches: 0.02994977030903101\n",
            "Avg Block Loss, 8 batches: 0.06729443941731006\n",
            "Avg Challenge Loss, 8 batches: 0.0644385633058846\n",
            "Avg Card Loss, 12 batches: 0.02806930507843693\n",
            "0\n",
            "episode 518 of 1000\n",
            "epsilon: 0.0745342397963533\n",
            "gamma: 0.99\n",
            "Number of games in episode 518: 77\n",
            "Avg Action Loss, 11 batches: 0.02833252450959249\n",
            "Avg Block Loss, 8 batches: 0.06857194262556732\n",
            "Avg Challenge Loss, 8 batches: 0.07007066532969475\n",
            "Avg Card Loss, 12 batches: 0.04162143252324313\n",
            "0\n",
            "episode 519 of 1000\n",
            "epsilon: 0.07416156859737154\n",
            "gamma: 0.99\n",
            "Number of games in episode 519: 80\n",
            "Avg Action Loss, 11 batches: 0.03447371374138377\n",
            "Avg Block Loss, 8 batches: 0.07238651486113667\n",
            "Avg Challenge Loss, 8 batches: 0.061700022662989795\n",
            "Avg Card Loss, 12 batches: 0.03899032789437721\n",
            "0\n",
            "episode 520 of 1000\n",
            "epsilon: 0.07379076075438468\n",
            "gamma: 0.99\n",
            "Number of games in episode 520: 72\n",
            "Avg Action Loss, 11 batches: 0.03369355481117964\n",
            "Avg Block Loss, 7 batches: 0.0598528544817652\n",
            "Avg Challenge Loss, 8 batches: 0.06748512387275696\n",
            "Avg Card Loss, 11 batches: 0.026030314535918562\n",
            "0\n",
            "win rate: 0.25\n",
            "episode 521 of 1000\n",
            "epsilon: 0.07342180695061275\n",
            "gamma: 0.99\n",
            "Number of games in episode 521: 77\n",
            "Avg Action Loss, 12 batches: 0.023205386474728584\n",
            "Avg Block Loss, 8 batches: 0.04054014652501792\n",
            "Avg Challenge Loss, 9 batches: 0.04270720052429371\n",
            "Avg Card Loss, 12 batches: 0.034940602141432464\n",
            "0\n",
            "episode 522 of 1000\n",
            "epsilon: 0.07305469791585968\n",
            "gamma: 0.99\n",
            "Number of games in episode 522: 77\n",
            "Avg Action Loss, 11 batches: 0.03818596713244915\n",
            "Avg Block Loss, 7 batches: 0.08942792391670602\n",
            "Avg Challenge Loss, 8 batches: 0.0906002577394247\n",
            "Avg Card Loss, 11 batches: 0.04669828670607372\n",
            "0\n",
            "episode 523 of 1000\n",
            "epsilon: 0.07268942442628039\n",
            "gamma: 0.99\n",
            "Number of games in episode 523: 74\n",
            "Avg Action Loss, 11 batches: 0.039146205664358356\n",
            "Avg Block Loss, 7 batches: 0.043348368390330246\n",
            "Avg Challenge Loss, 7 batches: 0.05041197407990694\n",
            "Avg Card Loss, 12 batches: 0.030642073329848547\n",
            "0\n",
            "episode 524 of 1000\n",
            "epsilon: 0.07232597730414898\n",
            "gamma: 0.99\n",
            "Number of games in episode 524: 79\n",
            "Avg Action Loss, 12 batches: 0.0289188582294931\n",
            "Avg Block Loss, 8 batches: 0.04491099016740918\n",
            "Avg Challenge Loss, 9 batches: 0.051222259592678815\n",
            "Avg Card Loss, 12 batches: 0.04418697780541455\n",
            "0\n",
            "episode 525 of 1000\n",
            "epsilon: 0.07196434741762824\n",
            "gamma: 0.99\n",
            "Number of games in episode 525: 74\n",
            "Avg Action Loss, 11 batches: 0.033247076948596674\n",
            "Avg Block Loss, 8 batches: 0.07335730013437569\n",
            "Avg Challenge Loss, 9 batches: 0.07577146641496155\n",
            "Avg Card Loss, 12 batches: 0.04011420998722315\n",
            "0\n",
            "episode 526 of 1000\n",
            "epsilon: 0.0716045256805401\n",
            "gamma: 0.99\n",
            "Number of games in episode 526: 82\n",
            "Avg Action Loss, 11 batches: 0.030159373429011215\n",
            "Avg Block Loss, 8 batches: 0.04822650318965316\n",
            "Avg Challenge Loss, 9 batches: 0.05553871372507678\n",
            "Avg Card Loss, 13 batches: 0.04501931483928974\n",
            "0\n",
            "episode 527 of 1000\n",
            "epsilon: 0.0712465030521374\n",
            "gamma: 0.99\n",
            "Number of games in episode 527: 72\n",
            "Avg Action Loss, 12 batches: 0.03403404292960962\n",
            "Avg Block Loss, 7 batches: 0.03547503160578864\n",
            "Avg Challenge Loss, 8 batches: 0.0390062197111547\n",
            "Avg Card Loss, 11 batches: 0.031039208702912383\n",
            "0\n",
            "episode 528 of 1000\n",
            "epsilon: 0.0708902705368767\n",
            "gamma: 0.99\n",
            "Number of games in episode 528: 81\n",
            "Avg Action Loss, 11 batches: 0.038637388836253776\n",
            "Avg Block Loss, 7 batches: 0.08935372957161494\n",
            "Avg Challenge Loss, 8 batches: 0.08322535851038992\n",
            "Avg Card Loss, 12 batches: 0.04946603517358502\n",
            "0\n",
            "episode 529 of 1000\n",
            "epsilon: 0.07053581918419231\n",
            "gamma: 0.99\n",
            "Number of games in episode 529: 68\n",
            "Avg Action Loss, 12 batches: 0.0352040211049219\n",
            "Avg Block Loss, 8 batches: 0.04233775241300464\n",
            "Avg Challenge Loss, 8 batches: 0.049206879921257496\n",
            "Avg Card Loss, 10 batches: 0.03641264103353024\n",
            "0\n",
            "episode 530 of 1000\n",
            "epsilon: 0.07018314008827135\n",
            "gamma: 0.99\n",
            "Number of games in episode 530: 63\n",
            "Avg Action Loss, 13 batches: 0.03233914714879715\n",
            "Avg Block Loss, 7 batches: 0.05441515014639923\n",
            "Avg Challenge Loss, 8 batches: 0.060553533025085926\n",
            "Avg Card Loss, 10 batches: 0.03975608255714178\n",
            "0\n",
            "win rate: 0.22\n",
            "episode 531 of 1000\n",
            "epsilon: 0.06983222438783\n",
            "gamma: 0.99\n",
            "Number of games in episode 531: 73\n",
            "Avg Action Loss, 11 batches: 0.02412511251697486\n",
            "Avg Block Loss, 7 batches: 0.07252574605601174\n",
            "Avg Challenge Loss, 8 batches: 0.08531312481500208\n",
            "Avg Card Loss, 11 batches: 0.03417930379509926\n",
            "0\n",
            "episode 532 of 1000\n",
            "epsilon: 0.06948306326589085\n",
            "gamma: 0.99\n",
            "Number of games in episode 532: 74\n",
            "Avg Action Loss, 13 batches: 0.02423934215823045\n",
            "Avg Block Loss, 9 batches: 0.04912278076840772\n",
            "Avg Challenge Loss, 9 batches: 0.054037012366784945\n",
            "Avg Card Loss, 11 batches: 0.05258387377993627\n",
            "0\n",
            "episode 533 of 1000\n",
            "epsilon: 0.0691356479495614\n",
            "gamma: 0.99\n",
            "Number of games in episode 533: 76\n",
            "Avg Action Loss, 13 batches: 0.02657390050948239\n",
            "Avg Block Loss, 9 batches: 0.05857816648979982\n",
            "Avg Challenge Loss, 9 batches: 0.06715625731481446\n",
            "Avg Card Loss, 11 batches: 0.03441328428347002\n",
            "0\n",
            "episode 534 of 1000\n",
            "epsilon: 0.06878996970981359\n",
            "gamma: 0.99\n",
            "Number of games in episode 534: 70\n",
            "Avg Action Loss, 11 batches: 0.034121174534613434\n",
            "Avg Block Loss, 7 batches: 0.061676597222685814\n",
            "Avg Challenge Loss, 8 batches: 0.06477276561781764\n",
            "Avg Card Loss, 11 batches: 0.03998827434737574\n",
            "0\n",
            "episode 535 of 1000\n",
            "epsilon: 0.06844601986126451\n",
            "gamma: 0.99\n",
            "Number of games in episode 535: 73\n",
            "Avg Action Loss, 12 batches: 0.028048959871133167\n",
            "Avg Block Loss, 8 batches: 0.056619311100803316\n",
            "Avg Challenge Loss, 9 batches: 0.05384984270979961\n",
            "Avg Card Loss, 10 batches: 0.03831950849853456\n",
            "0\n",
            "episode 536 of 1000\n",
            "epsilon: 0.06810378976195819\n",
            "gamma: 0.99\n",
            "Number of games in episode 536: 75\n",
            "Avg Action Loss, 12 batches: 0.04433523832509915\n",
            "Avg Block Loss, 8 batches: 0.051958021242171526\n",
            "Avg Challenge Loss, 8 batches: 0.0524108475074172\n",
            "Avg Card Loss, 11 batches: 0.03745010266588493\n",
            "0\n",
            "episode 537 of 1000\n",
            "epsilon: 0.0677632708131484\n",
            "gamma: 0.99\n",
            "Number of games in episode 537: 73\n",
            "Avg Action Loss, 12 batches: 0.050713574048131704\n",
            "Avg Block Loss, 8 batches: 0.04419887438416481\n",
            "Avg Challenge Loss, 9 batches: 0.05250901107986768\n",
            "Avg Card Loss, 11 batches: 0.0354393684220585\n",
            "0\n",
            "episode 538 of 1000\n",
            "epsilon: 0.06742445445908266\n",
            "gamma: 0.99\n",
            "Number of games in episode 538: 77\n",
            "Avg Action Loss, 12 batches: 0.02692208532243967\n",
            "Avg Block Loss, 8 batches: 0.06360536732245237\n",
            "Avg Challenge Loss, 8 batches: 0.07191934366710484\n",
            "Avg Card Loss, 11 batches: 0.05558734446425329\n",
            "0\n",
            "episode 539 of 1000\n",
            "epsilon: 0.06708733218678724\n",
            "gamma: 0.99\n",
            "Number of games in episode 539: 77\n",
            "Avg Action Loss, 12 batches: 0.05313213666280111\n",
            "Avg Block Loss, 8 batches: 0.07019588339608163\n",
            "Avg Challenge Loss, 9 batches: 0.08692398874296083\n",
            "Avg Card Loss, 11 batches: 0.04766555282879959\n",
            "0\n",
            "episode 540 of 1000\n",
            "epsilon: 0.0667518955258533\n",
            "gamma: 0.99\n",
            "Number of games in episode 540: 79\n",
            "Avg Action Loss, 12 batches: 0.03559998298684756\n",
            "Avg Block Loss, 8 batches: 0.057735255220904946\n",
            "Avg Challenge Loss, 9 batches: 0.06307688013960917\n",
            "Avg Card Loss, 12 batches: 0.044047735553855695\n",
            "0\n",
            "win rate: 0.26\n",
            "episode 541 of 1000\n",
            "epsilon: 0.06641813604822402\n",
            "gamma: 0.99\n",
            "Number of games in episode 541: 74\n",
            "Avg Action Loss, 12 batches: 0.027764590495886903\n",
            "Avg Block Loss, 8 batches: 0.04797395470086485\n",
            "Avg Challenge Loss, 8 batches: 0.05354246613569558\n",
            "Avg Card Loss, 11 batches: 0.036127396266568794\n",
            "0\n",
            "episode 542 of 1000\n",
            "epsilon: 0.0660860453679829\n",
            "gamma: 0.99\n",
            "Number of games in episode 542: 78\n",
            "Avg Action Loss, 12 batches: 0.028958465438336134\n",
            "Avg Block Loss, 7 batches: 0.041955798997410705\n",
            "Avg Challenge Loss, 8 batches: 0.042733922018669546\n",
            "Avg Card Loss, 12 batches: 0.045305014665549\n",
            "0\n",
            "episode 543 of 1000\n",
            "epsilon: 0.06575561514114299\n",
            "gamma: 0.99\n",
            "Number of games in episode 543: 78\n",
            "Avg Action Loss, 12 batches: 0.030435349132555228\n",
            "Avg Block Loss, 8 batches: 0.0634354327339679\n",
            "Avg Challenge Loss, 8 batches: 0.06726589892059565\n",
            "Avg Card Loss, 11 batches: 0.031108418394896118\n",
            "0\n",
            "episode 544 of 1000\n",
            "epsilon: 0.06542683706543727\n",
            "gamma: 0.99\n",
            "Number of games in episode 544: 80\n",
            "Avg Action Loss, 12 batches: 0.02935072189817826\n",
            "Avg Block Loss, 8 batches: 0.06871385232079774\n",
            "Avg Challenge Loss, 8 batches: 0.07298182672820985\n",
            "Avg Card Loss, 11 batches: 0.05409967471760782\n",
            "0\n",
            "episode 545 of 1000\n",
            "epsilon: 0.06509970288011008\n",
            "gamma: 0.99\n",
            "Number of games in episode 545: 77\n",
            "Avg Action Loss, 12 batches: 0.03759175321708123\n",
            "Avg Block Loss, 7 batches: 0.06532506844294923\n",
            "Avg Challenge Loss, 8 batches: 0.06368930148892105\n",
            "Avg Card Loss, 11 batches: 0.03996986379338936\n",
            "0\n",
            "episode 546 of 1000\n",
            "epsilon: 0.06477420436570952\n",
            "gamma: 0.99\n",
            "Number of games in episode 546: 81\n",
            "Avg Action Loss, 12 batches: 0.032499284483492374\n",
            "Avg Block Loss, 8 batches: 0.06557073653675616\n",
            "Avg Challenge Loss, 8 batches: 0.0649639330804348\n",
            "Avg Card Loss, 12 batches: 0.04082473608044287\n",
            "0\n",
            "episode 547 of 1000\n",
            "epsilon: 0.06445033334388098\n",
            "gamma: 0.99\n",
            "Number of games in episode 547: 76\n",
            "Avg Action Loss, 12 batches: 0.03718178113922477\n",
            "Avg Block Loss, 8 batches: 0.06471122882794589\n",
            "Avg Challenge Loss, 8 batches: 0.06402853387407959\n",
            "Avg Card Loss, 11 batches: 0.029137802924114196\n",
            "0\n",
            "episode 548 of 1000\n",
            "epsilon: 0.06412808167716157\n",
            "gamma: 0.99\n",
            "Number of games in episode 548: 77\n",
            "Avg Action Loss, 12 batches: 0.030297320646544296\n",
            "Avg Block Loss, 8 batches: 0.08749606332276016\n",
            "Avg Challenge Loss, 9 batches: 0.07651407985637586\n",
            "Avg Card Loss, 11 batches: 0.05292383293536576\n",
            "0\n",
            "episode 549 of 1000\n",
            "epsilon: 0.06380744126877576\n",
            "gamma: 0.99\n",
            "Number of games in episode 549: 72\n",
            "Avg Action Loss, 13 batches: 0.026425207821795575\n",
            "Avg Block Loss, 8 batches: 0.0578372988384217\n",
            "Avg Challenge Loss, 9 batches: 0.053706043089429535\n",
            "Avg Card Loss, 11 batches: 0.05217201499776407\n",
            "0\n",
            "episode 550 of 1000\n",
            "epsilon: 0.06348840406243188\n",
            "gamma: 0.99\n",
            "Number of games in episode 550: 75\n",
            "Avg Action Loss, 12 batches: 0.029932494663322966\n",
            "Avg Block Loss, 7 batches: 0.034181611612439156\n",
            "Avg Challenge Loss, 8 batches: 0.040098547004163265\n",
            "Avg Card Loss, 11 batches: 0.04207371742549268\n",
            "0\n",
            "win rate: 0.16\n",
            "episode 551 of 1000\n",
            "epsilon: 0.06317096204211972\n",
            "gamma: 0.99\n",
            "Number of games in episode 551: 71\n",
            "Avg Action Loss, 11 batches: 0.02665658113123341\n",
            "Avg Block Loss, 7 batches: 0.04728106568966593\n",
            "Avg Challenge Loss, 7 batches: 0.044897577193166525\n",
            "Avg Card Loss, 11 batches: 0.04241882933473045\n",
            "0\n",
            "episode 552 of 1000\n",
            "epsilon: 0.06285510723190912\n",
            "gamma: 0.99\n",
            "Number of games in episode 552: 80\n",
            "Avg Action Loss, 12 batches: 0.02478438848629594\n",
            "Avg Block Loss, 7 batches: 0.06513268420738834\n",
            "Avg Challenge Loss, 8 batches: 0.06371289258822799\n",
            "Avg Card Loss, 12 batches: 0.03890023628870646\n",
            "0\n",
            "episode 553 of 1000\n",
            "epsilon: 0.06254083169574957\n",
            "gamma: 0.99\n",
            "Number of games in episode 553: 84\n",
            "Avg Action Loss, 12 batches: 0.0302738471267124\n",
            "Avg Block Loss, 9 batches: 0.05511925679941972\n",
            "Avg Challenge Loss, 9 batches: 0.05935472705298\n",
            "Avg Card Loss, 12 batches: 0.04090543114580214\n",
            "0\n",
            "episode 554 of 1000\n",
            "epsilon: 0.062228127537270826\n",
            "gamma: 0.99\n",
            "Number of games in episode 554: 73\n",
            "Avg Action Loss, 12 batches: 0.031057453171039622\n",
            "Avg Block Loss, 8 batches: 0.059786275727674365\n",
            "Avg Challenge Loss, 9 batches: 0.05557345857636796\n",
            "Avg Card Loss, 11 batches: 0.04798327166248451\n",
            "0\n",
            "episode 555 of 1000\n",
            "epsilon: 0.06191698689958447\n",
            "gamma: 0.99\n",
            "Number of games in episode 555: 79\n",
            "Avg Action Loss, 12 batches: 0.03309504881811639\n",
            "Avg Block Loss, 8 batches: 0.05460982699878514\n",
            "Avg Challenge Loss, 9 batches: 0.06124009212685956\n",
            "Avg Card Loss, 12 batches: 0.04742230676735441\n",
            "0\n",
            "episode 556 of 1000\n",
            "epsilon: 0.061607401965086545\n",
            "gamma: 0.99\n",
            "Number of games in episode 556: 77\n",
            "Avg Action Loss, 12 batches: 0.02256964612752199\n",
            "Avg Block Loss, 7 batches: 0.05540376515792949\n",
            "Avg Challenge Loss, 8 batches: 0.058942059287801385\n",
            "Avg Card Loss, 12 batches: 0.030098421693158645\n",
            "0\n",
            "episode 557 of 1000\n",
            "epsilon: 0.06129936495526111\n",
            "gamma: 0.99\n",
            "Number of games in episode 557: 80\n",
            "Avg Action Loss, 13 batches: 0.03832827995602901\n",
            "Avg Block Loss, 8 batches: 0.055632977397181094\n",
            "Avg Challenge Loss, 9 batches: 0.06491514057334927\n",
            "Avg Card Loss, 12 batches: 0.05098534553932647\n",
            "0\n",
            "episode 558 of 1000\n",
            "epsilon: 0.0609928681304848\n",
            "gamma: 0.99\n",
            "Number of games in episode 558: 81\n",
            "Avg Action Loss, 13 batches: 0.026930933460020103\n",
            "Avg Block Loss, 9 batches: 0.05666155285305447\n",
            "Avg Challenge Loss, 10 batches: 0.06478313971310853\n",
            "Avg Card Loss, 12 batches: 0.028704373980872333\n",
            "0\n",
            "episode 559 of 1000\n",
            "epsilon: 0.060687903789832374\n",
            "gamma: 0.99\n",
            "Number of games in episode 559: 79\n",
            "Avg Action Loss, 12 batches: 0.03220967102485398\n",
            "Avg Block Loss, 8 batches: 0.05074817454442382\n",
            "Avg Challenge Loss, 8 batches: 0.05602547130547464\n",
            "Avg Card Loss, 12 batches: 0.023793072439730167\n",
            "0\n",
            "episode 560 of 1000\n",
            "epsilon: 0.06038446427088321\n",
            "gamma: 0.99\n",
            "Number of games in episode 560: 70\n",
            "Avg Action Loss, 11 batches: 0.0335483172993091\n",
            "Avg Block Loss, 8 batches: 0.03541736968327314\n",
            "Avg Challenge Loss, 9 batches: 0.048535719617373414\n",
            "Avg Card Loss, 11 batches: 0.040840423073280945\n",
            "0\n",
            "win rate: 0.16\n",
            "episode 561 of 1000\n",
            "epsilon: 0.06008254194952879\n",
            "gamma: 0.99\n",
            "Number of games in episode 561: 72\n",
            "Avg Action Loss, 12 batches: 0.056160313387711845\n",
            "Avg Block Loss, 8 batches: 0.05966081004589796\n",
            "Avg Challenge Loss, 9 batches: 0.05624920460912916\n",
            "Avg Card Loss, 11 batches: 0.028924244434826753\n",
            "0\n",
            "episode 562 of 1000\n",
            "epsilon: 0.05978212923978115\n",
            "gamma: 0.99\n",
            "Number of games in episode 562: 74\n",
            "Avg Action Loss, 12 batches: 0.03255051591744026\n",
            "Avg Block Loss, 8 batches: 0.06696871016174555\n",
            "Avg Challenge Loss, 8 batches: 0.06756799295544624\n",
            "Avg Card Loss, 10 batches: 0.04300228208303451\n",
            "0\n",
            "episode 563 of 1000\n",
            "epsilon: 0.05948321859358224\n",
            "gamma: 0.99\n",
            "Number of games in episode 563: 75\n",
            "Avg Action Loss, 12 batches: 0.03151729765037695\n",
            "Avg Block Loss, 8 batches: 0.06834112713113427\n",
            "Avg Challenge Loss, 9 batches: 0.07154198818736607\n",
            "Avg Card Loss, 11 batches: 0.03710280494256453\n",
            "0\n",
            "episode 564 of 1000\n",
            "epsilon: 0.05918580250061433\n",
            "gamma: 0.99\n",
            "Number of games in episode 564: 72\n",
            "Avg Action Loss, 12 batches: 0.02578334475401789\n",
            "Avg Block Loss, 7 batches: 0.0693947569067989\n",
            "Avg Challenge Loss, 8 batches: 0.06568696885369718\n",
            "Avg Card Loss, 11 batches: 0.039568320170722225\n",
            "0\n",
            "episode 565 of 1000\n",
            "epsilon: 0.058889873488111255\n",
            "gamma: 0.99\n",
            "Number of games in episode 565: 73\n",
            "Avg Action Loss, 12 batches: 0.03057175742772718\n",
            "Avg Block Loss, 8 batches: 0.0824981820769608\n",
            "Avg Challenge Loss, 9 batches: 0.07975673240919907\n",
            "Avg Card Loss, 11 batches: 0.036452344872734764\n",
            "0\n",
            "episode 566 of 1000\n",
            "epsilon: 0.058595424120670696\n",
            "gamma: 0.99\n",
            "Number of games in episode 566: 76\n",
            "Avg Action Loss, 13 batches: 0.040112042656311624\n",
            "Avg Block Loss, 9 batches: 0.0714569768557946\n",
            "Avg Challenge Loss, 11 batches: 0.08230327256023884\n",
            "Avg Card Loss, 10 batches: 0.07299892455339432\n",
            "0\n",
            "episode 567 of 1000\n",
            "epsilon: 0.05830244700006734\n",
            "gamma: 0.99\n",
            "Number of games in episode 567: 70\n",
            "Avg Action Loss, 12 batches: 0.032311189997320376\n",
            "Avg Block Loss, 8 batches: 0.04841388505883515\n",
            "Avg Challenge Loss, 9 batches: 0.049772133119404316\n",
            "Avg Card Loss, 11 batches: 0.042246689003976906\n",
            "0\n",
            "episode 568 of 1000\n",
            "epsilon: 0.058010934765067\n",
            "gamma: 0.99\n",
            "Number of games in episode 568: 75\n",
            "Avg Action Loss, 11 batches: 0.02290580171922391\n",
            "Avg Block Loss, 7 batches: 0.04200573850955282\n",
            "Avg Challenge Loss, 7 batches: 0.04093552912984576\n",
            "Avg Card Loss, 12 batches: 0.028585111974583317\n",
            "0\n",
            "episode 569 of 1000\n",
            "epsilon: 0.05772088009124167\n",
            "gamma: 0.99\n",
            "Number of games in episode 569: 74\n",
            "Avg Action Loss, 12 batches: 0.03233113636573156\n",
            "Avg Block Loss, 8 batches: 0.0556264448678121\n",
            "Avg Challenge Loss, 8 batches: 0.05785776884295046\n",
            "Avg Card Loss, 11 batches: 0.03668280724774708\n",
            "0\n",
            "episode 570 of 1000\n",
            "epsilon: 0.05743227569078546\n",
            "gamma: 0.99\n",
            "Number of games in episode 570: 77\n",
            "Avg Action Loss, 11 batches: 0.023485820422965018\n",
            "Avg Block Loss, 6 batches: 0.05252291518263519\n",
            "Avg Challenge Loss, 6 batches: 0.055969811199853815\n",
            "Avg Card Loss, 12 batches: 0.033466266080116235\n",
            "0\n",
            "win rate: 0.18\n",
            "episode 571 of 1000\n",
            "epsilon: 0.05714511431233153\n",
            "gamma: 0.99\n",
            "Number of games in episode 571: 70\n",
            "Avg Action Loss, 13 batches: 0.024426321785610456\n",
            "Avg Block Loss, 9 batches: 0.07805059705343512\n",
            "Avg Challenge Loss, 9 batches: 0.08639041748311785\n",
            "Avg Card Loss, 11 batches: 0.038163259329104963\n",
            "0\n",
            "episode 572 of 1000\n",
            "epsilon: 0.05685938874076987\n",
            "gamma: 0.99\n",
            "Number of games in episode 572: 71\n",
            "Avg Action Loss, 11 batches: 0.042231166481294415\n",
            "Avg Block Loss, 8 batches: 0.050631254678592086\n",
            "Avg Challenge Loss, 9 batches: 0.06473243153757519\n",
            "Avg Card Loss, 11 batches: 0.036906143608079714\n",
            "0\n",
            "episode 573 of 1000\n",
            "epsilon: 0.056575091797066025\n",
            "gamma: 0.99\n",
            "Number of games in episode 573: 78\n",
            "Avg Action Loss, 12 batches: 0.023352067490729194\n",
            "Avg Block Loss, 7 batches: 0.04134598507412842\n",
            "Avg Challenge Loss, 8 batches: 0.0522315934067592\n",
            "Avg Card Loss, 11 batches: 0.03240258513357152\n",
            "0\n",
            "episode 574 of 1000\n",
            "epsilon: 0.056292216338080694\n",
            "gamma: 0.99\n",
            "Number of games in episode 574: 72\n",
            "Avg Action Loss, 11 batches: 0.021863656499507753\n",
            "Avg Block Loss, 7 batches: 0.05354235600680113\n",
            "Avg Challenge Loss, 8 batches: 0.0532815275946632\n",
            "Avg Card Loss, 11 batches: 0.03229528411545537\n",
            "0\n",
            "episode 575 of 1000\n",
            "epsilon: 0.05601075525639029\n",
            "gamma: 0.99\n",
            "Number of games in episode 575: 77\n",
            "Avg Action Loss, 11 batches: 0.037757622078061104\n",
            "Avg Block Loss, 7 batches: 0.04306086019745895\n",
            "Avg Challenge Loss, 8 batches: 0.05097064422443509\n",
            "Avg Card Loss, 13 batches: 0.031980375902583964\n",
            "0\n",
            "episode 576 of 1000\n",
            "epsilon: 0.05573070148010834\n",
            "gamma: 0.99\n",
            "Number of games in episode 576: 80\n",
            "Avg Action Loss, 12 batches: 0.041402171521137156\n",
            "Avg Block Loss, 8 batches: 0.04667022870853543\n",
            "Avg Challenge Loss, 9 batches: 0.052988221454951495\n",
            "Avg Card Loss, 11 batches: 0.03500602157278494\n",
            "0\n",
            "episode 577 of 1000\n",
            "epsilon: 0.0554520479727078\n",
            "gamma: 0.99\n",
            "Number of games in episode 577: 77\n",
            "Avg Action Loss, 13 batches: 0.02953954216522666\n",
            "Avg Block Loss, 8 batches: 0.04498387186322361\n",
            "Avg Challenge Loss, 8 batches: 0.036036885634530336\n",
            "Avg Card Loss, 12 batches: 0.04552343992205957\n",
            "0\n",
            "episode 578 of 1000\n",
            "epsilon: 0.05517478773284426\n",
            "gamma: 0.99\n",
            "Number of games in episode 578: 75\n",
            "Avg Action Loss, 13 batches: 0.027852523068969067\n",
            "Avg Block Loss, 7 batches: 0.04236492434782641\n",
            "Avg Challenge Loss, 8 batches: 0.047551662544719875\n",
            "Avg Card Loss, 11 batches: 0.04082958688112823\n",
            "0\n",
            "episode 579 of 1000\n",
            "epsilon: 0.05489891379418004\n",
            "gamma: 0.99\n",
            "Number of games in episode 579: 69\n",
            "Avg Action Loss, 12 batches: 0.031341810167456664\n",
            "Avg Block Loss, 7 batches: 0.07670934458396264\n",
            "Avg Challenge Loss, 8 batches: 0.05812331475317478\n",
            "Avg Card Loss, 10 batches: 0.03234679470770061\n",
            "0\n",
            "episode 580 of 1000\n",
            "epsilon: 0.05462441922520914\n",
            "gamma: 0.99\n",
            "Number of games in episode 580: 78\n",
            "Avg Action Loss, 12 batches: 0.03363716253079474\n",
            "Avg Block Loss, 8 batches: 0.07259720261208713\n",
            "Avg Challenge Loss, 9 batches: 0.07194630574021074\n",
            "Avg Card Loss, 11 batches: 0.041458381890234625\n",
            "0\n",
            "win rate: 0.34\n",
            "episode 581 of 1000\n",
            "epsilon: 0.0543512971290831\n",
            "gamma: 0.99\n",
            "Number of games in episode 581: 75\n",
            "Avg Action Loss, 12 batches: 0.04709702699134747\n",
            "Avg Block Loss, 7 batches: 0.0927893781502332\n",
            "Avg Challenge Loss, 8 batches: 0.09158769901841879\n",
            "Avg Card Loss, 11 batches: 0.049889748662032864\n",
            "0\n",
            "episode 582 of 1000\n",
            "epsilon: 0.05407954064343768\n",
            "gamma: 0.99\n",
            "Number of games in episode 582: 73\n",
            "Avg Action Loss, 12 batches: 0.034797074583669506\n",
            "Avg Block Loss, 8 batches: 0.06112336670048535\n",
            "Avg Challenge Loss, 9 batches: 0.0641710754070017\n",
            "Avg Card Loss, 11 batches: 0.04268906883556734\n",
            "0\n",
            "episode 583 of 1000\n",
            "epsilon: 0.05380914294022049\n",
            "gamma: 0.99\n",
            "Number of games in episode 583: 76\n",
            "Avg Action Loss, 12 batches: 0.03900200532128414\n",
            "Avg Block Loss, 8 batches: 0.0643740837695077\n",
            "Avg Challenge Loss, 8 batches: 0.06480452173855156\n",
            "Avg Card Loss, 11 batches: 0.046321924183179035\n",
            "0\n",
            "episode 584 of 1000\n",
            "epsilon: 0.05354009722551939\n",
            "gamma: 0.99\n",
            "Number of games in episode 584: 76\n",
            "Avg Action Loss, 12 batches: 0.03570190460110704\n",
            "Avg Block Loss, 8 batches: 0.04953527927864343\n",
            "Avg Challenge Loss, 9 batches: 0.059701522812247276\n",
            "Avg Card Loss, 12 batches: 0.042163689814818404\n",
            "0\n",
            "episode 585 of 1000\n",
            "epsilon: 0.05327239673939179\n",
            "gamma: 0.99\n",
            "Number of games in episode 585: 72\n",
            "Avg Action Loss, 12 batches: 0.03742629708722234\n",
            "Avg Block Loss, 7 batches: 0.0560909735837153\n",
            "Avg Challenge Loss, 8 batches: 0.05937444022856653\n",
            "Avg Card Loss, 10 batches: 0.03813404571264982\n",
            "0\n",
            "episode 586 of 1000\n",
            "epsilon: 0.053006034755694834\n",
            "gamma: 0.99\n",
            "Number of games in episode 586: 73\n",
            "Avg Action Loss, 12 batches: 0.03190053541523715\n",
            "Avg Block Loss, 8 batches: 0.06670053314883262\n",
            "Avg Challenge Loss, 9 batches: 0.0730642904010084\n",
            "Avg Card Loss, 11 batches: 0.0495926510034637\n",
            "0\n",
            "episode 587 of 1000\n",
            "epsilon: 0.052741004581916356\n",
            "gamma: 0.99\n",
            "Number of games in episode 587: 71\n",
            "Avg Action Loss, 12 batches: 0.031227075882876914\n",
            "Avg Block Loss, 8 batches: 0.0566348860738799\n",
            "Avg Challenge Loss, 9 batches: 0.05683991230196423\n",
            "Avg Card Loss, 10 batches: 0.044840026553720234\n",
            "0\n",
            "episode 588 of 1000\n",
            "epsilon: 0.052477299559006776\n",
            "gamma: 0.99\n",
            "Number of games in episode 588: 69\n",
            "Avg Action Loss, 13 batches: 0.042299653976582564\n",
            "Avg Block Loss, 9 batches: 0.06951652487946881\n",
            "Avg Challenge Loss, 9 batches: 0.07476153110878335\n",
            "Avg Card Loss, 10 batches: 0.04607288064435124\n",
            "0\n",
            "episode 589 of 1000\n",
            "epsilon: 0.052214913061211746\n",
            "gamma: 0.99\n",
            "Number of games in episode 589: 71\n",
            "Avg Action Loss, 12 batches: 0.022546142591939617\n",
            "Avg Block Loss, 8 batches: 0.06945921946316957\n",
            "Avg Challenge Loss, 8 batches: 0.06378933624364436\n",
            "Avg Card Loss, 11 batches: 0.03993684078820727\n",
            "0\n",
            "episode 590 of 1000\n",
            "epsilon: 0.05195383849590569\n",
            "gamma: 0.99\n",
            "Number of games in episode 590: 78\n",
            "Avg Action Loss, 12 batches: 0.025869934470392764\n",
            "Avg Block Loss, 7 batches: 0.051852042387638776\n",
            "Avg Challenge Loss, 8 batches: 0.05136742233298719\n",
            "Avg Card Loss, 12 batches: 0.038558483259597175\n",
            "0\n",
            "win rate: 0.26\n",
            "episode 591 of 1000\n",
            "epsilon: 0.05169406930342616\n",
            "gamma: 0.99\n",
            "Number of games in episode 591: 66\n",
            "Avg Action Loss, 11 batches: 0.03326118940656835\n",
            "Avg Block Loss, 7 batches: 0.061980745888182094\n",
            "Avg Challenge Loss, 8 batches: 0.06097485241480172\n",
            "Avg Card Loss, 10 batches: 0.03669618503190577\n",
            "0\n",
            "episode 592 of 1000\n",
            "epsilon: 0.05143559895690903\n",
            "gamma: 0.99\n",
            "Number of games in episode 592: 70\n",
            "Avg Action Loss, 11 batches: 0.029825293882326645\n",
            "Avg Block Loss, 8 batches: 0.034423340344801545\n",
            "Avg Challenge Loss, 8 batches: 0.042433274327777326\n",
            "Avg Card Loss, 10 batches: 0.034669072134420276\n",
            "0\n",
            "episode 593 of 1000\n",
            "epsilon: 0.051178420962124486\n",
            "gamma: 0.99\n",
            "Number of games in episode 593: 79\n",
            "Avg Action Loss, 12 batches: 0.042336877125004925\n",
            "Avg Block Loss, 7 batches: 0.08483602186398846\n",
            "Avg Challenge Loss, 8 batches: 0.07868440169841051\n",
            "Avg Card Loss, 13 batches: 0.035460313835826054\n",
            "0\n",
            "episode 594 of 1000\n",
            "epsilon: 0.05092252885731386\n",
            "gamma: 0.99\n",
            "Number of games in episode 594: 78\n",
            "Avg Action Loss, 12 batches: 0.04476473371808728\n",
            "Avg Block Loss, 8 batches: 0.06807978800497949\n",
            "Avg Challenge Loss, 9 batches: 0.07720970114072163\n",
            "Avg Card Loss, 11 batches: 0.04921787638555874\n",
            "0\n",
            "episode 595 of 1000\n",
            "epsilon: 0.05066791621302729\n",
            "gamma: 0.99\n",
            "Number of games in episode 595: 74\n",
            "Avg Action Loss, 13 batches: 0.03464447663953671\n",
            "Avg Block Loss, 9 batches: 0.06539757777419355\n",
            "Avg Challenge Loss, 10 batches: 0.07220200151205063\n",
            "Avg Card Loss, 11 batches: 0.042846505106850105\n",
            "0\n",
            "episode 596 of 1000\n",
            "epsilon: 0.05041457663196215\n",
            "gamma: 0.99\n",
            "Number of games in episode 596: 75\n",
            "Avg Action Loss, 12 batches: 0.029915137216448784\n",
            "Avg Block Loss, 7 batches: 0.053731984991048067\n",
            "Avg Challenge Loss, 8 batches: 0.052165327593684196\n",
            "Avg Card Loss, 11 batches: 0.03883040035990151\n",
            "0\n",
            "episode 597 of 1000\n",
            "epsilon: 0.050162503748802344\n",
            "gamma: 0.99\n",
            "Number of games in episode 597: 73\n",
            "Avg Action Loss, 13 batches: 0.02861130194595227\n",
            "Avg Block Loss, 9 batches: 0.05473843692905373\n",
            "Avg Challenge Loss, 9 batches: 0.05696791369054052\n",
            "Avg Card Loss, 10 batches: 0.04637639056891203\n",
            "0\n",
            "episode 598 of 1000\n",
            "epsilon: 0.049911691230058335\n",
            "gamma: 0.99\n",
            "Number of games in episode 598: 74\n",
            "Avg Action Loss, 12 batches: 0.027925276818374794\n",
            "Avg Block Loss, 8 batches: 0.04006792209111154\n",
            "Avg Challenge Loss, 8 batches: 0.04100084805395454\n",
            "Avg Card Loss, 11 batches: 0.03502355981618166\n",
            "0\n",
            "episode 599 of 1000\n",
            "epsilon: 0.04966213277390804\n",
            "gamma: 0.99\n",
            "Number of games in episode 599: 75\n",
            "Avg Action Loss, 12 batches: 0.03790648320379356\n",
            "Avg Block Loss, 8 batches: 0.06345630332361907\n",
            "Avg Challenge Loss, 9 batches: 0.07142179521421592\n",
            "Avg Card Loss, 12 batches: 0.0372804228682071\n",
            "0\n",
            "episode 600 of 1000\n",
            "epsilon: 0.0494138221100385\n",
            "gamma: 0.99\n",
            "Number of games in episode 600: 72\n",
            "Avg Action Loss, 11 batches: 0.04058865847235376\n",
            "Avg Block Loss, 7 batches: 0.046912080741354396\n",
            "Avg Challenge Loss, 8 batches: 0.052214068244211376\n",
            "Avg Card Loss, 11 batches: 0.02841421087611128\n",
            "0\n",
            "win rate: 0.16\n",
            "episode 601 of 1000\n",
            "epsilon: 0.04916675299948831\n",
            "gamma: 0.99\n",
            "Number of games in episode 601: 73\n",
            "Avg Action Loss, 13 batches: 0.02156100830493065\n",
            "Avg Block Loss, 9 batches: 0.059380830782983035\n",
            "Avg Challenge Loss, 9 batches: 0.06031376889182462\n",
            "Avg Card Loss, 11 batches: 0.044923987980424004\n",
            "0\n",
            "episode 602 of 1000\n",
            "epsilon: 0.04892091923449087\n",
            "gamma: 0.99\n",
            "Number of games in episode 602: 75\n",
            "Avg Action Loss, 12 batches: 0.03441327375670274\n",
            "Avg Block Loss, 8 batches: 0.05389517592266202\n",
            "Avg Challenge Loss, 9 batches: 0.062340384556187525\n",
            "Avg Card Loss, 11 batches: 0.026920887472277336\n",
            "0\n",
            "episode 603 of 1000\n",
            "epsilon: 0.04867631463831842\n",
            "gamma: 0.99\n",
            "Number of games in episode 603: 76\n",
            "Avg Action Loss, 12 batches: 0.027181356369207304\n",
            "Avg Block Loss, 7 batches: 0.041302677510040145\n",
            "Avg Challenge Loss, 8 batches: 0.052701556123793125\n",
            "Avg Card Loss, 12 batches: 0.050263592042028904\n",
            "0\n",
            "episode 604 of 1000\n",
            "epsilon: 0.048432933065126825\n",
            "gamma: 0.99\n",
            "Number of games in episode 604: 77\n",
            "Avg Action Loss, 12 batches: 0.033341394426922\n",
            "Avg Block Loss, 8 batches: 0.06478883081581444\n",
            "Avg Challenge Loss, 8 batches: 0.0794242937117815\n",
            "Avg Card Loss, 12 batches: 0.04669135777900616\n",
            "0\n",
            "episode 605 of 1000\n",
            "epsilon: 0.048190768399801194\n",
            "gamma: 0.99\n",
            "Number of games in episode 605: 78\n",
            "Avg Action Loss, 13 batches: 0.03567601625735943\n",
            "Avg Block Loss, 9 batches: 0.054258775172962084\n",
            "Avg Challenge Loss, 10 batches: 0.05949657051824033\n",
            "Avg Card Loss, 11 batches: 0.04044972770762714\n",
            "0\n",
            "episode 606 of 1000\n",
            "epsilon: 0.04794981455780219\n",
            "gamma: 0.99\n",
            "Number of games in episode 606: 68\n",
            "Avg Action Loss, 13 batches: 0.034465183575566\n",
            "Avg Block Loss, 7 batches: 0.04368843876623681\n",
            "Avg Challenge Loss, 8 batches: 0.046613744110800326\n",
            "Avg Card Loss, 10 batches: 0.032811483321711424\n",
            "0\n",
            "episode 607 of 1000\n",
            "epsilon: 0.04771006548501318\n",
            "gamma: 0.99\n",
            "Number of games in episode 607: 73\n",
            "Avg Action Loss, 12 batches: 0.026196827568734687\n",
            "Avg Block Loss, 8 batches: 0.031112486380152404\n",
            "Avg Challenge Loss, 8 batches: 0.03600272838957608\n",
            "Avg Card Loss, 11 batches: 0.05311850111254237\n",
            "0\n",
            "episode 608 of 1000\n",
            "epsilon: 0.047471515157588115\n",
            "gamma: 0.99\n",
            "Number of games in episode 608: 78\n",
            "Avg Action Loss, 12 batches: 0.0296114021136115\n",
            "Avg Block Loss, 8 batches: 0.06783968536183238\n",
            "Avg Challenge Loss, 8 batches: 0.08127967151813209\n",
            "Avg Card Loss, 12 batches: 0.054999224453543626\n",
            "0\n",
            "episode 609 of 1000\n",
            "epsilon: 0.047234157581800176\n",
            "gamma: 0.99\n",
            "Number of games in episode 609: 80\n",
            "Avg Action Loss, 11 batches: 0.02933133528991179\n",
            "Avg Block Loss, 7 batches: 0.07628237935049194\n",
            "Avg Challenge Loss, 8 batches: 0.07863954780623317\n",
            "Avg Card Loss, 12 batches: 0.047995997592806816\n",
            "0\n",
            "episode 610 of 1000\n",
            "epsilon: 0.046997986793891174\n",
            "gamma: 0.99\n",
            "Number of games in episode 610: 75\n",
            "Avg Action Loss, 11 batches: 0.03274492851712487\n",
            "Avg Block Loss, 7 batches: 0.06494400809918131\n",
            "Avg Challenge Loss, 8 batches: 0.07708575786091387\n",
            "Avg Card Loss, 12 batches: 0.03588275035144761\n",
            "0\n",
            "win rate: 0.21\n",
            "episode 611 of 1000\n",
            "epsilon: 0.04676299685992172\n",
            "gamma: 0.99\n",
            "Number of games in episode 611: 76\n",
            "Avg Action Loss, 13 batches: 0.019685505441604897\n",
            "Avg Block Loss, 9 batches: 0.06202378413743443\n",
            "Avg Challenge Loss, 9 batches: 0.07493320148852137\n",
            "Avg Card Loss, 11 batches: 0.05520674179900776\n",
            "0\n",
            "episode 612 of 1000\n",
            "epsilon: 0.04652918187562211\n",
            "gamma: 0.99\n",
            "Number of games in episode 612: 72\n",
            "Avg Action Loss, 13 batches: 0.026801861822605133\n",
            "Avg Block Loss, 9 batches: 0.0695917776061429\n",
            "Avg Challenge Loss, 9 batches: 0.0753259581203262\n",
            "Avg Card Loss, 11 batches: 0.051284860650246795\n",
            "0\n",
            "episode 613 of 1000\n",
            "epsilon: 0.046296535966244\n",
            "gamma: 0.99\n",
            "Number of games in episode 613: 83\n",
            "Avg Action Loss, 12 batches: 0.035110286669805646\n",
            "Avg Block Loss, 8 batches: 0.06324538169428706\n",
            "Avg Challenge Loss, 8 batches: 0.06931442557834089\n",
            "Avg Card Loss, 12 batches: 0.03744703011276821\n",
            "0\n",
            "episode 614 of 1000\n",
            "epsilon: 0.046065053286412784\n",
            "gamma: 0.99\n",
            "Number of games in episode 614: 75\n",
            "Avg Action Loss, 13 batches: 0.025731865459909804\n",
            "Avg Block Loss, 8 batches: 0.06083924951963127\n",
            "Avg Challenge Loss, 9 batches: 0.06473022657963964\n",
            "Avg Card Loss, 11 batches: 0.045821393840014935\n",
            "0\n",
            "episode 615 of 1000\n",
            "epsilon: 0.04583472801998072\n",
            "gamma: 0.99\n",
            "Number of games in episode 615: 75\n",
            "Avg Action Loss, 11 batches: 0.030099907466633755\n",
            "Avg Block Loss, 7 batches: 0.052736099543316026\n",
            "Avg Challenge Loss, 8 batches: 0.06344109470956028\n",
            "Avg Card Loss, 12 batches: 0.04551839455962181\n",
            "0\n",
            "episode 616 of 1000\n",
            "epsilon: 0.045605554379880814\n",
            "gamma: 0.99\n",
            "Number of games in episode 616: 76\n",
            "Avg Action Loss, 12 batches: 0.023520877623620134\n",
            "Avg Block Loss, 8 batches: 0.04880496533587575\n",
            "Avg Challenge Loss, 8 batches: 0.05341777880676091\n",
            "Avg Card Loss, 11 batches: 0.03438006142492999\n",
            "0\n",
            "episode 617 of 1000\n",
            "epsilon: 0.04537752660798141\n",
            "gamma: 0.99\n",
            "Number of games in episode 617: 70\n",
            "Avg Action Loss, 12 batches: 0.030664486810564995\n",
            "Avg Block Loss, 7 batches: 0.06408328862328615\n",
            "Avg Challenge Loss, 8 batches: 0.07280650921165943\n",
            "Avg Card Loss, 10 batches: 0.045660897297784685\n",
            "0\n",
            "episode 618 of 1000\n",
            "epsilon: 0.0451506389749415\n",
            "gamma: 0.99\n",
            "Number of games in episode 618: 75\n",
            "Avg Action Loss, 13 batches: 0.02437826503927891\n",
            "Avg Block Loss, 8 batches: 0.0724912688601762\n",
            "Avg Challenge Loss, 8 batches: 0.07469832454808056\n",
            "Avg Card Loss, 12 batches: 0.05031432973919436\n",
            "0\n",
            "episode 619 of 1000\n",
            "epsilon: 0.044924885780066794\n",
            "gamma: 0.99\n",
            "Number of games in episode 619: 75\n",
            "Avg Action Loss, 13 batches: 0.042543682627953015\n",
            "Avg Block Loss, 8 batches: 0.0410704156383872\n",
            "Avg Challenge Loss, 9 batches: 0.05882754766692718\n",
            "Avg Card Loss, 11 batches: 0.045681715942919254\n",
            "0\n",
            "episode 620 of 1000\n",
            "epsilon: 0.04470026135116646\n",
            "gamma: 0.99\n",
            "Number of games in episode 620: 76\n",
            "Avg Action Loss, 12 batches: 0.03596040753958126\n",
            "Avg Block Loss, 7 batches: 0.06468280777335167\n",
            "Avg Challenge Loss, 8 batches: 0.06375184003263712\n",
            "Avg Card Loss, 12 batches: 0.05195038408661882\n",
            "0\n",
            "win rate: 0.29\n",
            "episode 621 of 1000\n",
            "epsilon: 0.04447676004441063\n",
            "gamma: 0.99\n",
            "Number of games in episode 621: 77\n",
            "Avg Action Loss, 12 batches: 0.03705725011726221\n",
            "Avg Block Loss, 8 batches: 0.056984701834153384\n",
            "Avg Challenge Loss, 9 batches: 0.05482480188624726\n",
            "Avg Card Loss, 12 batches: 0.03140966757200658\n",
            "0\n",
            "episode 622 of 1000\n",
            "epsilon: 0.04425437624418858\n",
            "gamma: 0.99\n",
            "Number of games in episode 622: 69\n",
            "Avg Action Loss, 12 batches: 0.02580597302100311\n",
            "Avg Block Loss, 7 batches: 0.07075631538672107\n",
            "Avg Challenge Loss, 8 batches: 0.08735368493944407\n",
            "Avg Card Loss, 11 batches: 0.04098753410984169\n",
            "0\n",
            "episode 623 of 1000\n",
            "epsilon: 0.04403310436296763\n",
            "gamma: 0.99\n",
            "Number of games in episode 623: 78\n",
            "Avg Action Loss, 12 batches: 0.028358601965010166\n",
            "Avg Block Loss, 8 batches: 0.0703679071739316\n",
            "Avg Challenge Loss, 9 batches: 0.08196660100171964\n",
            "Avg Card Loss, 12 batches: 0.036824941906767585\n",
            "0\n",
            "episode 624 of 1000\n",
            "epsilon: 0.043812938841152796\n",
            "gamma: 0.99\n",
            "Number of games in episode 624: 77\n",
            "Avg Action Loss, 12 batches: 0.025014361327824492\n",
            "Avg Block Loss, 8 batches: 0.05323452432639897\n",
            "Avg Challenge Loss, 9 batches: 0.046969764245053135\n",
            "Avg Card Loss, 13 batches: 0.041281443232527144\n",
            "0\n",
            "episode 625 of 1000\n",
            "epsilon: 0.04359387414694703\n",
            "gamma: 0.99\n",
            "Number of games in episode 625: 76\n",
            "Avg Action Loss, 13 batches: 0.04022547882050276\n",
            "Avg Block Loss, 8 batches: 0.07778744900133461\n",
            "Avg Challenge Loss, 9 batches: 0.08250622513393562\n",
            "Avg Card Loss, 11 batches: 0.048939603719521656\n",
            "0\n",
            "episode 626 of 1000\n",
            "epsilon: 0.043375904776212296\n",
            "gamma: 0.99\n",
            "Number of games in episode 626: 72\n",
            "Avg Action Loss, 12 batches: 0.023350200382992625\n",
            "Avg Block Loss, 7 batches: 0.05727356565850122\n",
            "Avg Challenge Loss, 7 batches: 0.06171528609203441\n",
            "Avg Card Loss, 11 batches: 0.04647572761909528\n",
            "0\n",
            "episode 627 of 1000\n",
            "epsilon: 0.043159025252331236\n",
            "gamma: 0.99\n",
            "Number of games in episode 627: 79\n",
            "Avg Action Loss, 11 batches: 0.03763823054561561\n",
            "Avg Block Loss, 8 batches: 0.067668137839064\n",
            "Avg Challenge Loss, 8 batches: 0.0750600544270128\n",
            "Avg Card Loss, 12 batches: 0.04808368952944875\n",
            "0\n",
            "episode 628 of 1000\n",
            "epsilon: 0.04294323012606958\n",
            "gamma: 0.99\n",
            "Number of games in episode 628: 76\n",
            "Avg Action Loss, 12 batches: 0.03371393385653695\n",
            "Avg Block Loss, 8 batches: 0.06103023746982217\n",
            "Avg Challenge Loss, 9 batches: 0.06819278601970938\n",
            "Avg Card Loss, 10 batches: 0.04158322401344776\n",
            "0\n",
            "episode 629 of 1000\n",
            "epsilon: 0.04272851397543923\n",
            "gamma: 0.99\n",
            "Number of games in episode 629: 75\n",
            "Avg Action Loss, 12 batches: 0.03164746635593474\n",
            "Avg Block Loss, 7 batches: 0.044334481337240765\n",
            "Avg Challenge Loss, 8 batches: 0.0394717255840078\n",
            "Avg Card Loss, 11 batches: 0.030707406007092108\n",
            "0\n",
            "episode 630 of 1000\n",
            "epsilon: 0.04251487140556204\n",
            "gamma: 0.99\n",
            "Number of games in episode 630: 73\n",
            "Avg Action Loss, 13 batches: 0.029619219712913036\n",
            "Avg Block Loss, 9 batches: 0.054725963725811906\n",
            "Avg Challenge Loss, 10 batches: 0.06467574285343289\n",
            "Avg Card Loss, 11 batches: 0.03947838582098484\n",
            "0\n",
            "win rate: 0.21\n",
            "episode 631 of 1000\n",
            "epsilon: 0.04230229704853423\n",
            "gamma: 0.99\n",
            "Number of games in episode 631: 79\n",
            "Avg Action Loss, 12 batches: 0.0368195252182583\n",
            "Avg Block Loss, 8 batches: 0.04793281457386911\n",
            "Avg Challenge Loss, 8 batches: 0.05014777975156903\n",
            "Avg Card Loss, 13 batches: 0.04611817174232923\n",
            "0\n",
            "episode 632 of 1000\n",
            "epsilon: 0.04209078556329156\n",
            "gamma: 0.99\n",
            "Number of games in episode 632: 77\n",
            "Avg Action Loss, 11 batches: 0.03458264436234127\n",
            "Avg Block Loss, 7 batches: 0.07437372713216714\n",
            "Avg Challenge Loss, 8 batches: 0.06632409221492708\n",
            "Avg Card Loss, 12 batches: 0.034121877183982484\n",
            "0\n",
            "episode 633 of 1000\n",
            "epsilon: 0.0418803316354751\n",
            "gamma: 0.99\n",
            "Number of games in episode 633: 80\n",
            "Avg Action Loss, 13 batches: 0.026056530383917\n",
            "Avg Block Loss, 9 batches: 0.07038083486258984\n",
            "Avg Challenge Loss, 9 batches: 0.07151183517028888\n",
            "Avg Card Loss, 13 batches: 0.03688903749347306\n",
            "0\n",
            "episode 634 of 1000\n",
            "epsilon: 0.041670929977297724\n",
            "gamma: 0.99\n",
            "Number of games in episode 634: 74\n",
            "Avg Action Loss, 12 batches: 0.02847167639993131\n",
            "Avg Block Loss, 7 batches: 0.07724078159247126\n",
            "Avg Challenge Loss, 8 batches: 0.07114074367564172\n",
            "Avg Card Loss, 11 batches: 0.025185880759222942\n",
            "0\n",
            "episode 635 of 1000\n",
            "epsilon: 0.04146257532741124\n",
            "gamma: 0.99\n",
            "Number of games in episode 635: 74\n",
            "Avg Action Loss, 12 batches: 0.034061952804525696\n",
            "Avg Block Loss, 8 batches: 0.06822444009594619\n",
            "Avg Challenge Loss, 8 batches: 0.058710911544039845\n",
            "Avg Card Loss, 11 batches: 0.03877103455703367\n",
            "0\n",
            "episode 636 of 1000\n",
            "epsilon: 0.04125526245077418\n",
            "gamma: 0.99\n",
            "Number of games in episode 636: 68\n",
            "Avg Action Loss, 12 batches: 0.03920307549803207\n",
            "Avg Block Loss, 8 batches: 0.03265617007855326\n",
            "Avg Challenge Loss, 9 batches: 0.04454663613190254\n",
            "Avg Card Loss, 10 batches: 0.04080710732378066\n",
            "0\n",
            "episode 637 of 1000\n",
            "epsilon: 0.04104898613852031\n",
            "gamma: 0.99\n",
            "Number of games in episode 637: 75\n",
            "Avg Action Loss, 11 batches: 0.03419058617543091\n",
            "Avg Block Loss, 8 batches: 0.05902764084748924\n",
            "Avg Challenge Loss, 8 batches: 0.05975938471965492\n",
            "Avg Card Loss, 11 batches: 0.029973203824325043\n",
            "0\n",
            "episode 638 of 1000\n",
            "epsilon: 0.04084374120782771\n",
            "gamma: 0.99\n",
            "Number of games in episode 638: 79\n",
            "Avg Action Loss, 12 batches: 0.045859480664754905\n",
            "Avg Block Loss, 9 batches: 0.058512559057109885\n",
            "Avg Challenge Loss, 9 batches: 0.05570624996390608\n",
            "Avg Card Loss, 11 batches: 0.04506629104302688\n",
            "0\n",
            "episode 639 of 1000\n",
            "epsilon: 0.04063952250178857\n",
            "gamma: 0.99\n",
            "Number of games in episode 639: 70\n",
            "Avg Action Loss, 11 batches: 0.03274049288169904\n",
            "Avg Block Loss, 7 batches: 0.04375349783471653\n",
            "Avg Challenge Loss, 7 batches: 0.04359529433505876\n",
            "Avg Card Loss, 11 batches: 0.041088307445699516\n",
            "0\n",
            "episode 640 of 1000\n",
            "epsilon: 0.04043632488927963\n",
            "gamma: 0.99\n",
            "Number of games in episode 640: 77\n",
            "Avg Action Loss, 12 batches: 0.02624258662884434\n",
            "Avg Block Loss, 8 batches: 0.04350837133824825\n",
            "Avg Challenge Loss, 8 batches: 0.041733358753845096\n",
            "Avg Card Loss, 11 batches: 0.021243201928551902\n",
            "0\n",
            "win rate: 0.22\n",
            "episode 641 of 1000\n",
            "epsilon: 0.04023414326483323\n",
            "gamma: 0.99\n",
            "Number of games in episode 641: 75\n",
            "Avg Action Loss, 12 batches: 0.0426297301116089\n",
            "Avg Block Loss, 8 batches: 0.06774442712776363\n",
            "Avg Challenge Loss, 9 batches: 0.0634976430899567\n",
            "Avg Card Loss, 12 batches: 0.04490263465171059\n",
            "0\n",
            "episode 642 of 1000\n",
            "epsilon: 0.040032972548509065\n",
            "gamma: 0.99\n",
            "Number of games in episode 642: 80\n",
            "Avg Action Loss, 11 batches: 0.03662044487216256\n",
            "Avg Block Loss, 7 batches: 0.0598750143711056\n",
            "Avg Challenge Loss, 8 batches: 0.061923593282699585\n",
            "Avg Card Loss, 12 batches: 0.045094057374323405\n",
            "0\n",
            "episode 643 of 1000\n",
            "epsilon: 0.03983280768576652\n",
            "gamma: 0.99\n",
            "Number of games in episode 643: 79\n",
            "Avg Action Loss, 12 batches: 0.02899444483531018\n",
            "Avg Block Loss, 9 batches: 0.06495480570528242\n",
            "Avg Challenge Loss, 9 batches: 0.06770260300901201\n",
            "Avg Card Loss, 12 batches: 0.04570259557416042\n",
            "0\n",
            "episode 644 of 1000\n",
            "epsilon: 0.03963364364733769\n",
            "gamma: 0.99\n",
            "Number of games in episode 644: 69\n",
            "Avg Action Loss, 13 batches: 0.034236134603046454\n",
            "Avg Block Loss, 9 batches: 0.06409247478263246\n",
            "Avg Challenge Loss, 10 batches: 0.07463773377239705\n",
            "Avg Card Loss, 10 batches: 0.05229066293686628\n",
            "0\n",
            "episode 645 of 1000\n",
            "epsilon: 0.039435475429100995\n",
            "gamma: 0.99\n",
            "Number of games in episode 645: 68\n",
            "Avg Action Loss, 12 batches: 0.038472280371934175\n",
            "Avg Block Loss, 8 batches: 0.08533940825145692\n",
            "Avg Challenge Loss, 9 batches: 0.08020882712056239\n",
            "Avg Card Loss, 11 batches: 0.048651512881571594\n",
            "0\n",
            "episode 646 of 1000\n",
            "epsilon: 0.03923829805195549\n",
            "gamma: 0.99\n",
            "Number of games in episode 646: 78\n",
            "Avg Action Loss, 11 batches: 0.02705991149625995\n",
            "Avg Block Loss, 7 batches: 0.07457811491830009\n",
            "Avg Challenge Loss, 8 batches: 0.07344435201957822\n",
            "Avg Card Loss, 11 batches: 0.03871685258028182\n",
            "0\n",
            "episode 647 of 1000\n",
            "epsilon: 0.03904210656169572\n",
            "gamma: 0.99\n",
            "Number of games in episode 647: 74\n",
            "Avg Action Loss, 12 batches: 0.033236890099942684\n",
            "Avg Block Loss, 9 batches: 0.05351739852792687\n",
            "Avg Challenge Loss, 9 batches: 0.05550321750342846\n",
            "Avg Card Loss, 11 batches: 0.03639501231637868\n",
            "0\n",
            "episode 648 of 1000\n",
            "epsilon: 0.03884689602888724\n",
            "gamma: 0.99\n",
            "Number of games in episode 648: 74\n",
            "Avg Action Loss, 12 batches: 0.03695470474970838\n",
            "Avg Block Loss, 7 batches: 0.06750067589538437\n",
            "Avg Challenge Loss, 7 batches: 0.07586583362093993\n",
            "Avg Card Loss, 11 batches: 0.03450836825438521\n",
            "0\n",
            "episode 649 of 1000\n",
            "epsilon: 0.0386526615487428\n",
            "gamma: 0.99\n",
            "Number of games in episode 649: 77\n",
            "Avg Action Loss, 12 batches: 0.04004628870946666\n",
            "Avg Block Loss, 7 batches: 0.06322347119982753\n",
            "Avg Challenge Loss, 8 batches: 0.06556419609114528\n",
            "Avg Card Loss, 11 batches: 0.039013613100078975\n",
            "0\n",
            "episode 650 of 1000\n",
            "epsilon: 0.03845939824099909\n",
            "gamma: 0.99\n",
            "Number of games in episode 650: 76\n",
            "Avg Action Loss, 11 batches: 0.022766664624214172\n",
            "Avg Block Loss, 8 batches: 0.06555562338326126\n",
            "Avg Challenge Loss, 8 batches: 0.06446720869280398\n",
            "Avg Card Loss, 11 batches: 0.03950701133263382\n",
            "0\n",
            "win rate: 0.23\n",
            "episode 651 of 1000\n",
            "epsilon: 0.03826710124979409\n",
            "gamma: 0.99\n",
            "Number of games in episode 651: 71\n",
            "Avg Action Loss, 11 batches: 0.01932948006486351\n",
            "Avg Block Loss, 7 batches: 0.05465571555708136\n",
            "Avg Challenge Loss, 7 batches: 0.07053004471319062\n",
            "Avg Card Loss, 11 batches: 0.02756315791471438\n",
            "0\n",
            "episode 652 of 1000\n",
            "epsilon: 0.038075765743545126\n",
            "gamma: 0.99\n",
            "Number of games in episode 652: 72\n",
            "Avg Action Loss, 12 batches: 0.033907442508886255\n",
            "Avg Block Loss, 8 batches: 0.059048500610515475\n",
            "Avg Challenge Loss, 8 batches: 0.0637644228991121\n",
            "Avg Card Loss, 11 batches: 0.03430448108437387\n",
            "0\n",
            "episode 653 of 1000\n",
            "epsilon: 0.0378853869148274\n",
            "gamma: 0.99\n",
            "Number of games in episode 653: 71\n",
            "Avg Action Loss, 12 batches: 0.01886393562502538\n",
            "Avg Block Loss, 7 batches: 0.04567234630563429\n",
            "Avg Challenge Loss, 8 batches: 0.041223590611480176\n",
            "Avg Card Loss, 11 batches: 0.039171986959197304\n",
            "0\n",
            "episode 654 of 1000\n",
            "epsilon: 0.03769595998025326\n",
            "gamma: 0.99\n",
            "Number of games in episode 654: 84\n",
            "Avg Action Loss, 12 batches: 0.028861785152306158\n",
            "Avg Block Loss, 8 batches: 0.043662849930115044\n",
            "Avg Challenge Loss, 9 batches: 0.04516958931667937\n",
            "Avg Card Loss, 13 batches: 0.032058850987456165\n",
            "0\n",
            "episode 655 of 1000\n",
            "epsilon: 0.03750748018035199\n",
            "gamma: 0.99\n",
            "Number of games in episode 655: 72\n",
            "Avg Action Loss, 12 batches: 0.029938748572021723\n",
            "Avg Block Loss, 9 batches: 0.057240498252213\n",
            "Avg Challenge Loss, 9 batches: 0.060799555025166936\n",
            "Avg Card Loss, 11 batches: 0.038768879222598945\n",
            "0\n",
            "episode 656 of 1000\n",
            "epsilon: 0.037319942779450235\n",
            "gamma: 0.99\n",
            "Number of games in episode 656: 75\n",
            "Avg Action Loss, 13 batches: 0.027875478594348982\n",
            "Avg Block Loss, 8 batches: 0.05391214322298765\n",
            "Avg Challenge Loss, 8 batches: 0.0558057347079739\n",
            "Avg Card Loss, 11 batches: 0.03554852711121467\n",
            "0\n",
            "episode 657 of 1000\n",
            "epsilon: 0.037133343065552986\n",
            "gamma: 0.99\n",
            "Number of games in episode 657: 78\n",
            "Avg Action Loss, 12 batches: 0.0325853219255805\n",
            "Avg Block Loss, 7 batches: 0.05309535376727581\n",
            "Avg Challenge Loss, 7 batches: 0.06118991758142199\n",
            "Avg Card Loss, 12 batches: 0.028865254658740014\n",
            "0\n",
            "episode 658 of 1000\n",
            "epsilon: 0.03694767635022522\n",
            "gamma: 0.99\n",
            "Number of games in episode 658: 79\n",
            "Avg Action Loss, 11 batches: 0.02558106641200456\n",
            "Avg Block Loss, 8 batches: 0.046834806795232\n",
            "Avg Challenge Loss, 8 batches: 0.04971604305319488\n",
            "Avg Card Loss, 13 batches: 0.036982789456557766\n",
            "0\n",
            "episode 659 of 1000\n",
            "epsilon: 0.036762937968474095\n",
            "gamma: 0.99\n",
            "Number of games in episode 659: 78\n",
            "Avg Action Loss, 11 batches: 0.03587136045098305\n",
            "Avg Block Loss, 8 batches: 0.05695956526324153\n",
            "Avg Challenge Loss, 9 batches: 0.06585491510728995\n",
            "Avg Card Loss, 11 batches: 0.0347500916316428\n",
            "0\n",
            "episode 660 of 1000\n",
            "epsilon: 0.03657912327863173\n",
            "gamma: 0.99\n",
            "Number of games in episode 660: 74\n",
            "Avg Action Loss, 13 batches: 0.03617596017340055\n",
            "Avg Block Loss, 7 batches: 0.047468697519174645\n",
            "Avg Challenge Loss, 8 batches: 0.06089955335482955\n",
            "Avg Card Loss, 11 batches: 0.037880631925707516\n",
            "0\n",
            "win rate: 0.24\n",
            "episode 661 of 1000\n",
            "epsilon: 0.036396227662238566\n",
            "gamma: 0.99\n",
            "Number of games in episode 661: 71\n",
            "Avg Action Loss, 11 batches: 0.04597628048875115\n",
            "Avg Block Loss, 7 batches: 0.05849354275103126\n",
            "Avg Challenge Loss, 7 batches: 0.06324394765709128\n",
            "Avg Card Loss, 11 batches: 0.0321799512444572\n",
            "0\n",
            "episode 662 of 1000\n",
            "epsilon: 0.03621424652392737\n",
            "gamma: 0.99\n",
            "Number of games in episode 662: 75\n",
            "Avg Action Loss, 12 batches: 0.038380027981474996\n",
            "Avg Block Loss, 8 batches: 0.0842329072766006\n",
            "Avg Challenge Loss, 8 batches: 0.07624916336499155\n",
            "Avg Card Loss, 11 batches: 0.04872447032142769\n",
            "0\n",
            "episode 663 of 1000\n",
            "epsilon: 0.036033175291307735\n",
            "gamma: 0.99\n",
            "Number of games in episode 663: 73\n",
            "Avg Action Loss, 13 batches: 0.029741230707329053\n",
            "Avg Block Loss, 8 batches: 0.07793552929069847\n",
            "Avg Challenge Loss, 9 batches: 0.078963628763126\n",
            "Avg Card Loss, 11 batches: 0.050421756649897856\n",
            "0\n",
            "episode 664 of 1000\n",
            "epsilon: 0.03585300941485119\n",
            "gamma: 0.99\n",
            "Number of games in episode 664: 79\n",
            "Avg Action Loss, 12 batches: 0.04267926964287957\n",
            "Avg Block Loss, 8 batches: 0.07624448835849762\n",
            "Avg Challenge Loss, 9 batches: 0.08608494378212425\n",
            "Avg Card Loss, 12 batches: 0.038633908766011395\n",
            "0\n",
            "episode 665 of 1000\n",
            "epsilon: 0.035673744367776934\n",
            "gamma: 0.99\n",
            "Number of games in episode 665: 72\n",
            "Avg Action Loss, 12 batches: 0.02580397141476472\n",
            "Avg Block Loss, 7 batches: 0.05387601416025843\n",
            "Avg Challenge Loss, 8 batches: 0.05542510561645031\n",
            "Avg Card Loss, 10 batches: 0.04858870916068554\n",
            "0\n",
            "episode 666 of 1000\n",
            "epsilon: 0.03549537564593805\n",
            "gamma: 0.99\n",
            "Number of games in episode 666: 71\n",
            "Avg Action Loss, 12 batches: 0.03595094149932265\n",
            "Avg Block Loss, 8 batches: 0.06333722360432148\n",
            "Avg Challenge Loss, 9 batches: 0.06888077408075333\n",
            "Avg Card Loss, 10 batches: 0.03779682964086532\n",
            "0\n",
            "episode 667 of 1000\n",
            "epsilon: 0.035317898767708356\n",
            "gamma: 0.99\n",
            "Number of games in episode 667: 77\n",
            "Avg Action Loss, 12 batches: 0.0331894886524727\n",
            "Avg Block Loss, 7 batches: 0.053777831473520825\n",
            "Avg Challenge Loss, 7 batches: 0.056797436837639124\n",
            "Avg Card Loss, 13 batches: 0.043912126002116844\n",
            "0\n",
            "episode 668 of 1000\n",
            "epsilon: 0.03514130927386981\n",
            "gamma: 0.99\n",
            "Number of games in episode 668: 75\n",
            "Avg Action Loss, 12 batches: 0.0307518367189914\n",
            "Avg Block Loss, 8 batches: 0.05518738809041679\n",
            "Avg Challenge Loss, 9 batches: 0.05627297817005052\n",
            "Avg Card Loss, 11 batches: 0.04654240159487182\n",
            "0\n",
            "episode 669 of 1000\n",
            "epsilon: 0.03496560272750046\n",
            "gamma: 0.99\n",
            "Number of games in episode 669: 76\n",
            "Avg Action Loss, 12 batches: 0.03198835393413901\n",
            "Avg Block Loss, 9 batches: 0.062456978381507926\n",
            "Avg Challenge Loss, 10 batches: 0.06792320962995291\n",
            "Avg Card Loss, 11 batches: 0.048750503666021607\n",
            "0\n",
            "episode 670 of 1000\n",
            "epsilon: 0.03479077471386296\n",
            "gamma: 0.99\n",
            "Number of games in episode 670: 69\n",
            "Avg Action Loss, 11 batches: 0.028065531768582085\n",
            "Avg Block Loss, 7 batches: 0.037874236702919006\n",
            "Avg Challenge Loss, 8 batches: 0.045982904033735394\n",
            "Avg Card Loss, 10 batches: 0.043036048859357835\n",
            "0\n",
            "win rate: 0.2\n",
            "episode 671 of 1000\n",
            "epsilon: 0.03461682084029365\n",
            "gamma: 0.99\n",
            "Number of games in episode 671: 73\n",
            "Avg Action Loss, 12 batches: 0.03875083957488338\n",
            "Avg Block Loss, 8 batches: 0.06524448445998132\n",
            "Avg Challenge Loss, 8 batches: 0.06743576494045556\n",
            "Avg Card Loss, 10 batches: 0.04779784046113491\n",
            "0\n",
            "episode 672 of 1000\n",
            "epsilon: 0.034443736736092176\n",
            "gamma: 0.99\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-60796da87302>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mdiscard_pile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macting_player\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreacting_player\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_player\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_game\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreaction_game\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchallenge_game\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcard_game\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoin_game\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchallenge_direction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcard_chosen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbots_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame_loop_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfluences_reverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mnum_games\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-84e50be44512>\u001b[0m in \u001b[0;36mgame_loop_random\u001b[0;34m(bots, actions, influences_reverse, epsilon)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mbots_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbots\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m26\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/copy.py\u001b[0m in \u001b[0;36m_deepcopy_list\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0mappend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__deepcopy__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mreductor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatch_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/parameter.py\u001b[0m in \u001b[0;36m__deepcopy__\u001b[0;34m(self, memo)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             result = type(self)(\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             )\n\u001b[1;32m     70\u001b[0m             \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from os import stat\n",
        "from collections import defaultdict\n",
        "num_episodes = 1000\n",
        "max_steps_per_episode = 200\n",
        "epsilon = 1.0\n",
        "list_division = 4\n",
        "gamma = 0.99\n",
        "\n",
        "bot = copy.deepcopy(bots[0])\n",
        "\n",
        "avg_losses_action = []\n",
        "avg_losses_block = []\n",
        "avg_losses_challenge = []\n",
        "avg_losses_card = []\n",
        "\n",
        "# bots.remove(bots[-1])\n",
        "# bots.remove(bots[-1])\n",
        "\n",
        "win_rates = []\n",
        "avg_game_lengths = []\n",
        "\n",
        "data_fraction = 1/5\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "\n",
        "  replay_buffer_actions = []\n",
        "  replay_buffer_blocks = []\n",
        "  replay_buffer_challenges = []\n",
        "  replay_buffer_cards = []\n",
        "\n",
        "  print(f'episode {episode} of 1000')\n",
        "  print(f'epsilon: {epsilon}')\n",
        "  print(f'gamma: {gamma}')\n",
        "\n",
        "  # discard_piles, acting_players, reacting_players, current_players, actions_game, reactions_game, challenges_game, cards_game, coins_game, challenges_direction, done, rewards, cards_chosen, bots_copy = game_loop_random(bots, actions, influences_reverse, epsilon)\n",
        "  # bots = bots_copy\n",
        "  state = torch.empty((0, 12), dtype=torch.float32)  # Assume state_size = 25 for action network\n",
        "  # states_action = torch.empty((0, 24), dtype=torch.float32)  # Assume state_size = 25 for action network\n",
        "  # next_states_action = torch.empty((0, 24), dtype=torch.float32)\n",
        "  actions_main = torch.empty((0,), dtype=torch.int64)\n",
        "  # states_block = torch.empty((0, 23), dtype=torch.float32)  # Assume state_size = 24 for block network\n",
        "  # next_states_block = torch.empty((0, 23), dtype=torch.float32)\n",
        "  actions_block = torch.empty((0,), dtype=torch.int64)\n",
        "  # states_challenge = torch.empty((0, 24), dtype=torch.float32)  # Assume state_size = 25 for challenge network\n",
        "  # next_states_challenge = torch.empty((0, 24), dtype=torch.float32)\n",
        "  actions_challenge = torch.empty((0,), dtype=torch.int64)\n",
        "  # states_card = torch.empty((0, 19), dtype=torch.float32)  # Assume state_size = 20 for card network\n",
        "  # next_states_card = torch.empty((0, 19), dtype=torch.float32)\n",
        "  actions_card = torch.empty((0,), dtype=torch.int64)\n",
        "  rewards = torch.empty((0,), dtype=torch.float32)\n",
        "  done = torch.empty((0,), dtype=torch.float32)\n",
        "  game_length_sum = 0\n",
        "  all_discard_piles = []\n",
        "  acting_players = []\n",
        "  reacting_players = []\n",
        "  reactions_game = []\n",
        "\n",
        "  num_games = 0\n",
        "\n",
        "\n",
        "  while len(state) <= 50 * batch_size:\n",
        "\n",
        "    discard_pile, acting_player, reacting_player, current_player, action_game, reaction_game, challenge_game, card_game, coin_game, challenge_direction, done_0, reward, card_chosen, bots_copy = game_loop_random(bots, actions, influences_reverse, epsilon)\n",
        "\n",
        "    num_games += 1\n",
        "    # print(f'Game Number {num_games}')\n",
        "\n",
        "    # if random.random():\n",
        "    #   print(reacting_player)\n",
        "\n",
        "    # start_index = int(3 * len(acting_player) / list_division)\n",
        "\n",
        "    bots = bots_copy\n",
        "\n",
        "    game_length_sum += len(acting_player)\n",
        "\n",
        "    # split_point = int((1 - data_fraction) * len(acting_player))\n",
        "    acting_players += acting_player\n",
        "    reacting_players += reacting_player\n",
        "    current_players = current_player\n",
        "    actions_game = action_game\n",
        "    reactions_game += reaction_game\n",
        "    challenges_game = challenge_game\n",
        "    cards_game = card_game\n",
        "    coins_game = coin_game\n",
        "    challenges_direction = challenge_direction\n",
        "    cards_chosen = card_chosen\n",
        "    discard_piles = discard_pile\n",
        "    all_discard_piles += discard_pile\n",
        "\n",
        "    avg_game_lengths.append(game_length_sum / 100)\n",
        "\n",
        "    # cards_game: N x 4 x 2 -> 8 tensors of size N\n",
        "    cards_game_tensors = [\n",
        "        torch.tensor([cards_game[i][j][k] for i in range(len(cards_game))] )\n",
        "        for j in range(4) for k in range(2)\n",
        "    ]\n",
        "\n",
        "    # current_players: N x 4 -> 4 tensors of size N\n",
        "    current_players_tensors = torch.tensor([sum(row) for row in current_players])\n",
        "\n",
        "    coins_game_tensors = [\n",
        "              torch.tensor([coins_game[i][j] for i in range(len(coins_game))] )\n",
        "              for j in range(4)\n",
        "    ]\n",
        "\n",
        "    # discard_piles: N x y (max y = 7) -> 7 tensors of size N\n",
        "    max_discard_len = 7  # Maximum possible length of discard_piles\n",
        "    discard_piles_tensors = [\n",
        "        torch.tensor([discard_piles[i][j] if j < len(discard_piles[i]) else 0\n",
        "                      for i in range(len(discard_piles))] )\n",
        "        for j in range(max_discard_len)\n",
        "    ]\n",
        "\n",
        "\n",
        "    # Concatenate tensors for states_action\n",
        "    # new_states_action = torch.cat(([\n",
        "    #     torch.tensor(acting_players[1:]).unsqueeze(1), # unsqueeze to add a dimension\n",
        "    #     torch.tensor(reacting_players[:-1]).unsqueeze(1),\n",
        "    #     torch.tensor(reactions_game[:-1]).unsqueeze(1),\n",
        "    #     torch.tensor(challenges_game[:-1]).unsqueeze(1),\n",
        "    #     torch.tensor(current_players_tensors[:-1]).unsqueeze(1),\n",
        "    #     *[t[:-1].unsqueeze(1) for t in cards_game_tensors],\n",
        "    #     *[t[:-1].unsqueeze(1) for t in discard_piles_tensors],\n",
        "    #     *[t[:-1].unsqueeze(1) for t in coins_game_tensors]\n",
        "    # ]), 1).type(torch.float32)  # changed dim to 1\n",
        "    # states_action = torch.cat([states_action, new_states_action], 0)\n",
        "\n",
        "\n",
        "    # 1. Cards in play (embedded):\n",
        "    all_cards_in_play_embedded = []\n",
        "\n",
        "    for current_discard_pile in discard_pile:\n",
        "        cards_in_play_embedded = []\n",
        "        for card_name in influences.keys():\n",
        "            num_in_discard = current_discard_pile.count(inf_map[card_name])\n",
        "            num_in_play = 3 - num_in_discard\n",
        "            cards_in_play_embedded.append(torch.tensor(num_in_play)) # Remove .tolist() here\n",
        "\n",
        "        all_cards_in_play_embedded.append(torch.stack(cards_in_play_embedded)) # Stack the embedded tensors here\n",
        "\n",
        "    # Convert to a single tensor outside the loop\n",
        "    all_cards_in_play_embedded = torch.stack(all_cards_in_play_embedded)\n",
        "\n",
        "    # 4. Bot 0's normalized coins:\n",
        "    bot0_coins_normalized = torch.tensor(coin_game)[:, 0] / 12  # Get Bot 0's coins and normalize\n",
        "\n",
        "    # 5. Average cards of other players (normalized and embedded):\n",
        "    avg_other_cards_normalized = []\n",
        "    for step_cards in cards_game:\n",
        "        other_bots_cards = [len([card for card in bot_cards if card != 0])\n",
        "                          for bot_cards in step_cards[1:]]  # Exclude Bot 0\n",
        "        avg_other_cards = sum(other_bots_cards) / len(other_bots_cards) if other_bots_cards else 0\n",
        "        avg_other_cards_normalized.append(avg_other_cards / 2)\n",
        "\n",
        "    avg_other_cards_normalized = torch.tensor(avg_other_cards_normalized)\n",
        "    # avg_other_cards_embedded = embedding_cards(torch.tensor(int(avg_other_cards_normalized))).tolist()  # Assuming embedding_cards is your embedding layer\n",
        "\n",
        "    # 6. Bot 0's current cards (embedded):\n",
        "    all_bot0_cards_embedded = []  # Store embedded cards for all steps\n",
        "\n",
        "    for step_cards in cards_game:\n",
        "        bot0_cards_embedded = []\n",
        "        for card in step_cards[0]:  # Get Bot 0's cards for this step\n",
        "            if card != 0:  # Assuming 0 represents the absence of a card\n",
        "                bot0_cards_embedded.extend(embedding_cards(torch.tensor(card)).tolist())\n",
        "\n",
        "        # If Bot 0 has no cards, add zero embeddings for consistency\n",
        "        while len(bot0_cards_embedded) < embedding_cards.embedding_dim * 2:  # Assuming 2 cards max\n",
        "            bot0_cards_embedded.extend([0] * embedding_cards.embedding_dim)\n",
        "\n",
        "        all_bot0_cards_embedded.append(torch.tensor(bot0_cards_embedded))  # Convert to tensor and store\n",
        "\n",
        "    all_bot0_cards_embedded = torch.stack(all_bot0_cards_embedded) # Stack to create a 2D tensor\n",
        "\n",
        "    # 7. The last action taken (embedded):\n",
        "    all_last_action_embedded = []  # Store embedded last actions for all steps\n",
        "\n",
        "    for i in range(len(actions_game)):\n",
        "        last_action = actions_game[i]  # Get the action for the current step\n",
        "        last_action_embedded = embedding_actions(torch.tensor(last_action)).tolist()\n",
        "        all_last_action_embedded.append(last_action_embedded)\n",
        "\n",
        "    all_last_action_embedded = torch.tensor(all_last_action_embedded)  # Convert to a tensor\n",
        "\n",
        "    new_state = torch.cat(([all_cards_in_play_embedded.unsqueeze(-1),\n",
        "                        bot0_coins_normalized.unsqueeze(-1).unsqueeze(-1),\n",
        "                        avg_other_cards_normalized.unsqueeze(-1).unsqueeze(-1),\n",
        "                        all_bot0_cards_embedded.unsqueeze(-1),\n",
        "                        all_last_action_embedded.unsqueeze(-1)]),\n",
        "                      1).squeeze(2)\n",
        "    state = torch.cat([state, new_state], 0)\n",
        "\n",
        "    # print(states_action)\n",
        "\n",
        "    # states_action = torch.tensor([acting_players[:-1], reacting_players[:-1], reactions_game[:-1], challenges_game[:-1], current_players[:-1], cards_game[:-1], coins_game[:-1], discard_piles[:-1], done[:-1]])\n",
        "    # new_next_states_action = torch.cat(([\n",
        "    #     torch.tensor(acting_players[1:]).unsqueeze(1), # unsqueeze to add a dimension\n",
        "    #     torch.tensor(reacting_players[1:]).unsqueeze(1),\n",
        "    #     torch.tensor(reactions_game[1:]).unsqueeze(1),\n",
        "    #     torch.tensor(challenges_game[1:]).unsqueeze(1),\n",
        "    #     torch.tensor(current_players_tensors[1:]).unsqueeze(1),\n",
        "    #     *[t[1:].unsqueeze(1) for t in cards_game_tensors],\n",
        "    #     *[t[1:].unsqueeze(1) for t in discard_piles_tensors],\n",
        "    #     *[t[1:].unsqueeze(1) for t in coins_game_tensors]\n",
        "    # ]), 1).type(torch.float32)  # changed dim to 1\n",
        "    # next_states_action = torch.cat([next_states_action, new_next_states_action], 0)\n",
        "\n",
        "    new_actions_main = torch.tensor(actions_game[1:]).type(torch.int64)\n",
        "    actions_main = torch.cat([actions_main, new_actions_main], 0)\n",
        "\n",
        "\n",
        "    # new_states_block = torch.cat(([\n",
        "    #     torch.tensor(acting_players[1:]).unsqueeze(1), # unsqueeze to add a dimension\n",
        "    #     torch.tensor(reacting_players[1:]).unsqueeze(1),\n",
        "    #     torch.tensor(actions_game[1:]).unsqueeze(1),\n",
        "    #     torch.tensor(current_players_tensors[:-1]).unsqueeze(1),\n",
        "    #     *[t[:-1].unsqueeze(1) for t in cards_game_tensors],\n",
        "    #     *[t[:-1].unsqueeze(1) for t in discard_piles_tensors],\n",
        "    #     *[t[:-1].unsqueeze(1) for t in coins_game_tensors]\n",
        "    # ]), 1).type(torch.float32)  # changed dim to 1\n",
        "    # states_block = torch.cat([states_block, new_states_block], 0)\n",
        "\n",
        "    # new_next_states_block = torch.cat(([\n",
        "    #     torch.tensor(acting_players[1:]).unsqueeze(1), # unsqueeze to add a dimension\n",
        "    #     torch.tensor(reacting_players[1:]).unsqueeze(1),\n",
        "    #     torch.tensor(actions_game[1:]).unsqueeze(1),\n",
        "    #     torch.tensor(current_players_tensors[1:]).unsqueeze(1),\n",
        "    #     *[t[1:].unsqueeze(1) for t in cards_game_tensors],\n",
        "    #     *[t[1:].unsqueeze(1) for t in discard_piles_tensors],\n",
        "    #     *[t[1:].unsqueeze(1) for t in coins_game_tensors]\n",
        "    # ]), 1).type(torch.float32)  # changed dim to 1\n",
        "    # next_states_block = torch.cat([next_states_block, new_next_states_block], 0)\n",
        "\n",
        "    new_actions_block = torch.tensor(reactions_game[1:]).type(torch.int64)\n",
        "    actions_block = torch.cat([actions_block, new_actions_block], 0)\n",
        "\n",
        "\n",
        "    # new_states_challenge = torch.cat(([\n",
        "    #     torch.tensor(acting_players[1:]).unsqueeze(1), # unsqueeze to add a dimension\n",
        "    #     torch.tensor(reacting_players[1:]).unsqueeze(1),\n",
        "    #     torch.tensor(actions_game[1:]).unsqueeze(1),\n",
        "    #     torch.tensor(reactions_game[1:]).unsqueeze(1),\n",
        "    #     torch.tensor(current_players_tensors[:-1]).unsqueeze(1),\n",
        "    #     *[t[:-1].unsqueeze(1) for t in cards_game_tensors],\n",
        "    #     *[t[:-1].unsqueeze(1) for t in discard_piles_tensors],\n",
        "    #     *[t[:-1].unsqueeze(1) for t in coins_game_tensors]\n",
        "    # ]), 1).type(torch.float32) # changed dim to 1\n",
        "    # states_challenge = torch.cat([states_challenge, new_states_challenge], 0)\n",
        "\n",
        "    # new_next_states_challenge = torch.cat(([\n",
        "    #     torch.tensor(acting_players[1:]).unsqueeze(1), # unsqueeze to add a dimension\n",
        "    #     torch.tensor(reacting_players[1:]).unsqueeze(1),\n",
        "    #     torch.tensor(actions_game[1:]).unsqueeze(1),\n",
        "    #     torch.tensor(reactions_game[1:]).unsqueeze(1),\n",
        "    #     torch.tensor(current_players_tensors[1:]).unsqueeze(1),\n",
        "    #     *[t[1:].unsqueeze(1) for t in cards_game_tensors],\n",
        "    #     *[t[1:].unsqueeze(1) for t in discard_piles_tensors],\n",
        "    #     *[t[1:].unsqueeze(1) for t in coins_game_tensors]\n",
        "    # ]), 1).type(torch.float32)  # changed dim to 1\n",
        "    # next_states_challenge = torch.cat([next_states_challenge, new_next_states_challenge], 0)\n",
        "\n",
        "    new_actions_challenge = torch.tensor(challenges_game[1:]).type(torch.int64)\n",
        "    actions_challenge = torch.cat([actions_challenge, new_actions_challenge], 0)\n",
        "\n",
        "\n",
        "    # new_states_card = torch.cat(([\n",
        "    #     *[t[:-1].unsqueeze(1) for t in cards_game_tensors],\n",
        "    #     *[t[:-1].unsqueeze(1) for t in discard_piles_tensors],\n",
        "    #     *[t[:-1].unsqueeze(1) for t in coins_game_tensors]\n",
        "    # ]), 1).type(torch.float32)  # changed dim to 1\n",
        "    # states_card = torch.cat([states_card, new_states_card], 0)\n",
        "\n",
        "    # new_next_states_card = torch.cat(([\n",
        "    #     *[t[1:].unsqueeze(1) for t in cards_game_tensors],\n",
        "    #     *[t[1:].unsqueeze(1) for t in discard_piles_tensors],\n",
        "    #     *[t[1:].unsqueeze(1) for t in coins_game_tensors]\n",
        "    # ]), 1).type(torch.float32)  # changed dim to 1\n",
        "    # next_states_card = torch.cat([next_states_card, new_next_states_card], 0)\n",
        "\n",
        "    # actions_card = torch.tensor([acting_players[1:], reacting_players[1:], challenges_game[1:], challenges_direction[1:], cards_game[1:]])\n",
        "    new_actions_card = torch.tensor(cards_chosen[1:]).type(torch.int64)\n",
        "    actions_card = torch.cat([actions_card, new_actions_card], 0)\n",
        "\n",
        "    new_rewards = torch.tensor(reward).type(torch.float32)\n",
        "    rewards = torch.cat([rewards, new_rewards], 0)\n",
        "\n",
        "    new_done = torch.tensor(done_0).type(torch.float32)\n",
        "    done = torch.cat([done, new_done], 0)\n",
        "\n",
        "  print(f'Number of games in episode {episode}: {num_games}')\n",
        "\n",
        "  states_action = torch.empty((0, 12), dtype=torch.float32)\n",
        "  next_states_action = torch.empty((0, 12), dtype=torch.float32)\n",
        "  states_block = torch.empty((0, 12), dtype=torch.float32)\n",
        "  next_states_block = torch.empty((0, 12), dtype=torch.float32)\n",
        "  states_challenge = torch.empty((0, 12), dtype=torch.float32)\n",
        "  next_states_challenge = torch.empty((0, 12), dtype=torch.float32)\n",
        "  states_card = torch.empty((0, 12), dtype=torch.float32)\n",
        "  next_states_card = torch.empty((0, 12), dtype=torch.float32)\n",
        "\n",
        "  # Assuming you have a list called 'all_states' that contains all the states\n",
        "  # generated using the 'new_state' calculation you provided\n",
        "  # and 'acting_players' list that has acting players per state,\n",
        "  # and 'reacting_players' list for reacting players\n",
        "\n",
        "  # all_states = []  # Initialize with your existing state generation logic\n",
        "\n",
        "  # print(state.shape)\n",
        "  # print(all_bot0_cards_embedded.shape)\n",
        "\n",
        "  # Create a dictionary to store current states and their corresponding next states\n",
        "  state_transitions = defaultdict(list)\n",
        "\n",
        "  for i in range(len(state) - 1):  # Iterate through all states (except the last one)\n",
        "      # print(next_state[7:9])\n",
        "      current_state = state[i]\n",
        "      next_state = state[i + 1]\n",
        "\n",
        "      # Add the next state to the list of next states for the current state\n",
        "      state_transitions[tuple(current_state.tolist())].append(next_state)  # Convert to tuple for dictionary key\n",
        "\n",
        "  state_indices = {}\n",
        "\n",
        "  for i, state_tensor in enumerate(state):\n",
        "      state_indices[tuple(state_tensor.tolist())] = i\n",
        "\n",
        "  for i in range(len(state) - 1):  # Iterate through all states (except the last one)\n",
        "\n",
        "      current_state = state[i]\n",
        "      next_state = state[i + 1]\n",
        "\n",
        "      # --- Action Network ---\n",
        "      if acting_players[i] == 0:  # Check acting player for current state\n",
        "          states_action = torch.cat([states_action, current_state.unsqueeze(0)], 0)\n",
        "      else:\n",
        "          next_states_action = torch.cat([next_states_action, next_state.unsqueeze(0)], 0)\n",
        "\n",
        "      # --- Reaction Network & Challenge Network ---\n",
        "      if reacting_players[i] == 0:  # Bot 0 is the reacting player\n",
        "          states_block = torch.cat([states_block, current_state.unsqueeze(0)], 0)\n",
        "      else:\n",
        "          next_states_block = torch.cat([next_states_block, next_state.unsqueeze(0)], 0)\n",
        "\n",
        "      # --- Challenge Network ---\n",
        "      if reacting_players[i] == 0:\n",
        "          states_challenge = torch.cat([states_challenge, current_state.unsqueeze(0)], 0)\n",
        "      elif acting_players[i] == 0 and reactions_game[i] == 1: # Check acting player for current state\n",
        "          states_challenge = torch.cat([states_challenge, current_state.unsqueeze(0)], 0)\n",
        "      else:\n",
        "          next_states_challenge = torch.cat([next_states_challenge, next_state.unsqueeze(0)], 0)\n",
        "\n",
        "      # --- Card Network ---\n",
        "      # Assuming 'all_discard_piles' contains discard piles for each state\n",
        "      # and 'all_bot0_cards_embedded' contains Bot 0's cards for each state\n",
        "\n",
        "      current_discard_pile_size = len(all_discard_piles[i])\n",
        "      next_discard_pile_size = len(all_discard_piles[i + 1])\n",
        "\n",
        "      # Check if Bot 0 lost a card in the transition\n",
        "      bot0_lost_card = (next_discard_pile_size > current_discard_pile_size) and \\\n",
        "                        any(torch.equal(card, torch.tensor(0)) for card in next_state[7:9])\n",
        "\n",
        "      if bot0_lost_card:\n",
        "          states_card = torch.cat([states_card, current_state.unsqueeze(0)], 0)\n",
        "      else:\n",
        "          next_states_card = torch.cat([next_states_card, next_state.unsqueeze(0)], 0)\n",
        "\n",
        "\n",
        "  # print(state.shape)\n",
        "\n",
        "  # print(states_action.shape)\n",
        "  # print(next_states_action.shape)\n",
        "  # print(len(states_action) + len(next_states_action))\n",
        "\n",
        "  # print(states_block.shape)\n",
        "  # print(next_states_block.shape)\n",
        "  # print(len(states_block) + len(next_states_block))\n",
        "\n",
        "  # print(states_challenge.shape)\n",
        "  # print(next_states_challenge.shape)\n",
        "  # print(len(states_challenge) + len(next_states_challenge))\n",
        "\n",
        "  # print(states_card.shape)\n",
        "  # print(next_states_card.shape)\n",
        "  # print(len(states_card) + len(next_states_card))\n",
        "\n",
        "  # acting_players_      = actions_card[0]  # shape [N]\n",
        "  # reacting_players_    = actions_card[1]  # shape [N]\n",
        "  # challenges_direction = actions_card[3]  # shape [N]\n",
        "  # cards_game_          = actions_card[4]  # shape [N, M]\n",
        "  # chosen_player_idx = torch.where(challenges_direction == 0,\n",
        "  #                                 reacting_players_.long(),\n",
        "  #                                 cards_game_.long())\n",
        "  # row_indices = torch.arange(cards_game.shape[0])\n",
        "  # actions_card = cards_game_[row_indices, chosen_player_idx]\n",
        "  # actions_card = actions_card.unsqueeze(0)\n",
        "\n",
        "  # Convert states_action and next_states_action to float\n",
        "  # states_action = states_action.type(torch.float32)\n",
        "  # next_states_action = next_states_action.type(torch.float32)\n",
        "\n",
        "  # # Convert states_block and next_states_block to float\n",
        "  # states_block = states_block.type(torch.float32)\n",
        "  # next_states_block = next_states_block.type(torch.float32)\n",
        "\n",
        "  # # Convert states_challenge and next_states_challenge to float\n",
        "  # states_challenge = states_challenge.type(torch.float32)\n",
        "  # next_states_challenge = next_states_challenge.type(torch.float32)\n",
        "\n",
        "  # # Convert states_card and next_states_card to float\n",
        "  # states_card = states_card.type(torch.float32)\n",
        "  # next_states_card = next_states_card.type(torch.float32)\n",
        "\n",
        "  # # Convert actions_card to float (if necessary)\n",
        "  # actions_card = actions_card.type(torch.int64)\n",
        "\n",
        "  # print(len(next_states_action))\n",
        "\n",
        "  # bot = copy.deepcopy(bots[0])\n",
        "\n",
        "  # Recalculate the actual state size\n",
        "  # state_size_card = states_card.shape[1]\n",
        "\n",
        "  # Update the input layer of bot.card_q to match the actual state size\n",
        "  # bot.card_q.fc1 = nn.Linear(state_size_card, 64)\n",
        "\n",
        "\n",
        "\n",
        "  # print(len(states_action[0]))\n",
        "\n",
        "  # batch_size = 128\n",
        "  # i = 0\n",
        "\n",
        "\n",
        "  losses_action = []\n",
        "  losses_block = []\n",
        "  losses_challenge = []\n",
        "  losses_card = []\n",
        "\n",
        "\n",
        "  num_batches_action = len(states_action) // batch_size  # Calculate number of batches\n",
        "  for i in range(num_batches_action):  # Loop through desired number of batches\n",
        "    # Randomly sample batch indices\n",
        "    batch_indices_action = random.sample(range(len(states_action)), min(batch_size, len(states_action)))\n",
        "\n",
        "    batch_indices_challenge = random.sample(range(len(states_challenge)), min(batch_size, len(states_challenge)))\n",
        "    batch_indices_card = random.sample(range(len(states_card)), min(batch_size, len(states_card)))\n",
        "\n",
        "\n",
        "\n",
        "    # Create batch for each replay buffer using sampled indices\n",
        "    batch_states_action = torch.stack([states_action[j] for j in batch_indices_action])\n",
        "    batch_actions_main = actions_main[batch_indices_action]  # Index to get correct shape\n",
        "\n",
        "    batch_next_states_action = torch.empty((0, 12), dtype=torch.float32)\n",
        "\n",
        "    batch_rewards_action = []\n",
        "    batch_done_action = []\n",
        "\n",
        "    for state_tensor in batch_states_action:\n",
        "\n",
        "        indices = torch.where((state == state_tensor).all(dim=1))[0]\n",
        "\n",
        "        # Handle cases where the state is not found (e.g., terminal state)\n",
        "        if len(indices) == 0:\n",
        "            batch_next_states_action = torch.cat([batch_next_states_action, state_tensor.unsqueeze(0)], 0)\n",
        "            batch_rewards_action.append(torch.tensor(0.0))  # Default reward for terminal state\n",
        "            batch_done_action.append(torch.tensor(1))\n",
        "            continue\n",
        "\n",
        "        state_index = indices[0].item()  # Get the index as an integer\n",
        "\n",
        "\n",
        "        # Extract next states until Bot 0's next turn\n",
        "        next_states = torch.empty((0, 12), dtype=torch.float32)\n",
        "        current_index = state_index + 1\n",
        "        rewards_for_state = []\n",
        "        while current_index < len(state) and acting_players[current_index] != 0 and done[current_index] != 0:  # Assuming acting_player is a list of acting player IDs\n",
        "            next_states = torch.cat([next_states, state[current_index].unsqueeze(0)], 0)\n",
        "            rewards_for_state.append(rewards[current_index])\n",
        "            current_index += 1\n",
        "\n",
        "        if not next_states.numel():\n",
        "            batch_next_states_action = torch.cat([batch_next_states_action, state_tensor.unsqueeze(0)], 0)\n",
        "            batch_rewards_action.append(rewards[current_index])\n",
        "            batch_done_action.append(torch.tensor(1))\n",
        "            continue\n",
        "\n",
        "        # print(next_states.shape)\n",
        "\n",
        "        # Summarize the next states using the summarizer\n",
        "        next_states = next_states.view(1, next_states.shape[0], 12)\n",
        "        next_state_tensor = summarizer(next_states)\n",
        "        # print(next_state_tensor.shape)\n",
        "        batch_next_states_action = torch.cat([batch_next_states_action, next_state_tensor], 0)\n",
        "        batch_rewards_action.append(torch.tensor(sum(rewards_for_state) / len(rewards_for_state) if rewards_for_state else rewards[current_index-1])) # Average reward, handle empty list\n",
        "        batch_done_action.append(torch.tensor(done[current_index - 1] if current_index >= len(state) else torch.tensor(done[current_index])))\n",
        "\n",
        "\n",
        "    # Stack the next states into a single tensor\n",
        "    # batch_next_states_action = torch.stack(batch_next_states_action)\n",
        "\n",
        "    # print(batch_states_action.shape)\n",
        "    # print(batch_next_states_action.shape)\n",
        "    # print(batch_actions_main.shape)\n",
        "\n",
        "    # batch_rewards_action = rewards[batch_indices_action]\n",
        "    # batch_done_action = done[batch_indices_action]\n",
        "\n",
        "    action_q_next = bot.action_q(batch_next_states_action).max(1)[0]\n",
        "    action_target_q = torch.tensor(batch_rewards_action) + (gamma * action_q_next * (1 - torch.tensor(batch_done_action)))\n",
        "    q_values_action = bot.action_q(batch_states_action).gather(1, batch_actions_main.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss_action = criterion(q_values_action, action_target_q)\n",
        "\n",
        "    # Optimize the Q-network\n",
        "    bot.optimizer_action.zero_grad()\n",
        "    loss_action.backward()\n",
        "    bot.optimizer_action.step()\n",
        "\n",
        "    states_action = np.delete(states_action.cpu().numpy(), batch_indices_action, axis=0)\n",
        "    actions_main = np.delete(actions_main.cpu().numpy(), batch_indices_action, axis=0)  # Adjust axis for actions_main\n",
        "    next_states_action = np.delete(next_states_action.cpu().numpy(), batch_indices_action, axis=0)\n",
        "\n",
        "    states_action = torch.tensor(states_action, dtype=torch.float32)\n",
        "    actions_main = torch.tensor(actions_main, dtype=torch.int64)\n",
        "    next_states_action = torch.tensor(next_states_action, dtype=torch.float32)\n",
        "\n",
        "    losses_action.append(loss_action.item())\n",
        "\n",
        "    # print('action success')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  num_batches_block = len(states_block) // batch_size  # Calculate number of batches\n",
        "  for i in range(num_batches_block):\n",
        "    batch_indices_block = random.sample(range(len(states_block)), min(batch_size, len(states_block)))\n",
        "\n",
        "    batch_states_block = torch.stack([states_block[j] for j in batch_indices_block])\n",
        "    batch_actions_block = actions_block[batch_indices_block]\n",
        "\n",
        "    batch_next_states_block = torch.empty((0, 12), dtype=torch.float32)\n",
        "\n",
        "    batch_rewards_block = []\n",
        "    batch_done_block = []\n",
        "\n",
        "    for state_tensor in batch_states_block:\n",
        "\n",
        "        indices = torch.where((state == state_tensor).all(dim=1))[0]\n",
        "\n",
        "        # Handle cases where the state is not found (e.g., terminal state)\n",
        "        if len(indices) == 0:\n",
        "            batch_next_states_block = torch.cat([batch_next_states_block, state_tensor.unsqueeze(0)], 0)\n",
        "            batch_rewards_block.append(torch.tensor(0.0))  # Default reward for terminal state\n",
        "            batch_done_block.append(torch.tensor(1))\n",
        "            continue\n",
        "\n",
        "        state_index = indices[0].item()  # Get the index as an integer\n",
        "\n",
        "\n",
        "        # Extract next states until Bot 0's next turn\n",
        "        next_states = torch.empty((0, 12), dtype=torch.float32)\n",
        "        current_index = state_index + 1\n",
        "        rewards_for_state = []\n",
        "        while current_index < len(state) and reacting_players[current_index] != 0 and done[current_index] != 1:  # Assuming acting_player is a list of acting player IDs\n",
        "            next_states = torch.cat([next_states, state[current_index].unsqueeze(0)], 0)\n",
        "            rewards_for_state.append(rewards[current_index])\n",
        "            current_index += 1\n",
        "\n",
        "        if not next_states.numel():\n",
        "            batch_next_states_block = torch.cat([batch_next_states_block, state_tensor.unsqueeze(0)], 0)\n",
        "            batch_rewards_block.append(rewards[current_index])\n",
        "            batch_done_block.append(torch.tensor(1))\n",
        "            continue\n",
        "\n",
        "        # print(next_states.shape)\n",
        "\n",
        "        # Summarize the next states using the summarizer\n",
        "        next_states = next_states.view(1, next_states.shape[0], 12)\n",
        "        next_state_tensor = summarizer(next_states)\n",
        "        # print(next_state_tensor.shape)\n",
        "        batch_next_states_block = torch.cat([batch_next_states_block, next_state_tensor], 0)\n",
        "        batch_rewards_block.append(torch.tensor(sum(rewards_for_state) / len(rewards_for_state) if rewards_for_state else rewards[current_index-1])) # Average reward, handle empty list\n",
        "        batch_done_block.append(torch.tensor(done[current_index-1]) if current_index >= len(state) else torch.tensor(done[current_index]))\n",
        "\n",
        "    # batch_rewards_block = rewards[batch_indices_block]\n",
        "    # batch_done_block = done[batch_indices_block]\n",
        "\n",
        "    block_q_next = bot.block_q(batch_next_states_block).max(1)[0]\n",
        "    block_target_q = torch.tensor(batch_rewards_block) + (gamma * block_q_next * (1 - torch.tensor(batch_done_block)))\n",
        "    q_values_block = bot.block_q(batch_states_block).gather(1, batch_actions_block.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss_block = criterion(q_values_block, block_target_q)\n",
        "\n",
        "    # Optimize the Q-network\n",
        "    bot.optimizer_block.zero_grad()\n",
        "    loss_block.backward()\n",
        "    bot.optimizer_block.step()\n",
        "\n",
        "    states_block = np.delete(states_block.cpu().numpy(), batch_indices_block, axis=0)\n",
        "    actions_block = np.delete(actions_block.cpu().numpy(), batch_indices_block, axis=0)\n",
        "    next_states_block = np.delete(next_states_block.cpu().numpy(), batch_indices_block, axis=0)\n",
        "\n",
        "    states_block = torch.tensor(states_block, dtype=torch.float32)\n",
        "    actions_block = torch.tensor(actions_block, dtype=torch.int64)\n",
        "    next_states_block = torch.tensor(next_states_block, dtype=torch.float32)\n",
        "\n",
        "    losses_block.append(loss_block.item())\n",
        "\n",
        "    # print('block success')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  num_batches_challenge = len(states_challenge) // batch_size  # Calculate number of batches\n",
        "  for i in range(num_batches_challenge):\n",
        "    batch_indices_challenge = random.sample(range(len(states_challenge)), min(batch_size, len(states_challenge)))\n",
        "\n",
        "    batch_states_challenge = torch.stack([states_challenge[j] for j in batch_indices_challenge])\n",
        "    batch_actions_challenge = actions_challenge[batch_indices_challenge]\n",
        "    batch_next_states_challenge = torch.empty((0, 12), dtype=torch.float32)\n",
        "\n",
        "    batch_rewards_challenge = []\n",
        "    batch_done_challenge = []\n",
        "\n",
        "    for state_tensor in batch_states_challenge:\n",
        "\n",
        "        indices = torch.where((state == state_tensor).all(dim=1))[0]\n",
        "\n",
        "        # Handle cases where the state is not found (e.g., terminal state)\n",
        "        if len(indices) == 0:\n",
        "            batch_next_states_challenge = torch.cat([batch_next_states_challenge, state_tensor.unsqueeze(0)], 0)\n",
        "            batch_rewards_challenge.append(torch.tensor(0.0))  # Default reward for terminal state\n",
        "            batch_done_challenge.append(torch.tensor(1))  # Mark terminal state as done\n",
        "            continue\n",
        "\n",
        "        state_index = indices[0].item()  # Get the index as an integer\n",
        "\n",
        "\n",
        "        # Extract next states until Bot 0's next turn\n",
        "        next_states = torch.empty((0, 12), dtype=torch.float32)\n",
        "        current_index = state_index + 1\n",
        "        rewards_for_state = []\n",
        "        while current_index < len(state) and ((reacting_players[current_index] != 0) and (acting_players[current_index] != 0 or reactions_game[current_index] != 1)) and done[current_index] != 1:  # Assuming acting_player is a list of acting player IDs\n",
        "            next_states = torch.cat([next_states, state[current_index].unsqueeze(0)], 0)\n",
        "            rewards_for_state.append(rewards[current_index])\n",
        "            current_index += 1\n",
        "\n",
        "        if not next_states.numel():\n",
        "            batch_next_states_challenge = torch.cat([batch_next_states_challenge, state_tensor.unsqueeze(0)], 0)\n",
        "            batch_rewards_challenge.append(rewards[current_index])\n",
        "            batch_done_challenge.append(torch.tensor(1))\n",
        "            continue\n",
        "\n",
        "        # print(next_states.shape)\n",
        "\n",
        "        # Summarize the next states using the summarizer\n",
        "        next_states = next_states.view(1, next_states.shape[0], 12)\n",
        "        next_state_tensor = summarizer(next_states)\n",
        "        # print(next_state_tensor.shape)\n",
        "        batch_next_states_challenge = torch.cat([batch_next_states_challenge, next_state_tensor], 0)\n",
        "\n",
        "        batch_rewards_challenge.append(torch.tensor(sum(rewards_for_state) / len(rewards_for_state) if rewards_for_state else rewards[current_index-1])) # Average reward, handle empty list\n",
        "        batch_done_challenge.append(torch.tensor(done[current_index-1]) if current_index >= len(state) else torch.tensor(done[current_index]))\n",
        "\n",
        "    # batch_rewards_challenge = rewards[batch_indices_challenge]\n",
        "    # batch_done_challenge = done[batch_indices_challenge]\n",
        "\n",
        "    challenge_q_next = bot.challenge_q(batch_next_states_challenge).max(1)[0]\n",
        "    challenge_target_q = torch.tensor(batch_rewards_challenge) + (gamma * challenge_q_next * (1 - torch.tensor(batch_done_challenge)))\n",
        "    q_values_challenge = bot.challenge_q(batch_states_challenge).gather(1, batch_actions_challenge.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss_challenge = criterion(q_values_challenge, challenge_target_q)\n",
        "\n",
        "    # Optimize the Q-network\n",
        "    bot.optimizer_challenge.zero_grad()\n",
        "    loss_challenge.backward()\n",
        "    bot.optimizer_challenge.step()\n",
        "\n",
        "    states_challenge = np.delete(states_challenge.cpu().numpy(), batch_indices_challenge, axis=0)\n",
        "    actions_challenge = np.delete(actions_challenge.cpu().numpy(), batch_indices_challenge, axis=0)\n",
        "    next_states_challenge = np.delete(next_states_challenge.cpu().numpy(), batch_indices_challenge, axis=0)\n",
        "\n",
        "    states_challenge = torch.tensor(states_challenge, dtype=torch.float32)\n",
        "    actions_challenge = torch.tensor(actions_challenge, dtype=torch.int64)\n",
        "    next_states_challenge = torch.tensor(next_states_challenge, dtype=torch.float32)\n",
        "\n",
        "    losses_challenge.append(loss_challenge.item())\n",
        "\n",
        "    # print('challenge success')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  num_batches_card = len(states_card) // batch_size  # Calculate number of batches\n",
        "  for i in range(num_batches_card):\n",
        "    batch_indices_card = random.sample(range(len(states_card)), min(batch_size, len(states_card)))\n",
        "\n",
        "    batch_states_card = torch.stack([states_card[j] for j in batch_indices_card])\n",
        "    batch_actions_card = actions_card[batch_indices_card]\n",
        "\n",
        "    batch_next_states_card = torch.empty((0, 12), dtype=torch.float32)\n",
        "    batch_rewards_card = []\n",
        "    batch_done_card = []\n",
        "\n",
        "    for state_tensor in batch_states_card:\n",
        "\n",
        "        indices = torch.where((state == state_tensor).all(dim=1))[0]\n",
        "\n",
        "        # Handle cases where the state is not found (e.g., terminal state)\n",
        "        if len(indices) == 0:\n",
        "            batch_next_states_card = torch.cat([batch_next_states_card, state_tensor.unsqueeze(0)], 0)\n",
        "            batch_rewards_card.append(torch.tensor(0.0))  # Default reward for terminal state\n",
        "            batch_done_card.append(torch.tensor(1))  # Mark terminal state as done\n",
        "            continue\n",
        "\n",
        "        state_index = indices[0].item()  # Get the index as an integer\n",
        "\n",
        "\n",
        "        # Extract next states until Bot 0's next turn\n",
        "        next_states = torch.empty((0, 12), dtype=torch.float32)\n",
        "        current_index = state_index + 1\n",
        "        rewards_for_state = []\n",
        "        if current_index == len(state):\n",
        "            batch_next_states_card = torch.cat([batch_next_states_card, state_tensor.unsqueeze(0)], 0)\n",
        "            rewards_for_state.append(rewards[current_index])  # Collect reward for each next state\n",
        "            continue\n",
        "\n",
        "        while current_index < len(state) - 1 and done[current_index] != 1:  # Assuming acting_player is a list of acting player IDs\n",
        "            current_discard_pile_size = len(all_discard_piles[current_index])\n",
        "            next_discard_pile_size = len(all_discard_piles[current_index + 1])\n",
        "\n",
        "            current_bot0_card_length = torch.sum(state[current_index][7:9] == 0).item()\n",
        "            next_bot0_card_length = torch.sum(state[current_index + 1][7:9] == 0).item()\n",
        "\n",
        "            # Check if Bot 0 doesn't lose a card in the transition\n",
        "            bot0_not_lost_card = (next_discard_pile_size == current_discard_pile_size) or \\\n",
        "             (current_bot0_card_length == next_bot0_card_length)\n",
        "\n",
        "            if bot0_not_lost_card:\n",
        "                next_states = torch.cat([next_states, state_tensor.unsqueeze(0)], 0)\n",
        "\n",
        "            rewards_for_state.append(rewards[current_index])  # Collect reward for each next state\n",
        "            current_index += 1\n",
        "\n",
        "        if not next_states.numel():\n",
        "            batch_next_states_card = torch.cat([batch_next_states_card, state_tensor.unsqueeze(0)], 0)\n",
        "            batch_rewards_card.append(rewards[current_index])\n",
        "            batch_done_card.append(torch.tensor(1))  # Mark terminal state as done\n",
        "            continue\n",
        "\n",
        "        # print(next_states.shape)\n",
        "\n",
        "        # Summarize the next states using the summarizer\n",
        "        next_states = next_states.view(1, next_states.shape[0], 12)\n",
        "        next_state_tensor = summarizer(next_states)\n",
        "        # print(next_state_tensor.shape)\n",
        "        batch_next_states_card = torch.cat([batch_next_states_card, next_state_tensor], 0)\n",
        "\n",
        "        batch_rewards_card.append(torch.tensor(sum(rewards_for_state) / len(rewards_for_state) if rewards_for_state else rewards[current_index-1])) # Average reward, handle empty list\n",
        "        batch_done_card.append(torch.tensor(done[current_index-1]) if current_index >= len(state) else torch.tensor(done[current_index]))\n",
        "\n",
        "    # print(batch_states_card.shape)\n",
        "    # print(batch_next_states_card.shape)\n",
        "    # print(batch_actions_card.shape)\n",
        "\n",
        "    # batch_rewards_card = rewards[batch_indices_card]\n",
        "    # batch_done_card = done[batch_indices_card]\n",
        "\n",
        "    # print(batch_rewards_card.shape)\n",
        "    # print(batch_done_card.shape)\n",
        "\n",
        "    card_q_next = bot.card_q(batch_next_states_card).max(1)[0]\n",
        "    card_target_q = torch.tensor(batch_rewards_card) + (gamma * card_q_next * (1 - torch.tensor(batch_done_card)))\n",
        "    # print(f\"batch_actions_card: {batch_actions_card}\")\n",
        "    q_values_card = bot.card_q(batch_states_card).gather(1, batch_actions_card.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss_card = criterion(q_values_card, card_target_q)\n",
        "\n",
        "    # Optimize the Q-network\n",
        "    bot.optimizer_card.zero_grad()\n",
        "    loss_card.backward()\n",
        "    bot.optimizer_card.step()\n",
        "\n",
        "    states_card = np.delete(states_card.cpu().numpy(), batch_indices_card, axis=0)\n",
        "    actions_card = np.delete(actions_card.cpu().numpy(), batch_indices_card, axis=0)\n",
        "    next_states_card = np.delete(next_states_card.cpu().numpy(), batch_indices_card, axis=0)\n",
        "\n",
        "    states_card = torch.tensor(states_card, dtype=torch.float32)\n",
        "    actions_card = torch.tensor(actions_card, dtype=torch.int64)\n",
        "    next_states_card = torch.tensor(next_states_card, dtype=torch.float32)\n",
        "\n",
        "    losses_card.append(loss_card.item())\n",
        "\n",
        "    # print('card success')\n",
        "\n",
        "    # Delete the randomly selected rows from the tensors\n",
        "\n",
        "    # rewards = np.delete(rewards.cpu().numpy(), batch_indices_action+batch_indices_block+batch_indices_challenge+batch_indices_card, axis=0)\n",
        "    # done = np.delete(done.cpu().numpy(), batch_indices_action+batch_indices_block+batch_indices_challenge+batch_indices_card, axis=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Convert back to tensors for the next batch\n",
        "\n",
        "    # rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "    # done = torch.tensor(done, dtype=torch.float32)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # print(f'batch {i}:')\n",
        "    # print(f'loss for action: {loss_action}')\n",
        "    # print(f'loss for block: {loss_block}')\n",
        "    # print(f'loss for challenge: {loss_challenge}')\n",
        "    # print(f'loss for card: {loss_card}')\n",
        "\n",
        "    bot.cards = bots[0].cards\n",
        "    bot.num_coins = bots[0].num_coins\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # i += 1\n",
        "  epsilon *= 0.995\n",
        "\n",
        "  # if (episode + 1) % 100 == 0:\n",
        "  #   data_fraction = min(data_fraction + 1/5, 1)\n",
        "  #   epsilon = 1.0\n",
        "\n",
        "  # gamma = min(0.99, gamma + 0.001)\n",
        "\n",
        "  avg_losses_action.append(sum(losses_action) / len(losses_action))\n",
        "  avg_losses_block.append(sum(losses_block) / len(losses_block))\n",
        "  avg_losses_challenge.append(sum(losses_challenge) / len(losses_challenge))\n",
        "  avg_losses_card.append(sum(losses_card) / len(losses_card))\n",
        "\n",
        "  print(f'Avg Action Loss, {num_batches_action} batches: {avg_losses_action[-1]}')\n",
        "  print(f'Avg Block Loss, {num_batches_block} batches: {avg_losses_block[-1]}')\n",
        "  print(f'Avg Challenge Loss, {num_batches_challenge} batches: {avg_losses_challenge[-1]}')\n",
        "  print(f'Avg Card Loss, {num_batches_card} batches: {avg_losses_card[-1]}')\n",
        "\n",
        "  # Copy parameters of action_q network\n",
        "  bots[0].action_q.load_state_dict(bot.action_q.state_dict())\n",
        "\n",
        "  # Copy parameters of block_q network\n",
        "  bots[0].block_q.load_state_dict(bot.block_q.state_dict())\n",
        "\n",
        "  # Copy parameters of challenge_q network\n",
        "  bots[0].challenge_q.load_state_dict(bot.challenge_q.state_dict())\n",
        "\n",
        "  # Copy parameters of card_q network\n",
        "  bots[0].card_q.load_state_dict(bot.card_q.state_dict())\n",
        "\n",
        "  print(bots[0].name)\n",
        "\n",
        "  if episode % 10 == 0:\n",
        "\n",
        "    win_rate = 0\n",
        "\n",
        "    for i in range(100):\n",
        "\n",
        "      # print(i)\n",
        "      discard_pile, acting_player, reacting_player, current_player, action_game, reaction_game, challenge_game, card_game, coin_game, challenge_direction, done_0, reward, card_chosen, bots_copy = game_loop_random(bots, actions, influences_reverse, 0.0)\n",
        "      bots = bots_copy\n",
        "      if reward[-1] == 1:\n",
        "        win_rate += 1\n",
        "\n",
        "    # win_rate = win_rate / 50\n",
        "    # print(f'Bot 0 Win Rate, Random Actions: {win_rate / 100}')\n",
        "\n",
        "    win_rate = win_rate / 100\n",
        "    win_rates.append(win_rate)\n",
        "\n",
        "    print(f'win rate: {win_rate}')\n",
        "\n",
        "\n",
        "  # df = pd.DataFrame(data = data)\n",
        "  # print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Dky8U78hbAps",
      "metadata": {
        "id": "Dky8U78hbAps"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(avg_losses_action, label='Avg Action Loss')\n",
        "plt.plot(avg_losses_block, label='Avg Block Loss')\n",
        "plt.plot(avg_losses_challenge, label='Avg Challenge Loss')\n",
        "plt.plot(avg_losses_card, label='Avg Card Loss')\n",
        "\n",
        "plt.xlabel('Episode/Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Losses')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(win_rates, label='Win Rate')\n",
        "\n",
        "plt.xlabel('Episode/Iteration')\n",
        "plt.ylabel('Win Rate')\n",
        "plt.title('Win Rate over Time')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vKOENygAg-jY"
      },
      "id": "vKOENygAg-jY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(avg_game_lengths, label='Avg Game Lengths')\n",
        "\n",
        "plt.xlabel('Episode/Iteration')\n",
        "plt.ylabel('Avg Game Length (timesteps)')\n",
        "plt.title('Avg Game Length over Time')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mQYFQF_TiYhE"
      },
      "id": "mQYFQF_TiYhE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RttgbXIgzFfB",
      "metadata": {
        "id": "RttgbXIgzFfB"
      },
      "outputs": [],
      "source": [
        "torch.save({\n",
        "        'action_q_state_dict': bot.action_q.state_dict(),\n",
        "        'block_q_state_dict': bot.block_q.state_dict(),\n",
        "        'challenge_q_state_dict': bot.challenge_q.state_dict(),\n",
        "        'card_q_state_dict': bot.card_q.state_dict(),\n",
        "        'optimizer_action_state_dict': bot.optimizer_action.state_dict(),\n",
        "        'optimizer_block_state_dict': bot.optimizer_block.state_dict(),\n",
        "        'optimizer_challenge_state_dict': bot.optimizer_challenge.state_dict(),\n",
        "        'optimizer_card_state_dict': bot.optimizer_card.state_dict()\n",
        "    }, 'bot_parameters.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wh_pxHPuzNGM",
      "metadata": {
        "id": "wh_pxHPuzNGM"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load('bot_parameters.pth')\n",
        "bot.action_q.load_state_dict(checkpoint['action_q_state_dict'])\n",
        "bot.block_q.load_state_dict(checkpoint['block_q_state_dict'])\n",
        "bot.challenge_q.load_state_dict(checkpoint['challenge_q_state_dict'])\n",
        "bot.card_q.load_state_dict(checkpoint['card_q_state_dict'])\n",
        "bot.optimizer_action.load_state_dict(checkpoint['optimizer_action_state_dict'])\n",
        "bot.optimizer_block.load_state_dict(checkpoint['optimizer_block_state_dict'])\n",
        "bot.optimizer_challenge.load_state_dict(checkpoint['optimizer_challenge_state_dict'])\n",
        "bot.optimizer_card.load_state_dict(checkpoint['optimizer_card_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q4YC59Kfkn0B",
      "metadata": {
        "id": "Q4YC59Kfkn0B"
      },
      "outputs": [],
      "source": [
        "bots[0].action_q.load_state_dict(bot.action_q.state_dict())\n",
        "\n",
        "# Copy parameters of block_q network\n",
        "bots[0].block_q.load_state_dict(bot.block_q.state_dict())\n",
        "\n",
        "# Copy parameters of challenge_q network\n",
        "bots[0].challenge_q.load_state_dict(bot.challenge_q.state_dict())\n",
        "\n",
        "# Copy parameters of card_q network\n",
        "bots[0].card_q.load_state_dict(bot.card_q.state_dict())\n",
        "\n",
        "\n",
        "win_rate = 0\n",
        "\n",
        "for i in range(100):\n",
        "\n",
        "  # print(i)\n",
        "  discard_pile, acting_player, reacting_player, current_player, action_game, reaction_game, challenge_game, card_game, coin_game, challenge_direction, done_0, reward, card_chosen, bots_copy = game_loop_random(bots, actions, influences_reverse, 0.0)\n",
        "  bots = bots_copy\n",
        "  if reward[-1] == 1:\n",
        "    win_rate += 1\n",
        "\n",
        "# win_rate = win_rate / 50\n",
        "print(f'Bot 0 Win Rate, Random Actions: {win_rate / 100}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}